{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b48a0fd-291a-40cd-acd8-8c829e95d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gc\n",
    "import argparse\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "from models.vae import VAE \n",
    "from losses.cyl_ptpz_mae import CylPtPzMAE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7a62fa7-f106-4ac2-a162-e9f71c579c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting a seed like in vae_legacy\n",
    "def set_seed(seed=123):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e1d77cf-1b21-4d68-9b17-0e3456eda6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_corr(var_1,var_2,normedweight,power=1):\n",
    "    \"\"\"var_1: First variable to decorrelate (eg mass)\n",
    "    var_2: Second variable to decorrelate (eg classifier output)\n",
    "    normedweight: Per-example weight. Sum of weights should add up to N (where N is the number of examples)\n",
    "    power: Exponent used in calculating the distance correlation\n",
    "    \n",
    "    va1_1, var_2 and normedweight should all be 1D torch tensors with the same number of entries\n",
    "    \n",
    "    Usage: Add to your loss function. total_loss = BCE_loss + lambda * distance_corr\n",
    "    \"\"\" \n",
    "    \n",
    "    xx = var_1.view(-1, 1).repeat(1, len(var_1)).view(len(var_1),len(var_1))\n",
    "    yy = var_1.repeat(len(var_1),1).view(len(var_1),len(var_1))\n",
    "    amat = (xx-yy).abs()\n",
    "\n",
    "    xx = var_2.view(-1, 1).repeat(1, len(var_2)).view(len(var_2),len(var_2))\n",
    "    yy = var_2.repeat(len(var_2),1).view(len(var_2),len(var_2))\n",
    "    bmat = (xx-yy).abs()\n",
    "\n",
    "    amatavg = torch.mean(amat*normedweight,dim=1)\n",
    "    Amat=amat-amatavg.repeat(len(var_1),1).view(len(var_1),len(var_1))\\\n",
    "        -amatavg.view(-1, 1).repeat(1, len(var_1)).view(len(var_1),len(var_1))\\\n",
    "        +torch.mean(amatavg*normedweight)\n",
    "\n",
    "    bmatavg = torch.mean(bmat*normedweight,dim=1)\n",
    "    Bmat=bmat-bmatavg.repeat(len(var_2),1).view(len(var_2),len(var_2))\\\n",
    "        -bmatavg.view(-1, 1).repeat(1, len(var_2)).view(len(var_2),len(var_2))\\\n",
    "        +torch.mean(bmatavg*normedweight)\n",
    "\n",
    "    ABavg = torch.mean(Amat*Bmat*normedweight,dim=1)\n",
    "    AAavg = torch.mean(Amat*Amat*normedweight,dim=1)\n",
    "    BBavg = torch.mean(Bmat*Bmat*normedweight,dim=1)\n",
    "\n",
    "    if(power==1):\n",
    "        dCorr=(torch.mean(ABavg*normedweight))/torch.sqrt((torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight)))\n",
    "    elif(power==2):\n",
    "        dCorr=(torch.mean(ABavg*normedweight))**2/(torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight))\n",
    "    else:\n",
    "        dCorr=((torch.mean(ABavg*normedweight))/torch.sqrt((torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight))))**power\n",
    "    \n",
    "    return dCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b8c591-3e47-475b-bb84-e5bd2fa05c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates anomaly score like in vae legacy\n",
    "def distance_pt(model_vae, data_np, device):\n",
    "    x = torch.tensor(data_np, dtype=torch.float32, device=device)\n",
    "    z_mean, z_logvar, _ = model_vae.encoder(x)\n",
    "    score = torch.sum(z_mean**2, dim=1)\n",
    "    return score.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3cba8fd-5877-42ba-99d8-6a364b851f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print h5 tree\n",
    "def print_h5_tree(h, prefix=\"\"):\n",
    "    for k in h.keys():\n",
    "        item = h[k]\n",
    "        if hasattr(item, 'keys'):\n",
    "            print(prefix + f\"[GROUP] {k}\")\n",
    "            print_h5_tree(item, prefix + \"  \")\n",
    "        else:\n",
    "            try:\n",
    "                print(prefix + f\"{k}: shape={item.shape}, dtype={item.dtype}\")\n",
    "            except Exception:\n",
    "                print(prefix + f\"{k}: <dataset>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4b5eb8d-ae86-4af9-b69b-54c4c0d81689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to make 2D histograms\n",
    "def make_2D_hist(x, y, xlabel, ylabel, title, wandb_key, bins=40):\n",
    "    fig = plt.figure(figsize=(5,4))\n",
    "    plt.hist2d(x, y, bins=bins)\n",
    "    plt.colorbar(label='Counts')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    wandb.log({wandb_key: wandb.Image(fig)})\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "664818a8-841b-40cc-ab1d-6f23ec4138a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config: Dict):\n",
    "    # set seed\n",
    "    seed = int(config.get('seed', 123))\n",
    "    set_seed(seed)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using: {device}\")\n",
    "\n",
    "    #login to wandb\n",
    "    print(\"Logging in to wandb...\")\n",
    "    wandb.login(key=\"24d1d60ce26563c74d290d7b487cb104fc251271\")\n",
    "    wandb.init(\n",
    "        project=\"Double Disco Axo Training\",\n",
    "        settings=wandb.Settings(_disable_stats=True),\n",
    "        config=config\n",
    "    )\n",
    "    run_name = wandb.run.name\n",
    "    print(f\"Run name: {run_name}\")\n",
    "\n",
    "    #scaling\n",
    "    beta = float(config['beta'])\n",
    "    alpha = float(config['alpha'])\n",
    "    vae_lr = float(config['vae_lr'])\n",
    "\n",
    "    #load data\n",
    "    print(\"Loading dataset...\")\n",
    "    fpath = '/axovol/training/v5/conditionsupdate_apr25.h5'\n",
    "    with h5.File(fpath, 'r') as f:\n",
    "        root = f['data'] if 'data' in f else f\n",
    "\n",
    "        # print h5 tree to view\n",
    "        # print(\"Printing h5 tree...\")\n",
    "        # print_h5_tree(root)\n",
    "\n",
    "        x_train = root['Background_data']['Train']['DATA'][:]\n",
    "        x_test = root['Background_data']['Test']['DATA'][:]\n",
    "        print(f\"Train shape: {x_train.shape}, Test shape: {x_test.shape}\")\n",
    "\n",
    "        #flatten event for vae 1\n",
    "        x_train_bkg = x_train.reshape(x_train.shape[0], -1).astype('float32')\n",
    "        x_test_bkg = x_test.reshape(x_test.shape[0],  -1).astype('float32') \n",
    "\n",
    "        #scale and biases\n",
    "        scale = root['Normalisation']['norm_scale'][:].astype('float32')\n",
    "        bias = root['Normalisation']['norm_bias'][:].astype('float32')\n",
    "\n",
    "        #HT and ET\n",
    "        HT_train = root['Background_data']['Train']['HT'][:].astype('float32') \n",
    "        ET_train = root['Background_data']['Train']['ET'][:].astype('float32')\n",
    "        HT_test = root['Background_data']['Test']['HT'][:].astype('float32')\n",
    "        ET_test = root['Background_data']['Test']['ET'][:].astype('float32')\n",
    "\n",
    "    print(\"Data finished loading.\")\n",
    "\n",
    "    #vae 1 is unchanged\n",
    "    X1_train = x_train_bkg \n",
    "    features1 = X1_train.shape[1] \n",
    "\n",
    "    #vae 2 trained on HT, MET + jet kinematics\n",
    "    #using 0 as MET, 1-4 and egamma, 5-8 mu, 9-18 jets (check with melissa)\n",
    "    met_idx = 0\n",
    "    n_eg = 4\n",
    "    n_mu = 4\n",
    "    n_jet = 10\n",
    "    jet_start = 1 + n_eg + n_mu\n",
    "    jet_stop = jet_start + n_jet\n",
    "\n",
    "    #flatten jets\n",
    "    jets_train = x_train[:, jet_start:jet_stop, :].reshape(x_train.shape[0], -1).astype('float32') \n",
    "    jets_test = x_test[:,  jet_start:jet_stop,  :].reshape(x_test.shape[0],  -1).astype('float32')\n",
    "\n",
    "    #make vae 2 matrices\n",
    "    X2_train = np.concatenate([HT_train[:, None], ET_train[:, None], jets_train], axis=1).astype('float32')\n",
    "    X2_test = np.concatenate([HT_test[:,  None], ET_test[:,  None], jets_test],  axis=1).astype('float32')\n",
    "    features2 = X2_train.shape[1]\n",
    "\n",
    "    #scales/biases for jets only\n",
    "    scale_jets = scale[jet_start:jet_stop, :] \n",
    "    bias_jets = bias[jet_start:jet_stop, :]\n",
    "\n",
    "    #vae 1 reco loss\n",
    "    reco1_loss_fn = CylPtPzMAE(scale, bias).to(device)\n",
    "\n",
    "    #vae 2 reco loss is MSE on [HT, MET] + CylPtPzMAE on jets\n",
    "    reco2_jet_loss_fn = CylPtPzMAE(scale_jets, bias_jets).to(device)\n",
    "    def reco2_loss_fn(y_pred, y_true):\n",
    "        mse_scalars = torch.mean((y_pred[:, :2] - y_true[:, :2])**2, dim=1)\n",
    "        mae_jets = reco2_jet_loss_fn(y_pred[:, 2:], y_true[:, 2:])\n",
    "        return mse_scalars + mae_jets\n",
    "\n",
    "    #configs\n",
    "    latent_dim = int(config['vae_latent'])\n",
    "    enc_nodes = list(config['vae_nodes'])\n",
    "    dec_nodes1 = [24, 32, 64, 128, features1]\n",
    "    dec_nodes2 = [24, 32, 64, 128, features2]\n",
    "\n",
    "    vae1_cfg = {\n",
    "        \"features\": features1,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"encoder_config\": {\"nodes\": enc_nodes},\n",
    "        \"decoder_config\": {\"nodes\": dec_nodes1},\n",
    "        \"alpha\": alpha,\n",
    "        \"beta\":  beta,\n",
    "    }\n",
    "    vae2_cfg = {\n",
    "        \"features\": features2,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"encoder_config\": {\"nodes\": enc_nodes},\n",
    "        \"decoder_config\": {\"nodes\": dec_nodes2},\n",
    "        \"alpha\": alpha,\n",
    "        \"beta\":  beta,\n",
    "    }\n",
    "\n",
    "    vae_1 = VAE(vae1_cfg).to(device)\n",
    "    vae_2 = VAE(vae2_cfg).to(device)\n",
    "    print(\"VAEs are ready.\")\n",
    "\n",
    "    #optimizer\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(vae_1.parameters()) + list(vae_2.parameters()),\n",
    "        lr=vae_lr\n",
    "    )\n",
    "\n",
    "    #cosine restarts\n",
    "    warmup_epochs = int(config.get('warmup_epochs', 10))\n",
    "    cos = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=32, T_mult=2, eta_min=0.0\n",
    "    )\n",
    "\n",
    "    #sets learning rate\n",
    "    def set_lr(lr):\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "\n",
    "    #hyperparameters\n",
    "    Epochs_VAE = int(config.get('epochs', 50))\n",
    "    Batch_size = int(config.get('batch_size', 16384))\n",
    "\n",
    "    #disco scaling\n",
    "    lambda_disco = float(config.get(\"lambda_disco\", 1.0))\n",
    "\n",
    "    #move data to device\n",
    "    print(\"Moving data to device...\")\n",
    "    X1 = torch.tensor(X1_train, dtype=torch.float32, device=device)\n",
    "    X2 = torch.tensor(X2_train, dtype=torch.float32, device=device) \n",
    "    print(\"Data on device.\")\n",
    "\n",
    "    #training loop\n",
    "    print(\"Starting the training loop!\")\n",
    "    N = X1.size(0)\n",
    "    for epoch in range(Epochs_VAE):\n",
    "\n",
    "        #lists for 2D hists\n",
    "        vae1_total_loss = []\n",
    "        vae2_total_loss = []\n",
    "        vae1_kl_loss = []\n",
    "        vae2_kl_loss = []\n",
    "        vae1_reco_loss = []\n",
    "        vae2_reco_loss = []\n",
    "\n",
    "        #cosine warmup step\n",
    "        if epoch < warmup_epochs:\n",
    "            lr = vae_lr * (epoch + 1) / warmup_epochs\n",
    "            set_lr(lr)\n",
    "        else:\n",
    "            cos.step(epoch - warmup_epochs)\n",
    "\n",
    "        #shuffle indices\n",
    "        perm = torch.randperm(N, device=device)\n",
    "\n",
    "        total_loss = total_reco1 = total_reco2 = total_kl1 = total_kl2 = total_disco = 0.0\n",
    "\n",
    "        #training loop\n",
    "        for i in range(0, N, Batch_size):\n",
    "            idx = perm[i:i+Batch_size]\n",
    "            xb1 = X1[idx]  # full input for vae 1\n",
    "            xb2 = X2[idx]  # HT, MET, jets for vae 2\n",
    "\n",
    "            #vae 1\n",
    "            recon1, mu1, logvar1, z1 = vae_1(xb1)\n",
    "            \n",
    "            #vae 2\n",
    "            recon2, mu2, logvar2, z2 = vae_2(xb2)\n",
    "\n",
    "            #get reco loss from custom func\n",
    "            reco1_per = reco1_loss_fn(recon1, xb1)\n",
    "            reco2_per = reco2_loss_fn(recon2, xb2)\n",
    "\n",
    "            #get kl div per sample\n",
    "            kl1_per = VAE.kl_divergence(mu1, logvar1)\n",
    "            kl2_per = VAE.kl_divergence(mu2, logvar2)\n",
    "            \n",
    "            tot1_per = reco1_per + kl1_per\n",
    "            tot2_per = reco2_per + kl2_per\n",
    "\n",
    "            #from paper code weight\n",
    "            B = xb1.shape[0]\n",
    "            w = torch.ones(B, device=tot1_per.device, dtype=tot1_per.dtype)\n",
    "\n",
    "            #disco loss (ask Melissa about since using mu instead of z)\n",
    "            #disco = disco_loss(mu1, mu2)\n",
    "            disco = distance_corr((tot1_per), (tot2_per), w, power=1)\n",
    "\n",
    "            reco1 = vae_1.reco_scale * reco1_per.mean()\n",
    "            reco2 = vae_2.reco_scale * reco2_per.mean()\n",
    "            kl1 = vae_1.kl_scale   * kl1_per.mean()\n",
    "            kl2 = vae_2.kl_scale   * kl2_per.mean()\n",
    "\n",
    "            #total loss\n",
    "            loss = (reco1 + kl1) + (reco2 + kl2) + lambda_disco * disco\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(vae_1.parameters()) + list(vae_2.parameters()),\n",
    "                max_norm=5.0\n",
    "            )\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss+= float(loss.item())\n",
    "            total_reco1+= float(reco1.item())\n",
    "            total_reco2+= float(reco2.item())\n",
    "            total_kl1+= float(kl1.item())\n",
    "            total_kl2+= float(kl2.item())\n",
    "            total_disco+= float(disco.item())\n",
    "\n",
    "            #add loss to lists\n",
    "            vae1_total_loss.append((reco1 + kl1).item())\n",
    "            vae2_total_loss.append((reco2 + kl2).item())\n",
    "            vae1_kl_loss.append(kl1.item())\n",
    "            vae2_kl_loss.append(kl2.item())\n",
    "            vae1_reco_loss.append(reco1.item())\n",
    "            vae2_reco_loss.append(reco2.item())\n",
    "\n",
    "        print(f\"[EPOCH {epoch}/{Epochs_VAE}] \"\n",
    "              f\"Loss={total_loss:.4f} \"\n",
    "              f\"Reco1={total_reco1:.4f} Reco2={total_reco2:.4f} \"\n",
    "              f\"KL1={total_kl1:.4f} KL2={total_kl2:.4f} \"\n",
    "              f\"DisCo={total_disco:.4f}\")\n",
    "\n",
    "        wandb.log({\n",
    "            \"EpochVae\": epoch,\n",
    "            \"TotalLossVae\": total_loss,\n",
    "            \"RecoLossVae1\": total_reco1,\n",
    "            \"RecoLossVae2\": total_reco2,\n",
    "            \"KLLossVae1\": total_kl1,\n",
    "            \"KLLossVae2\": total_kl2,\n",
    "            \"DisCoLoss\": total_disco,\n",
    "        })\n",
    "\n",
    "        #2d hist plotting\n",
    "        vae1_total_np = np.array(vae1_total_loss)\n",
    "        vae2_total_np = np.array(vae2_total_loss)\n",
    "        vae1_kl_np = np.array(vae1_kl_loss)\n",
    "        vae2_kl_np = np.array(vae2_kl_loss)\n",
    "        vae1_reco_np = np.array(vae1_reco_loss)\n",
    "        vae2_reco_np = np.array(vae2_reco_loss)\n",
    "\n",
    "        make_2D_hist(vae1_total_np, vae2_total_np,\n",
    "                     \"Total Loss (VAE1)\", \"Total Loss (VAE2)\",\n",
    "                     f\"Epoch {epoch}: Total VAE1 vs Total VAE2\",\n",
    "                     wandb_key=\"Hists2D/Total_VAE1_vs_Total_VAE2\")\n",
    "\n",
    "        make_2D_hist(vae1_kl_np, vae2_kl_np,\n",
    "                     \"KL Loss (VAE1)\", \"KL Loss (VAE2)\",\n",
    "                     f\"Epoch {epoch}: KL VAE1 vs KL VAE2\",\n",
    "                     wandb_key=\"Hists2D/KL_VAE_1_vs_KL_VAE_2\")\n",
    "\n",
    "        make_2D_hist(vae1_reco_np, vae2_reco_np,\n",
    "                     \"Reco Loss (VAE1)\", \"Reco Loss (VAE2)\",\n",
    "                     f\"Epoch {epoch}: Reco VAE1 vs Reco VAE2\",\n",
    "                     wandb_key=\"Hists2D/Reco_VAE1_vs_Reco_VAE2\")\n",
    "\n",
    "    print(\"Finished training.\")\n",
    "    torch.save(vae_1.state_dict(), \"vae1_trained.pth\")\n",
    "    torch.save(vae_2.state_dict(), \"vae2_trained.pth\")\n",
    "    print(\"Saved vae1_trained.pth and vae2_trained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d8510e-f772-4aed-bb82-45bba1f8f8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n",
      "Logging in to wandb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jovyan/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mescheuller\u001b[0m (\u001b[33mescheuller-uc-san-diego\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20250828_202944-mfu3h3mn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/mfu3h3mn' target=\"_blank\">dutiful-frog-128</a></strong> to <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training' target=\"_blank\">https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/mfu3h3mn' target=\"_blank\">https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/mfu3h3mn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: dutiful-frog-128\n",
      "Loading dataset...\n",
      "Train shape: (1999965, 19, 3), Test shape: (4511092, 19, 3)\n",
      "Data finished loading.\n",
      "VAEs are ready.\n",
      "Moving data to device...\n",
      "Data on device.\n",
      "Starting the training loop!\n",
      "[EPOCH 0/50] Loss=9939819.8828 Reco1=565639.3574 Reco2=9355283.4766 KL1=13.0455 KL2=8022.1641 DisCo=10.8618\n",
      "[EPOCH 1/50] Loss=9643295.1406 Reco1=464339.0737 Reco2=9155141.4453 KL1=12.9806 KL2=8443.0349 DisCo=15.3586\n",
      "[EPOCH 2/50] Loss=9360974.2109 Reco1=341681.0542 Reco2=8986353.0625 KL1=13.5263 KL2=9427.7883 DisCo=23.4988\n",
      "[EPOCH 3/50] Loss=9220387.0703 Reco1=266274.2717 Reco2=8917923.0078 KL1=15.1320 KL2=6978.4545 DisCo=29.1962\n",
      "[EPOCH 4/50] Loss=9176936.5781 Reco1=245970.6062 Reco2=8896924.9219 KL1=16.0770 KL2=3656.3808 DisCo=30.3686\n",
      "[EPOCH 5/50] Loss=9149188.5938 Reco1=242475.6259 Reco2=8873777.4531 KL1=15.6541 KL2=2340.2903 DisCo=30.5795\n",
      "[EPOCH 6/50] Loss=9124294.3047 Reco1=241683.4819 Reco2=8850121.9062 KL1=14.4276 KL2=1852.3495 DisCo=30.6222\n",
      "[EPOCH 7/50] Loss=9096868.4375 Reco1=241370.5791 Reco2=8823159.1797 KL1=12.5650 KL2=1837.4957 DisCo=30.4886\n",
      "[EPOCH 8/50] Loss=9069668.2969 Reco1=241332.0555 Reco2=8795720.8125 KL1=11.6054 KL2=2275.2384 DisCo=30.3286\n",
      "[EPOCH 9/50] Loss=9040012.9609 Reco1=241244.5742 Reco2=8765809.9141 KL1=11.1346 KL2=2851.6739 DisCo=30.0957\n",
      "[EPOCH 10/50] Loss=8990530.2266 Reco1=241284.4957 Reco2=8716701.0625 KL1=10.8944 KL2=2995.2134 DisCo=29.5386\n",
      "[EPOCH 11/50] Loss=8962027.4297 Reco1=241211.8982 Reco2=8688526.8125 KL1=10.7849 KL2=3066.3798 DisCo=29.2116\n",
      "[EPOCH 12/50] Loss=8953224.0625 Reco1=241286.4175 Reco2=8679708.6953 KL1=10.7133 KL2=3150.8408 DisCo=29.0674\n",
      "[EPOCH 13/50] Loss=8944108.0781 Reco1=241186.9691 Reco2=8670716.8438 KL1=10.6248 KL2=3233.8153 DisCo=28.9598\n",
      "[EPOCH 14/50] Loss=8941938.4297 Reco1=241222.4097 Reco2=8668462.9844 KL1=10.5591 KL2=3298.1126 DisCo=28.9443\n",
      "[EPOCH 15/50] Loss=8939105.2422 Reco1=241309.3938 Reco2=8665589.3516 KL1=10.5086 KL2=3342.7901 DisCo=28.8532\n",
      "[EPOCH 16/50] Loss=8932679.1250 Reco1=241231.1299 Reco2=8659237.9766 KL1=10.4469 KL2=3362.1487 DisCo=28.8375\n",
      "[EPOCH 17/50] Loss=8931004.1562 Reco1=241283.8906 Reco2=8657498.1953 KL1=10.3882 KL2=3378.8271 DisCo=28.8328\n",
      "[EPOCH 18/50] Loss=8928413.4141 Reco1=241266.7643 Reco2=8654927.0547 KL1=10.3249 KL2=3383.3384 DisCo=28.8260\n",
      "[EPOCH 19/50] Loss=8925586.0312 Reco1=241293.2208 Reco2=8652091.4141 KL1=10.2457 KL2=3384.0998 DisCo=28.8071\n",
      "[EPOCH 20/50] Loss=8922389.2891 Reco1=241238.9169 Reco2=8649007.6641 KL1=10.1810 KL2=3382.0758 DisCo=28.7504\n",
      "[EPOCH 21/50] Loss=8919819.8750 Reco1=241247.5381 Reco2=8646409.7266 KL1=10.1211 KL2=3382.4245 DisCo=28.7701\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"vae_lr\": 1e-4,\n",
    "    \"beta\": 0.5,\n",
    "    \"alpha\": 0.5,\n",
    "    \"vae_latent\": 8,\n",
    "    \"vae_nodes\": [28, 14],\n",
    "    \"lambda_disco\": 1000.0,\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 16384,\n",
    "    \"warmup_epochs\": 10,\n",
    "}\n",
    "run(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8c1625-d5b4-488c-b660-973d5f145e54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
