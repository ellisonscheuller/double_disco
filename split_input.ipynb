{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b48a0fd-291a-40cd-acd8-8c829e95d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gc\n",
    "import argparse\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "from models.vae import VAE \n",
    "from losses.cyl_ptpz_mae import CylPtPzMAE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7a62fa7-f106-4ac2-a162-e9f71c579c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting a seed like in vae_legacy\n",
    "def set_seed(seed=123):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e1d77cf-1b21-4d68-9b17-0e3456eda6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_corr(var_1,var_2,normedweight,power=1):\n",
    "    \"\"\"var_1: First variable to decorrelate (eg mass)\n",
    "    var_2: Second variable to decorrelate (eg classifier output)\n",
    "    normedweight: Per-example weight. Sum of weights should add up to N (where N is the number of examples)\n",
    "    power: Exponent used in calculating the distance correlation\n",
    "    \n",
    "    va1_1, var_2 and normedweight should all be 1D torch tensors with the same number of entries\n",
    "    \n",
    "    Usage: Add to your loss function. total_loss = BCE_loss + lambda * distance_corr\n",
    "    \"\"\" \n",
    "    \n",
    "    xx = var_1.view(-1, 1).repeat(1, len(var_1)).view(len(var_1),len(var_1))\n",
    "    yy = var_1.repeat(len(var_1),1).view(len(var_1),len(var_1))\n",
    "    amat = (xx-yy).abs()\n",
    "\n",
    "    xx = var_2.view(-1, 1).repeat(1, len(var_2)).view(len(var_2),len(var_2))\n",
    "    yy = var_2.repeat(len(var_2),1).view(len(var_2),len(var_2))\n",
    "    bmat = (xx-yy).abs()\n",
    "\n",
    "    amatavg = torch.mean(amat*normedweight,dim=1)\n",
    "    Amat=amat-amatavg.repeat(len(var_1),1).view(len(var_1),len(var_1))\\\n",
    "        -amatavg.view(-1, 1).repeat(1, len(var_1)).view(len(var_1),len(var_1))\\\n",
    "        +torch.mean(amatavg*normedweight)\n",
    "\n",
    "    bmatavg = torch.mean(bmat*normedweight,dim=1)\n",
    "    Bmat=bmat-bmatavg.repeat(len(var_2),1).view(len(var_2),len(var_2))\\\n",
    "        -bmatavg.view(-1, 1).repeat(1, len(var_2)).view(len(var_2),len(var_2))\\\n",
    "        +torch.mean(bmatavg*normedweight)\n",
    "\n",
    "    ABavg = torch.mean(Amat*Bmat*normedweight,dim=1)\n",
    "    AAavg = torch.mean(Amat*Amat*normedweight,dim=1)\n",
    "    BBavg = torch.mean(Bmat*Bmat*normedweight,dim=1)\n",
    "\n",
    "    if(power==1):\n",
    "        dCorr=(torch.mean(ABavg*normedweight))/torch.sqrt((torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight)))\n",
    "    elif(power==2):\n",
    "        dCorr=(torch.mean(ABavg*normedweight))**2/(torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight))\n",
    "    else:\n",
    "        dCorr=((torch.mean(ABavg*normedweight))/torch.sqrt((torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight))))**power\n",
    "    \n",
    "    return dCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b8c591-3e47-475b-bb84-e5bd2fa05c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates anomaly score like in vae legacy\n",
    "def distance_pt(model_vae, data_np, device):\n",
    "    x = torch.tensor(data_np, dtype=torch.float32, device=device)\n",
    "    z_mean, z_logvar, _ = model_vae.encoder(x)\n",
    "    score = torch.sum(z_mean**2, dim=1)\n",
    "    return score.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3cba8fd-5877-42ba-99d8-6a364b851f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print h5 tree\n",
    "def print_h5_tree(h, prefix=\"\"):\n",
    "    for k in h.keys():\n",
    "        item = h[k]\n",
    "        if hasattr(item, 'keys'):\n",
    "            print(prefix + f\"[GROUP] {k}\")\n",
    "            print_h5_tree(item, prefix + \"  \")\n",
    "        else:\n",
    "            try:\n",
    "                print(prefix + f\"{k}: shape={item.shape}, dtype={item.dtype}\")\n",
    "            except Exception:\n",
    "                print(prefix + f\"{k}: <dataset>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4b5eb8d-ae86-4af9-b69b-54c4c0d81689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to make 2D histograms\n",
    "def make_2D_hist(x, y, xlabel, ylabel, title, wandb_key, bins=40):\n",
    "    fig = plt.figure(figsize=(5,4))\n",
    "    plt.hist2d(x, y, bins=bins)\n",
    "    plt.colorbar(label='Counts')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    wandb.log({wandb_key: wandb.Image(fig)})\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "664818a8-841b-40cc-ab1d-6f23ec4138a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config: Dict):\n",
    "    # set seed\n",
    "    seed = int(config.get('seed', 123))\n",
    "    set_seed(seed)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using: {device}\")\n",
    "\n",
    "    #login to wandb\n",
    "    print(\"Logging in to wandb...\")\n",
    "    wandb.login(key=\"24d1d60ce26563c74d290d7b487cb104fc251271\")\n",
    "    wandb.init(\n",
    "        project=\"Double Disco Axo Training\",\n",
    "        settings=wandb.Settings(_disable_stats=True),\n",
    "        config=config\n",
    "    )\n",
    "    run_name = wandb.run.name\n",
    "    print(f\"Run name: {run_name}\")\n",
    "\n",
    "    #scaling\n",
    "    beta = float(config['beta'])\n",
    "    alpha = float(config['alpha'])\n",
    "    vae_lr = float(config['vae_lr'])\n",
    "\n",
    "    #load data\n",
    "    print(\"Loading dataset...\")\n",
    "    fpath = '/axovol/training/v5/conditionsupdate_apr25.h5'\n",
    "    with h5.File(fpath, 'r') as f:\n",
    "        root = f['data'] if 'data' in f else f\n",
    "\n",
    "        # print h5 tree to view\n",
    "        # print(\"Printing h5 tree...\")\n",
    "        # print_h5_tree(root)\n",
    "\n",
    "        x_train = root['Background_data']['Train']['DATA'][:]\n",
    "        x_test = root['Background_data']['Test']['DATA'][:]\n",
    "        print(f\"Train shape: {x_train.shape}, Test shape: {x_test.shape}\")\n",
    "\n",
    "        #flatten event for vae 1\n",
    "        x_train_bkg = x_train.reshape(x_train.shape[0], -1).astype('float32')\n",
    "        x_test_bkg = x_test.reshape(x_test.shape[0],  -1).astype('float32') \n",
    "\n",
    "        #scale and biases\n",
    "        scale = root['Normalisation']['norm_scale'][:].astype('float32')\n",
    "        bias = root['Normalisation']['norm_bias'][:].astype('float32')\n",
    "\n",
    "        #HT and ET\n",
    "        HT_train = root['Background_data']['Train']['HT'][:].astype('float32') \n",
    "        ET_train = root['Background_data']['Train']['ET'][:].astype('float32')\n",
    "        HT_test = root['Background_data']['Test']['HT'][:].astype('float32')\n",
    "        ET_test = root['Background_data']['Test']['ET'][:].astype('float32')\n",
    "\n",
    "    print(\"Data finished loading.\")\n",
    "\n",
    "    #vae 1 is unchanged\n",
    "    X1_train = x_train_bkg \n",
    "    features1 = X1_train.shape[1] \n",
    "\n",
    "    #vae 2 trained on HT, MET + jet kinematics\n",
    "    #using 0 as MET, 1-4 and egamma, 5-8 mu, 9-18 jets (check with melissa)\n",
    "    met_idx = 0\n",
    "    n_eg = 4\n",
    "    n_mu = 4\n",
    "    n_jet = 10\n",
    "    jet_start = 1 + n_eg + n_mu\n",
    "    jet_stop = jet_start + n_jet\n",
    "\n",
    "    #flatten jets\n",
    "    jets_train = x_train[:, jet_start:jet_stop, :].reshape(x_train.shape[0], -1).astype('float32') \n",
    "    jets_test = x_test[:,  jet_start:jet_stop,  :].reshape(x_test.shape[0],  -1).astype('float32')\n",
    "\n",
    "    #make vae 2 matrices\n",
    "    X2_train = np.concatenate([HT_train[:, None], ET_train[:, None], jets_train], axis=1).astype('float32')\n",
    "    X2_test = np.concatenate([HT_test[:,  None], ET_test[:,  None], jets_test],  axis=1).astype('float32')\n",
    "    features2 = X2_train.shape[1]\n",
    "\n",
    "    #scales/biases for jets only\n",
    "    scale_jets = scale[jet_start:jet_stop, :] \n",
    "    bias_jets = bias[jet_start:jet_stop, :]\n",
    "\n",
    "    #vae 1 reco loss\n",
    "    reco1_loss_fn = CylPtPzMAE(scale, bias).to(device)\n",
    "\n",
    "    #vae 2 reco loss is MSE on [HT, MET] + CylPtPzMAE on jets\n",
    "    reco2_jet_loss_fn = CylPtPzMAE(scale_jets, bias_jets).to(device)\n",
    "    def reco2_loss_fn(y_pred, y_true):\n",
    "        mse_scalars = torch.mean((y_pred[:, :2] - y_true[:, :2])**2, dim=1)\n",
    "        mae_jets = reco2_jet_loss_fn(y_pred[:, 2:], y_true[:, 2:])\n",
    "        return mse_scalars + mae_jets\n",
    "\n",
    "    #configs\n",
    "    latent_dim = int(config['vae_latent'])\n",
    "    enc_nodes = list(config['vae_nodes'])\n",
    "    dec_nodes1 = [24, 32, 64, 128, features1]\n",
    "    dec_nodes2 = [24, 32, 64, 128, features2]\n",
    "\n",
    "    vae1_cfg = {\n",
    "        \"features\": features1,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"encoder_config\": {\"nodes\": enc_nodes},\n",
    "        \"decoder_config\": {\"nodes\": dec_nodes1},\n",
    "        \"alpha\": alpha,\n",
    "        \"beta\":  beta,\n",
    "    }\n",
    "    vae2_cfg = {\n",
    "        \"features\": features2,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"encoder_config\": {\"nodes\": enc_nodes},\n",
    "        \"decoder_config\": {\"nodes\": dec_nodes2},\n",
    "        \"alpha\": alpha,\n",
    "        \"beta\":  beta,\n",
    "    }\n",
    "\n",
    "    vae_1 = VAE(vae1_cfg).to(device)\n",
    "    vae_2 = VAE(vae2_cfg).to(device)\n",
    "    print(\"VAEs are ready.\")\n",
    "\n",
    "    #optimizer\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(vae_1.parameters()) + list(vae_2.parameters()),\n",
    "        lr=vae_lr\n",
    "    )\n",
    "\n",
    "    #cosine restarts\n",
    "    warmup_epochs = int(config.get('warmup_epochs', 10))\n",
    "    cos = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=32, T_mult=2, eta_min=0.0\n",
    "    )\n",
    "\n",
    "    #sets learning rate\n",
    "    def set_lr(lr):\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "\n",
    "    #hyperparameters\n",
    "    Epochs_VAE = int(config.get('epochs', 50))\n",
    "    Batch_size = int(config.get('batch_size', 16384))\n",
    "\n",
    "    #disco scaling\n",
    "    lambda_disco = float(config.get(\"lambda_disco\", 1.0))\n",
    "\n",
    "    #move data to device\n",
    "    print(\"Moving data to device...\")\n",
    "    X1 = torch.tensor(X1_train, dtype=torch.float32, device=device)\n",
    "    X2 = torch.tensor(X2_train, dtype=torch.float32, device=device) \n",
    "    print(\"Data on device.\")\n",
    "\n",
    "    #training loop\n",
    "    print(\"Starting the training loop!\")\n",
    "    N = X1.size(0)\n",
    "    for epoch in range(Epochs_VAE):\n",
    "\n",
    "        #lists for 2D hists\n",
    "        vae1_total_loss = []\n",
    "        vae2_total_loss = []\n",
    "        vae1_kl_loss = []\n",
    "        vae2_kl_loss = []\n",
    "        vae1_reco_loss = []\n",
    "        vae2_reco_loss = []\n",
    "\n",
    "        #cosine warmup step\n",
    "        if epoch < warmup_epochs:\n",
    "            lr = vae_lr * (epoch + 1) / warmup_epochs\n",
    "            set_lr(lr)\n",
    "        else:\n",
    "            cos.step(epoch - warmup_epochs)\n",
    "\n",
    "        #shuffle indices\n",
    "        perm = torch.randperm(N, device=device)\n",
    "\n",
    "        total_loss = total_reco1 = total_reco2 = total_kl1 = total_kl2 = total_disco = 0.0\n",
    "\n",
    "        #training loop\n",
    "        for i in range(0, N, Batch_size):\n",
    "            idx = perm[i:i+Batch_size]\n",
    "            xb1 = X1[idx]  # full input for vae 1\n",
    "            xb2 = X2[idx]  # HT, MET, jets for vae 2\n",
    "\n",
    "            #vae 1\n",
    "            recon1, mu1, logvar1, z1 = vae_1(xb1)\n",
    "            \n",
    "            #vae 2\n",
    "            recon2, mu2, logvar2, z2 = vae_2(xb2)\n",
    "\n",
    "            #get reco loss from custom func\n",
    "            reco1_per = reco1_loss_fn(recon1, xb1)\n",
    "            reco2_per = reco2_loss_fn(recon2, xb2)\n",
    "\n",
    "            #get kl div per sample\n",
    "            kl1_per = VAE.kl_divergence(mu1, logvar1)\n",
    "            kl2_per = VAE.kl_divergence(mu2, logvar2)\n",
    "            \n",
    "            tot1_per = reco1_per + kl1_per\n",
    "            tot2_per = reco2_per + kl2_per\n",
    "\n",
    "            #from paper code weight\n",
    "            B = xb1.shape[0]\n",
    "            w = torch.ones(B, device=tot1_per.device, dtype=tot1_per.dtype)\n",
    "\n",
    "            #disco loss (ask Melissa about since using mu instead of z)\n",
    "            #disco = disco_loss(mu1, mu2)\n",
    "            disco = distance_corr((tot1_per), (tot2_per), w, power=1)\n",
    "\n",
    "            reco1 = vae_1.reco_scale * reco1_per.mean()\n",
    "            reco2 = vae_2.reco_scale * reco2_per.mean()\n",
    "            kl1 = vae_1.kl_scale   * kl1_per.mean()\n",
    "            kl2 = vae_2.kl_scale   * kl2_per.mean()\n",
    "\n",
    "            #total loss\n",
    "            loss = (reco1 + kl1) + (reco2 + kl2) + lambda_disco * disco\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(vae_1.parameters()) + list(vae_2.parameters()),\n",
    "                max_norm=5.0\n",
    "            )\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss+= float(loss.item())\n",
    "            total_reco1+= float(reco1.item())\n",
    "            total_reco2+= float(reco2.item())\n",
    "            total_kl1+= float(kl1.item())\n",
    "            total_kl2+= float(kl2.item())\n",
    "            total_disco+= float(disco.item())\n",
    "\n",
    "            #add loss to lists\n",
    "            vae1_total_loss.append((reco1 + kl1).item())\n",
    "            vae2_total_loss.append((reco2 + kl2).item())\n",
    "            vae1_kl_loss.append(kl1.item())\n",
    "            vae2_kl_loss.append(kl2.item())\n",
    "            vae1_reco_loss.append(reco1.item())\n",
    "            vae2_reco_loss.append(reco2.item())\n",
    "\n",
    "        print(f\"[EPOCH {epoch}/{Epochs_VAE}] \"\n",
    "              f\"Loss={total_loss:.4f} \"\n",
    "              f\"Reco1={total_reco1:.4f} Reco2={total_reco2:.4f} \"\n",
    "              f\"KL1={total_kl1:.4f} KL2={total_kl2:.4f} \"\n",
    "              f\"DisCo={total_disco:.4f}\")\n",
    "\n",
    "        wandb.log({\n",
    "            \"EpochVae\": epoch,\n",
    "            \"TotalLossVae\": total_loss,\n",
    "            \"RecoLossVae1\": total_reco1,\n",
    "            \"RecoLossVae2\": total_reco2,\n",
    "            \"KLLossVae1\": total_kl1,\n",
    "            \"KLLossVae2\": total_kl2,\n",
    "            \"DisCoLoss\": total_disco,\n",
    "        })\n",
    "\n",
    "        #2d hist plotting\n",
    "        vae1_total_np = np.array(vae1_total_loss)\n",
    "        vae2_total_np = np.array(vae2_total_loss)\n",
    "        vae1_kl_np = np.array(vae1_kl_loss)\n",
    "        vae2_kl_np = np.array(vae2_kl_loss)\n",
    "        vae1_reco_np = np.array(vae1_reco_loss)\n",
    "        vae2_reco_np = np.array(vae2_reco_loss)\n",
    "\n",
    "        make_2D_hist(vae1_total_np, vae2_total_np,\n",
    "                     \"Total Loss (VAE1)\", \"Total Loss (VAE2)\",\n",
    "                     f\"Epoch {epoch}: Total VAE1 vs Total VAE2\",\n",
    "                     wandb_key=\"Hists2D/Total_VAE1_vs_Total_VAE2\")\n",
    "\n",
    "        make_2D_hist(vae1_kl_np, vae2_kl_np,\n",
    "                     \"KL Loss (VAE1)\", \"KL Loss (VAE2)\",\n",
    "                     f\"Epoch {epoch}: KL VAE1 vs KL VAE2\",\n",
    "                     wandb_key=\"Hists2D/KL_VAE_1_vs_KL_VAE_2\")\n",
    "\n",
    "        make_2D_hist(vae1_reco_np, vae2_reco_np,\n",
    "                     \"Reco Loss (VAE1)\", \"Reco Loss (VAE2)\",\n",
    "                     f\"Epoch {epoch}: Reco VAE1 vs Reco VAE2\",\n",
    "                     wandb_key=\"Hists2D/Reco_VAE1_vs_Reco_VAE2\")\n",
    "\n",
    "    print(\"Finished training.\")\n",
    "    torch.save(vae_1.state_dict(), \"vae1_trained.pth\")\n",
    "    torch.save(vae_2.state_dict(), \"vae2_trained.pth\")\n",
    "    print(\"Saved vae1_trained.pth and vae2_trained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38d8510e-f772-4aed-bb82-45bba1f8f8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n",
      "Logging in to wandb...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">major-flower-119</strong> at: <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/k70971gb' target=\"_blank\">https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/k70971gb</a><br> View project at: <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training' target=\"_blank\">https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250828_183818-k70971gb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20250828_184238-mr5zu2at</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/mr5zu2at' target=\"_blank\">ruby-vortex-120</a></strong> to <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training' target=\"_blank\">https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/mr5zu2at' target=\"_blank\">https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/mr5zu2at</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: ruby-vortex-120\n",
      "Loading dataset...\n",
      "Train shape: (1999965, 19, 3), Test shape: (4511092, 19, 3)\n",
      "Data finished loading.\n",
      "VAEs are ready.\n",
      "Moving data to device...\n",
      "Data on device.\n",
      "Starting the training loop!\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 10.57 GiB of which 841.12 MiB is free. Including non-PyTorch memory, this process has 9.74 GiB memory in use. Of the allocated memory 9.55 GiB is allocated by PyTorch, and 15.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvae_lr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1e-4\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarmup_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     11\u001b[0m }\n\u001b[0;32m---> 12\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 202\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    198\u001b[0m w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(B, device\u001b[38;5;241m=\u001b[39mtot1_per\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtot1_per\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m#disco loss (ask Melissa about since using mu instead of z)\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m#disco = disco_loss(mu1, mu2)\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m disco \u001b[38;5;241m=\u001b[39m \u001b[43mdistance_corr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtot1_per\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtot2_per\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m reco1 \u001b[38;5;241m=\u001b[39m vae_1\u001b[38;5;241m.\u001b[39mreco_scale \u001b[38;5;241m*\u001b[39m reco1_per\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    205\u001b[0m reco2 \u001b[38;5;241m=\u001b[39m vae_2\u001b[38;5;241m.\u001b[39mreco_scale \u001b[38;5;241m*\u001b[39m reco2_per\u001b[38;5;241m.\u001b[39mmean()\n",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m, in \u001b[0;36mdistance_corr\u001b[0;34m(var_1, var_2, normedweight, power)\u001b[0m\n\u001b[1;32m     18\u001b[0m bmat \u001b[38;5;241m=\u001b[39m (xx\u001b[38;5;241m-\u001b[39myy)\u001b[38;5;241m.\u001b[39mabs()\n\u001b[1;32m     20\u001b[0m amatavg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(amat\u001b[38;5;241m*\u001b[39mnormedweight,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m Amat\u001b[38;5;241m=\u001b[39m\u001b[43mamat\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mamatavg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvar_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvar_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvar_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mamatavg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvar_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvar_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvar_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\\\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;241m+\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(amatavg\u001b[38;5;241m*\u001b[39mnormedweight)\n\u001b[1;32m     25\u001b[0m bmatavg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(bmat\u001b[38;5;241m*\u001b[39mnormedweight,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m Bmat\u001b[38;5;241m=\u001b[39mbmat\u001b[38;5;241m-\u001b[39mbmatavg\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mlen\u001b[39m(var_2),\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mlen\u001b[39m(var_2),\u001b[38;5;28mlen\u001b[39m(var_2))\\\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;241m-\u001b[39mbmatavg\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(var_2))\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mlen\u001b[39m(var_2),\u001b[38;5;28mlen\u001b[39m(var_2))\\\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;241m+\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(bmatavg\u001b[38;5;241m*\u001b[39mnormedweight)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 10.57 GiB of which 841.12 MiB is free. Including non-PyTorch memory, this process has 9.74 GiB memory in use. Of the allocated memory 9.55 GiB is allocated by PyTorch, and 15.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"vae_lr\": 1e-4,\n",
    "    \"beta\": 0.5,\n",
    "    \"alpha\": 0.5,\n",
    "    \"vae_latent\": 8,\n",
    "    \"vae_nodes\": [28, 14],\n",
    "    \"lambda_disco\": 1000.0,\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 16384,\n",
    "    \"warmup_epochs\": 10,\n",
    "}\n",
    "run(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8c1625-d5b4-488c-b660-973d5f145e54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
