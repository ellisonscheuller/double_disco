{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a5269f0-7f2e-40fd-917b-c3062e30516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gc\n",
    "import argparse\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "\n",
    "from models.autoencoder import Autoencoder \n",
    "#from losses.cyl_ptpz_mae import CylPtPzMAE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LogNorm\n",
    "from scipy.stats import binned_statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0252ca81-bc87-49de-bad4-be27f0f4ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting a seed like in ae_legacy\n",
    "def set_seed(seed=123):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33be852-c54f-4652-b770-402d2add7e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_corr(var_1,var_2,normedweight,power=1):\n",
    "    \"\"\"var_1: First variable to decorrelate (eg mass)\n",
    "    var_2: Second variable to decorrelate (eg classifier output)\n",
    "    normedweight: Per-example weight. Sum of weights should add up to N (where N is the number of examples)\n",
    "    power: Exponent used in calculating the distance correlation\n",
    "    \n",
    "    va1_1, var_2 and normedweight should all be 1D torch tensors with the same number of entries\n",
    "    \n",
    "    Usage: Add to your loss function. total_loss = BCE_loss + lambda * distance_corr\n",
    "    \"\"\" \n",
    "    \n",
    "    xx = var_1.view(-1, 1).repeat(1, len(var_1)).view(len(var_1),len(var_1))\n",
    "    yy = var_1.repeat(len(var_1),1).view(len(var_1),len(var_1))\n",
    "    amat = (xx-yy).abs()\n",
    "\n",
    "    xx = var_2.view(-1, 1).repeat(1, len(var_2)).view(len(var_2),len(var_2))\n",
    "    yy = var_2.repeat(len(var_2),1).view(len(var_2),len(var_2))\n",
    "    bmat = (xx-yy).abs()\n",
    "\n",
    "    amatavg = torch.mean(amat*normedweight,dim=1)\n",
    "    Amat=amat-amatavg.repeat(len(var_1),1).view(len(var_1),len(var_1))\\\n",
    "        -amatavg.view(-1, 1).repeat(1, len(var_1)).view(len(var_1),len(var_1))\\\n",
    "        +torch.mean(amatavg*normedweight)\n",
    "\n",
    "    bmatavg = torch.mean(bmat*normedweight,dim=1)\n",
    "    Bmat=bmat-bmatavg.repeat(len(var_2),1).view(len(var_2),len(var_2))\\\n",
    "        -bmatavg.view(-1, 1).repeat(1, len(var_2)).view(len(var_2),len(var_2))\\\n",
    "        +torch.mean(bmatavg*normedweight)\n",
    "\n",
    "    ABavg = torch.mean(Amat*Bmat*normedweight,dim=1)\n",
    "    AAavg = torch.mean(Amat*Amat*normedweight,dim=1)\n",
    "    BBavg = torch.mean(Bmat*Bmat*normedweight,dim=1)\n",
    "\n",
    "    if(power==1):\n",
    "        dCorr=(torch.mean(ABavg*normedweight))/torch.sqrt((torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight)))\n",
    "    elif(power==2):\n",
    "        dCorr=(torch.mean(ABavg*normedweight))**2/(torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight))\n",
    "    else:\n",
    "        dCorr=((torch.mean(ABavg*normedweight))/torch.sqrt((torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight))))**power\n",
    "    \n",
    "    return dCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c8d3b42-b905-4340-87cc-3c96411cae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute differentiable ABCD region counts (just like discotec paper)\n",
    "def sigmoid_counts(var1, var2, cut1, cut2, weights, scale=100.0):\n",
    "    #flatten 1D\n",
    "    v1 = var1.view(-1)\n",
    "    v2 = var2.view(-1)\n",
    "    w  = weights.view(-1)\n",
    "\n",
    "    #use sigmoids instead of hard cuts so ABCD is differentiable\n",
    "    s1_high = torch.sigmoid(scale * (v1 - cut1))      \n",
    "    s1_low  = torch.sigmoid(scale * (cut1 - v1)) \n",
    "    s2_high = torch.sigmoid(scale * (v2 - cut2))\n",
    "    s2_low  = torch.sigmoid(scale * (cut2 - v2))\n",
    "\n",
    "    #counts for each region\n",
    "    NA = torch.sum(s1_high * s2_high * w)\n",
    "    NB = torch.sum(s1_high * s2_low  * w)\n",
    "    NC = torch.sum(s1_low  * s2_high * w)\n",
    "    ND = torch.sum(s1_low  * s2_low  * w)\n",
    "    return NA, NB, NC, ND\n",
    "\n",
    "#compute the ABCD closure loss for a batch\n",
    "def closure_loss_batch(var1, var2, weights, symmetrize=True,\n",
    "                       n_events_min=10, max_tries=20):\n",
    "\n",
    "    #flatten losses\n",
    "    v1 = var1.view(-1)\n",
    "    v2 = var2.view(-1)\n",
    "    w = weights.view(-1)\n",
    "\n",
    "    #pick random cuts within 1-99% of the batch like in discotec paper\n",
    "    for _ in range(max_tries):\n",
    "        with torch.no_grad():\n",
    "            x_min = torch.quantile(v1, 0.01).item()\n",
    "            x_max = torch.quantile(v1, 0.99).item()\n",
    "            y_min = torch.quantile(v2, 0.01).item()\n",
    "            y_max = torch.quantile(v2, 0.99).item()\n",
    "            cut1 = np.random.uniform(x_min, x_max)\n",
    "            cut2 = np.random.uniform(y_min, y_max)\n",
    "\n",
    "        #compute counts for ABCD\n",
    "        NA, NB, NC, ND = sigmoid_counts(v1, v2, cut1, cut2, w)\n",
    "\n",
    "        #check if these random cuts give good statistics, if not skip and try new cuts\n",
    "        if (NA.item() > n_events_min and NB.item() > n_events_min and\n",
    "            NC.item() > n_events_min and ND.item() > n_events_min):\n",
    "            break\n",
    "    else:\n",
    "        #if we somehow never found good cuts just return 0 to avoid NaNs\n",
    "        return torch.tensor(0.0, device=var1.device, dtype=var1.dtype)\n",
    "\n",
    "    #compute closure loss like discotec paper\n",
    "    if symmetrize:\n",
    "        num = torch.abs(NA * ND - NB * NC)\n",
    "        den = NA * ND + NB * NC + 1e-8\n",
    "    else:\n",
    "        num = torch.abs(NA * ND - NB * NC)\n",
    "        den = NB * NC + 1e-8\n",
    "\n",
    "    return num / den\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "320ba38b-4c09-4ea0-b883-6f7011de2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates anomaly score like in ae legacy\n",
    "def distance_pt(model_ae, data_np, device):\n",
    "    x = torch.tensor(data_np, dtype=torch.float32, device=device)\n",
    "    z_mean, z_logvar, _ = model_ae.encoder(x)\n",
    "    score = torch.sum(z_mean**2, dim=1)\n",
    "    return score.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df9cf7a6-624b-491c-ad49-5eca472a676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to make 2D histograms\n",
    "def make_2D_hist(x, y, xlabel, ylabel, title, wandb_key, bins=40):\n",
    "    fig = plt.figure(figsize=(5,4))\n",
    "    plt.hist2d(x, y, bins=bins)\n",
    "    plt.colorbar(label='Counts')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    wandb.log({wandb_key: wandb.Image(fig)})\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3855723b-e71a-4348-9d28-4edcadec73c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerSampleMSE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction='none')\n",
    "    def forward(self, recon, target):\n",
    "        per_feat = self.mse(recon, target)\n",
    "        return per_feat.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8c2e8ec-dd88-46fb-91a0-71c7048af7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_standard_scaler(X, eps=1e-8):\n",
    "    mu  = X.mean(axis=0).astype(np.float32)\n",
    "    std = X.std(axis=0).astype(np.float32)\n",
    "    std = np.where(std < eps, 1.0, std)\n",
    "    return mu, std\n",
    "\n",
    "def transform_standard(X, mu, std):\n",
    "    return (X - mu) / (std + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70f9494b-d8c8-4bf6-b3a0-1d2850958bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints h5 tree to examine\n",
    "def print_h5_tree(h, prefix=\"\"):\n",
    "    for k in h.keys():\n",
    "        item = h[k]\n",
    "        if hasattr(item, 'keys'):\n",
    "            print(prefix + f\"[GROUP] {k}\")\n",
    "            print_h5_tree(item, prefix + \"  \")\n",
    "        else:\n",
    "            try:\n",
    "                print(prefix + f\"{k}: shape={item.shape}, dtype={item.dtype}\")\n",
    "            except Exception:\n",
    "                print(prefix + f\"{k}: <dataset>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dd000d9-98e3-4fcb-a254-131f09b88bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_plot(ax, x, y, nbins=30, logx=False, min_per_bin=20, label=\"mean ± SE\"):\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    m = np.isfinite(x) & np.isfinite(y)\n",
    "    if logx:\n",
    "        m &= (x > 0)\n",
    "\n",
    "    x = x[m]\n",
    "    y = y[m]\n",
    "\n",
    "    # bin along x (linear or log space)\n",
    "    if logx:\n",
    "        xu = np.log10(x)\n",
    "    else:\n",
    "        xu = x\n",
    "\n",
    "    # uniform bins over the chosen coordinate\n",
    "    lo = float(xu.min())\n",
    "    hi = float(xu.max())\n",
    "    if lo == hi:\n",
    "        hi = np.nextafter(hi, np.inf)\n",
    "\n",
    "    edges = np.linspace(lo, hi, nbins + 1)\n",
    "    centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "\n",
    "    # stats per bin\n",
    "    mean, _, _ = binned_statistic(xu, y, statistic=\"mean\", bins=edges)\n",
    "    std,  _, _ = binned_statistic(xu, y, statistic=\"std\",  bins=edges)\n",
    "    cnt,  _, _ = binned_statistic(xu, y, statistic=\"count\", bins=edges)\n",
    "\n",
    "    sem = std / np.sqrt(np.maximum(cnt, 1))\n",
    "\n",
    "    # keep well-populated bins\n",
    "    good = cnt >= min_per_bin\n",
    "    xc = centers[good]\n",
    "    ym   = mean[good]\n",
    "    ye   = sem[good]\n",
    "\n",
    "    # convert x-axis back from log if needed\n",
    "    if logx:\n",
    "        xplot = 10.0 ** xc\n",
    "        ax.set_xscale(\"log\")\n",
    "    else:\n",
    "        xplot = xc\n",
    "\n",
    "    ax.errorbar(xplot, ym, yerr=ye, fmt=\"o\", ms=3, lw=1, capsize=2, label=label)\n",
    "    ax.grid(alpha=0.3)\n",
    "    return {\"x\": xplot, \"mean\": ym, \"sem\": ye, \"count\": cnt[good]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1df498a0-5325-4e58-be19-cea785549dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_loss_plots(\n",
    "    ae_1,\n",
    "    ae_2,\n",
    "    dataset_test,                # single array OR (X1_test, X2_test)\n",
    "    reco_loss_fn1=None,\n",
    "    reco_loss_fn2=None,\n",
    "    device=None,\n",
    "    batch_size=2048,\n",
    "    outdir=\"plots\",\n",
    "    bins=200,\n",
    "    logy=False,\n",
    "    ae1_input_slicer=None,\n",
    "    ae2_input_slicer=None,\n",
    "    ae1_loss_slicer=None,\n",
    "    ae2_loss_slicer=None\n",
    "):\n",
    "    import os, numpy as np, torch, matplotlib.pyplot as plt\n",
    "    from matplotlib.colors import LogNorm\n",
    "\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    assert device is not None\n",
    "\n",
    "    do_ae1 = ae_1 is not None\n",
    "    do_ae2 = ae_2 is not None\n",
    "    assert do_ae1 or do_ae2\n",
    "\n",
    "    if do_ae1:\n",
    "        ae_1.eval(); assert reco_loss_fn1 is not None\n",
    "    if do_ae2:\n",
    "        ae_2.eval(); assert reco_loss_fn2 is not None\n",
    "\n",
    "    # Allow (X1_test, X2_test) OR a single dataset for both\n",
    "    if isinstance(dataset_test, (tuple, list)) and len(dataset_test) == 2:\n",
    "        ds1, ds2 = dataset_test\n",
    "    else:\n",
    "        ds1 = dataset_test\n",
    "        ds2 = dataset_test\n",
    "\n",
    "    losses_1, losses_2 = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if do_ae1 and ds1 is not None:\n",
    "            X1_full = torch.tensor(ds1, dtype=torch.float32, device=device)\n",
    "            for i in range(0, X1_full.size(0), batch_size):\n",
    "                xb_full = X1_full[i:i + batch_size]\n",
    "                xb1 = ae1_input_slicer(xb_full) if ae1_input_slicer else xb_full\n",
    "                recon1, _ = ae_1(xb1)\n",
    "                y1_pred, y1_true = recon1, xb1\n",
    "                if ae1_loss_slicer:\n",
    "                    y1_pred = ae1_loss_slicer(y1_pred); y1_true = ae1_loss_slicer(y1_true)\n",
    "                loss_1 = reco_loss_fn1(y1_pred, y1_true).detach().cpu().numpy().reshape(-1)\n",
    "                loss_1 = np.nan_to_num(loss_1, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                losses_1.append(loss_1)\n",
    "\n",
    "        if do_ae2 and ds2 is not None:\n",
    "            X2_full = torch.tensor(ds2, dtype=torch.float32, device=device)\n",
    "            for i in range(0, X2_full.size(0), batch_size):\n",
    "                xb_full = X2_full[i:i + batch_size]\n",
    "                xb2 = ae2_input_slicer(xb_full) if ae2_input_slicer else xb_full\n",
    "                recon2, _ = ae_2(xb2)\n",
    "                y2_pred, y2_true = recon2, xb2\n",
    "                if ae2_loss_slicer:\n",
    "                    y2_pred = ae2_loss_slicer(y2_pred); y2_true = ae2_loss_slicer(y2_true)\n",
    "                loss_2 = reco_loss_fn2(y2_pred, y2_true).detach().cpu().numpy().reshape(-1)\n",
    "                loss_2 = np.nan_to_num(loss_2, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                losses_2.append(loss_2)\n",
    "\n",
    "    losses_1_all = np.concatenate(losses_1, axis=0) if (do_ae1 and losses_1) else None\n",
    "    losses_2_all = np.concatenate(losses_2, axis=0) if (do_ae2 and losses_2) else None\n",
    "\n",
    "    # Save raw arrays\n",
    "    if losses_1_all is not None:\n",
    "        np.save(os.path.join(outdir, \"test_reco_loss_ae1.npy\"), losses_1_all)\n",
    "    if losses_2_all is not None:\n",
    "        np.save(os.path.join(outdir, \"test_reco_loss_ae2.npy\"), losses_2_all)\n",
    "\n",
    "    # Helper to plot one histogram\n",
    "    def _one_hist(arr, tag):\n",
    "        if arr is None or arr.size == 0:\n",
    "            print(f\"[WARN] No losses for {tag}; skipping histogram.\")\n",
    "            return None\n",
    "        mask = arr > 0\n",
    "        x = arr[mask]\n",
    "        if x.size == 0:\n",
    "            print(f\"[WARN] No positive losses for {tag}; skipping histogram.\")\n",
    "            return None\n",
    "        x_max = np.nextafter(float(x.max()), np.inf)\n",
    "        x_min = max(min(x.min(), x_max/1e6), 1e-12)\n",
    "        edges = np.logspace(np.log10(x_min), np.log10(x_max), bins + 1)\n",
    "        plt.figure(figsize=(7, 4))\n",
    "        plt.hist(x, bins=edges)\n",
    "        plt.xscale(\"log\")\n",
    "        if logy: plt.yscale(\"log\")\n",
    "        plt.xlabel(f\"{tag} reconstruction loss\"); plt.ylabel(\"Counts\")\n",
    "        plt.title(f\"{tag} reconstruction loss (Test)\")\n",
    "        plt.xlim(edges[0], edges[-1])\n",
    "        plt.tight_layout()\n",
    "        out_png = os.path.join(outdir, f\"hist_{tag}.png\")\n",
    "        plt.savefig(out_png, dpi=150); plt.close()\n",
    "        try:\n",
    "            import wandb; wandb.log({f\"Eval/hist_{tag}\": wandb.Image(out_png)})\n",
    "        except Exception:\n",
    "            pass\n",
    "        return edges\n",
    "\n",
    "    # Per-AE histograms\n",
    "    edges1 = _one_hist(losses_1_all, \"AE1\") if do_ae1 else None\n",
    "    edges2 = _one_hist(losses_2_all, \"AE2\") if do_ae2 else None\n",
    "\n",
    "    # Cross-AE plots only if BOTH exist and are aligned in length\n",
    "    if (losses_1_all is not None) and (losses_2_all is not None) and \\\n",
    "       (losses_1_all.shape[0] == losses_2_all.shape[0]):\n",
    "\n",
    "        mask = (losses_1_all > 0) & (losses_2_all > 0)\n",
    "        x1 = losses_1_all[mask]; x2 = losses_2_all[mask]\n",
    "\n",
    "        # Choose edges robustly if None (fallback)\n",
    "        if edges1 is None and x1.size:\n",
    "            x1_max = np.nextafter(float(x1.max()), np.inf)\n",
    "            x1_min = max(min(x1.min(), x1_max/1e6), 1e-12)\n",
    "            edges1 = np.logspace(np.log10(x1_min), np.log10(x1_max), bins + 1)\n",
    "        if edges2 is None and x2.size:\n",
    "            x2_max = np.nextafter(float(x2.max()), np.inf)\n",
    "            x2_min = max(min(x2.min(), x2_max/1e6), 1e-12)\n",
    "            edges2 = np.logspace(np.log10(x2_min), np.log10(x2_max), bins + 1)\n",
    "\n",
    "        # 2D hist\n",
    "        if x1.size and x2.size:\n",
    "            plt.figure(figsize=(6, 5))\n",
    "            plt.hist2d(x1, x2, bins=[edges1, edges2], norm=LogNorm(vmin=1), cmin=1)\n",
    "            plt.xscale(\"log\"); plt.yscale(\"log\")\n",
    "            plt.xlabel(\"AE1 reconstruction loss\"); plt.ylabel(\"AE2 reconstruction loss\")\n",
    "            plt.title(\"AE1 vs AE2 reconstruction loss (Test) — log-log\")\n",
    "            plt.xlim(edges1[0], edges1[-1]); plt.ylim(edges2[0], edges2[-1])\n",
    "            plt.colorbar(label=\"Counts (log)\")\n",
    "            plt.tight_layout()\n",
    "            out_png = os.path.join(outdir, \"hist2d_AE1_vs_AE2.png\")\n",
    "            plt.savefig(out_png, dpi=150); plt.close()\n",
    "            try:\n",
    "                import wandb; wandb.log({\"Eval/hist2d_AE1_vs_AE2\": wandb.Image(out_png)})\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Profile plots\n",
    "    if (losses_1_all is not None) and (losses_2_all is not None):\n",
    "        n = min(len(losses_1_all), len(losses_2_all))\n",
    "        x1 = losses_1_all[:n]\n",
    "        x2 = losses_2_all[:n]\n",
    "    \n",
    "        fig, ax = plt.subplots(figsize=(6.2, 4.6))\n",
    "        profile_plot(ax, x=x2, y=x1, nbins=100, logx=True, min_per_bin=50, label=\"mean ± SE\")\n",
    "        ax.set_title(\"Profile: ⟨AE1 loss⟩ vs AE2 loss (Test)\")\n",
    "        ax.set_xlabel(\"AE2 reconstruction loss\")\n",
    "        ax.set_ylabel(\"⟨AE1 reconstruction loss⟩\")\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.legend()\n",
    "        fig.tight_layout()\n",
    "        out_png = os.path.join(outdir, \"profile_AE1_vs_AE2.png\")\n",
    "        fig.savefig(out_png, dpi=150)\n",
    "        plt.close(fig)\n",
    "        try:\n",
    "            import wandb; wandb.log({\"Profiles/AE1_vs_AE2\": wandb.Image(out_png)})\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "        fig, ax = plt.subplots(figsize=(6.2, 4.6))\n",
    "        profile_plot(ax, x=x1, y=x2, nbins=100, logx=True, min_per_bin=50, label=\"mean ± SE\")\n",
    "        ax.set_title(\"Profile: ⟨AE2 loss⟩ vs AE1 loss (Test)\")\n",
    "        ax.set_xlabel(\"AE1 reconstruction loss\")\n",
    "        ax.set_ylabel(\"⟨AE2 reconstruction loss⟩\")\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.legend()\n",
    "        fig.tight_layout()\n",
    "        out_png = os.path.join(outdir, \"profile_AE2_vs_AE1.png\")\n",
    "        fig.savefig(out_png, dpi=150)\n",
    "        plt.close(fig)\n",
    "        try:\n",
    "            import wandb; wandb.log({\"Profiles/AE2_vs_AE1\": wandb.Image(out_png)})\n",
    "        except Exception:\n",
    "            pass\n",
    "                \n",
    "    return losses_1_all, losses_2_all\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "540a813e-5d61-4ce4-9593-d2ef7e03bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config):\n",
    "    seed = 123\n",
    "    set_seed(seed)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using: {device}\", flush=True)\n",
    "\n",
    "    print(\"Logging in to wandb...\", flush=True)\n",
    "    wandb.login(key=\"24d1d60ce26563c74d290d7b487cb104fc251271\")\n",
    "    wandb.init(project=\"Double Disco Axo Training\",\n",
    "               settings=wandb.Settings(_disable_stats=True),\n",
    "               config=config)\n",
    "    run_name = wandb.run.name\n",
    "    print(f\"Run name: {run_name}\", flush=True)\n",
    "\n",
    "    alpha = float(config['alpha'])\n",
    "    ae_lr = float(config['ae_lr'])\n",
    "    latent_dim = int(config['ae_latent'])\n",
    "    enc_nodes = list(config['ae_nodes'])\n",
    "    dec_nodes_template = [24, 32, 64, 128]\n",
    "\n",
    "    lambda_disco_max = float(config.get(\"lambda_disco\", 1.0))\n",
    "    lambda_closure_max = float(config.get(\"lambda_closure\", 0.0))\n",
    "    disco_warmup_epochs = int(config.get(\"disco_warmup_epochs\", 0))\n",
    "\n",
    "    #load data\n",
    "    data_path = '//axovol/HLT_data_nov_12_2024I.h5'\n",
    "    print(\"Loading dataset...\", flush=True)\n",
    "    with h5.File(data_path, 'r') as f:\n",
    "        r = f['data'] if 'data' in f else f\n",
    "\n",
    "        print(\"H5 tree:\")\n",
    "        print_h5_tree(r)\n",
    "\n",
    "        x_train = r['Background_data']['Train']['DATA'][:]\n",
    "        x_test = r['Background_data']['Test']['DATA'][:]\n",
    "\n",
    "        print(f\"Train shape: {x_train.shape}, Test shape: {x_test.shape}\", flush=True)\n",
    "\n",
    "        Xtr_raw = x_train\n",
    "        Xte_raw = x_test\n",
    "\n",
    "        # zero out padding objects\n",
    "        pad_tr = (x_train == 0.0).all(axis=-1)\n",
    "        pad_te = (x_test == 0.0).all(axis=-1)\n",
    "        Xtr_raw[pad_tr] = 0.0\n",
    "        Xte_raw[pad_te] = 0.0\n",
    "\n",
    "        #slots for objects\n",
    "        SLOTS = {\n",
    "            \"ELECTRONS\": (0, 4),\n",
    "            \"MUONS\": (4, 8),\n",
    "            \"PHOTONS\": (8, 12),\n",
    "            \"JETS\": (12, 22),\n",
    "            \"FATJETS\": (22, 32),\n",
    "            \"MET\": (32, 33),\n",
    "        }\n",
    "\n",
    "        def _slice_slots(x, start, end):\n",
    "            n, _, fdim = x.shape\n",
    "            return x[:, start:end, :].reshape(n, (end - start) * fdim)\n",
    "\n",
    "        def _take_groups(x, groups):\n",
    "            parts = [_slice_slots(x, *SLOTS[g]) for g in groups]\n",
    "            return np.concatenate(parts, axis=1) if len(parts) > 1 else parts[0]\n",
    "\n",
    "        # AE-1 = jets + fatjets + MET, AE-2 = leptons + photons\n",
    "        X1_train = _take_groups(Xtr_raw, [\"JETS\", \"FATJETS\", \"MET\"])\n",
    "        X1_test = _take_groups(Xte_raw, [\"JETS\", \"FATJETS\", \"MET\"])\n",
    "        X2_train = _take_groups(Xtr_raw, [\"ELECTRONS\", \"MUONS\", \"PHOTONS\"])\n",
    "        X2_test = _take_groups(Xte_raw, [\"ELECTRONS\", \"MUONS\", \"PHOTONS\"])\n",
    "\n",
    "        # keep copies for later plots/inference\n",
    "        X1_train_raw, X1_test_raw = X1_train, X1_test\n",
    "        X2_train_raw, X2_test_raw = X2_train, X2_test\n",
    "\n",
    "        # feature-wise standardization\n",
    "        mu1, std1 = fit_standard_scaler(X1_train_raw)\n",
    "        mu2, std2 = fit_standard_scaler(X2_train_raw)\n",
    "\n",
    "        X1_train_z = transform_standard(X1_train_raw, mu1, std1)\n",
    "        X1_test_z = transform_standard(X1_test_raw,  mu1, std1)\n",
    "        X2_train_z = transform_standard(X2_train_raw, mu2, std2)\n",
    "        X2_test_z = transform_standard(X2_test_raw,  mu2, std2)\n",
    "\n",
    "    feat1 = X1_train.shape[1]\n",
    "    feat2 = X2_train.shape[1]\n",
    "\n",
    "    reco_loss_fn1 = PerSampleMSE().to(device)\n",
    "    reco_loss_fn2 = PerSampleMSE().to(device)\n",
    "    print(\"Loss functions ready.\", flush=True)\n",
    "\n",
    "    ae1_cfg = {\n",
    "        \"features\": feat1,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"encoder_config\": {\"nodes\": enc_nodes},\n",
    "        \"decoder_config\": {\"nodes\": dec_nodes_template + [feat1]},\n",
    "        \"alpha\": alpha\n",
    "    }\n",
    "    ae2_cfg = {\n",
    "        \"features\": feat2,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"encoder_config\": {\"nodes\": enc_nodes},\n",
    "        \"decoder_config\": {\"nodes\": dec_nodes_template + [feat2]},\n",
    "        \"alpha\": alpha\n",
    "    }\n",
    "\n",
    "    ae_1 = Autoencoder(ae1_cfg).to(device)\n",
    "    ae_2 = Autoencoder(ae2_cfg).to(device)\n",
    "    print(\"Autoencoders are ready.\", flush=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(ae_1.parameters()) + list(ae_2.parameters()),\n",
    "        lr=ae_lr\n",
    "    )\n",
    "\n",
    "    warmup_epochs = 10  \n",
    "    cos = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=32, T_mult=2, eta_min=0.0\n",
    "    )\n",
    "\n",
    "    def set_lr(lr):\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "\n",
    "    Epochs_AE = 100\n",
    "    Batch_size = 2048\n",
    "\n",
    "    print(\"Moving data to device...\", flush=True)\n",
    "    X1 = torch.tensor(X1_train_z, dtype=torch.float32, device=device)\n",
    "    X2 = torch.tensor(X2_train_z, dtype=torch.float32, device=device)\n",
    "    print(\"Data on device.\", flush=True)\n",
    "\n",
    "    #training loop\n",
    "    print(\"Starting the training loop!\", flush=True)\n",
    "    N1 = X1.size(0)\n",
    "    N2 = X2.size(0)\n",
    "    Nmin = min(N1, N2)\n",
    "\n",
    "    for epoch in range(Epochs_AE):\n",
    "        ae1_reco_loss = []\n",
    "        ae2_reco_loss = []\n",
    "\n",
    "        # LR warmup\n",
    "        if epoch < warmup_epochs:\n",
    "            lr = ae_lr * (epoch + 1) / warmup_epochs\n",
    "            set_lr(lr)\n",
    "        else:\n",
    "            cos.step(epoch - warmup_epochs)\n",
    "\n",
    "        #disco warmup\n",
    "        if disco_warmup_epochs > 0 and epoch < disco_warmup_epochs:\n",
    "            ramp = (epoch + 1) / disco_warmup_epochs\n",
    "            lambda_disco = lambda_disco_max * ramp\n",
    "            lambda_closure = lambda_closure_max * ramp\n",
    "        else:\n",
    "            lambda_disco = lambda_disco_max\n",
    "            lambda_closure = lambda_closure_max\n",
    "\n",
    "        #perm1 = torch.randperm(N1, device=device)\n",
    "        perm = torch.randperm(Nmin, device=device)\n",
    "\n",
    "        num_batches = 0\n",
    "        total_loss = total_reco1 = total_reco2 = total_disco  = total_closure = 0.0\n",
    "\n",
    "        for i0 in range(0, Nmin, Batch_size):\n",
    "            i1 = min(i0 + Batch_size, Nmin)\n",
    "            bsz = i1 - i0\n",
    "\n",
    "            idx = perm[i0:i1]\n",
    "\n",
    "            xb1 = X1[idx]\n",
    "            xb2 = X2[idx]\n",
    "\n",
    "            recon1, z1 = ae_1(xb1)\n",
    "            recon2, z2 = ae_2(xb2)\n",
    "\n",
    "            # per-event reconstruction losses\n",
    "            reco1_per = reco_loss_fn1(recon1, xb1)\n",
    "            reco2_per = reco_loss_fn2(recon2, xb2)\n",
    "\n",
    "            w = torch.ones(len(idx), device=device, dtype=reco1_per.dtype)\n",
    "\n",
    "            #disco\n",
    "            disco = distance_corr(reco1_per, reco2_per, w, power=1)\n",
    "\n",
    "            #ABCD closure loss on this batch (AE1_loss vs AE2_loss)\n",
    "            closure = closure_loss_batch(\n",
    "                reco1_per,\n",
    "                reco2_per,\n",
    "                w,\n",
    "                symmetrize=True,\n",
    "                n_events_min=10,\n",
    "            )\n",
    "\n",
    "            #mean reconstruction losses\n",
    "            reco1 = ae_1.alpha * reco1_per.mean()\n",
    "            reco2 = ae_2.alpha * reco2_per.mean()\n",
    "\n",
    "            #total loss with warmup scaled lambdas\n",
    "            loss = reco1 + reco2 + lambda_disco * disco +lambda_closure*closure\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(ae_1.parameters()) + list(ae_2.parameters()), max_norm=5.0\n",
    "            )\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss  += loss.item()\n",
    "            total_reco1 += reco1.item()\n",
    "            total_reco2 += reco2.item()\n",
    "            total_disco += disco.item()\n",
    "            total_closure += closure.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            ae1_reco_loss.append(reco1.item())\n",
    "            ae2_reco_loss.append(reco2.item())\n",
    "\n",
    "        avg_loss = total_loss/max(1, num_batches)\n",
    "        avg_reco1 = total_reco1/max(1, num_batches)\n",
    "        avg_reco2 = total_reco2/max(1, num_batches)\n",
    "        avg_disco = total_disco/max(1, num_batches)\n",
    "        avg_closure = total_closure/max(1, num_batches)\n",
    "\n",
    "        print(f\"[EPOCH {epoch}/{Epochs_AE}] \"\n",
    "              f\"Loss={avg_loss:.4f} \"\n",
    "              f\"Reco1(jets)={avg_reco1:.4f} Reco2(lep/photon/MET)={avg_reco2:.4f} \"\n",
    "              f\"DisCo={avg_disco:.4f} \"\n",
    "              f\"Closure={avg_closure:4f}\"\n",
    "              f\"(lambda_dis={lambda_disco:.3g},flush=True)\")\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"TotalLoss\": avg_loss,\n",
    "            \"RecoLoss_AE1_jets\": avg_reco1,\n",
    "            \"RecoLoss_AE2_lepPhotMET\": avg_reco2,\n",
    "            \"DisCoLoss\": avg_disco,\n",
    "            \"ClosureLoss\": avg_closure,\n",
    "            \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "            \"lambda_disco\": lambda_disco,\n",
    "            \"lambda_closure\": lambda_closure,\n",
    "        })\n",
    "\n",
    "        ae1_reco_np = np.array(ae1_reco_loss)\n",
    "        ae2_reco_np = np.array(ae2_reco_loss)\n",
    "\n",
    "        make_2D_hist(ae1_reco_np, ae2_reco_np,\n",
    "                     \"Reco Loss (jets)\", \"Reco Loss (lep/photon/MET)\",\n",
    "                     f\"Epoch {epoch}: Reco jets vs Reco lep/photon/MET\",\n",
    "                     wandb_key=\"Hists2D/Reco_jets_vs_Reco_lepPhotMET\")\n",
    "\n",
    "    print(\"Finished training.\", flush=True)\n",
    "\n",
    "    # save models\n",
    "    torch.save(ae_1.state_dict(), \"ae1_trained_jets.pth\")\n",
    "    torch.save(ae_2.state_dict(), \"ae2_trained_lepPhotMET.pth\")\n",
    "\n",
    "    # tensors for inference normalisation\n",
    "    mu1_t = torch.tensor(mu1, dtype=torch.float32, device=device)\n",
    "    std1_t = torch.tensor(std1, dtype=torch.float32, device=device)\n",
    "    mu2_t = torch.tensor(mu2, dtype=torch.float32, device=device)\n",
    "    std2_t = torch.tensor(std2, dtype=torch.float32, device=device)\n",
    "    \n",
    "    def ae1_input_from_raw(t_flat: torch.Tensor):\n",
    "        return (t_flat - mu1_t) / (std1_t + 1e-8)\n",
    "    \n",
    "    def ae2_input_from_raw(t_flat: torch.Tensor):\n",
    "        return (t_flat - mu2_t) / (std2_t + 1e-8)\n",
    "\n",
    "    _ae1_losses, _ae2_losses = inference_loss_plots(\n",
    "        ae_1=ae_1,\n",
    "        ae_2=ae_2,\n",
    "        dataset_test=(X1_test_raw, X2_test_raw),\n",
    "        reco_loss_fn1=reco_loss_fn1,\n",
    "        reco_loss_fn2=reco_loss_fn2,\n",
    "        device=device,\n",
    "        batch_size=2048,\n",
    "        outdir=\"plots_profiles_same_events\",\n",
    "        bins=200,\n",
    "        logy=False,\n",
    "        ae1_input_slicer=ae1_input_from_raw,\n",
    "        ae2_input_slicer=ae2_input_from_raw,\n",
    "        ae1_loss_slicer=None,\n",
    "        ae2_loss_slicer=None\n",
    "    )\n",
    "        \n",
    "    return {\n",
    "        \"ae_1\": ae_1,\n",
    "        \"ae_2\": ae_2,\n",
    "        \"X1_test_z\": X1_test_z,\n",
    "        \"X2_test_z\": X2_test_z,\n",
    "        \"reco_loss_fn1\": reco_loss_fn1,\n",
    "        \"reco_loss_fn2\": reco_loss_fn2,\n",
    "        \"device\": device,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8160a1fc-439a-429f-a154-7c134e0ecfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'ae_lr': 1e-4,\n",
    "    'alpha': 0.5,\n",
    "    'ae_latent': 8,\n",
    "    'ae_nodes': [28, 14],\n",
    "    'lambda_disco': 100.0,     \n",
    "    'lambda_closure': 1,     \n",
    "    'disco_warmup_epochs': 0,  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9c7fffe-b068-4905-ba33-bd2407037e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f82cc7ec-0f8c-4f91-bc28-9bad851f8d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n",
      "Logging in to wandb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jovyan/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mescheuller\u001b[0m (\u001b[33mescheuller-uc-san-diego\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20251119_094928-zoaj2o9z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/zoaj2o9z' target=\"_blank\">distinctive-shadow-302</a></strong> to <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training' target=\"_blank\">https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/zoaj2o9z' target=\"_blank\">https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/zoaj2o9z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: distinctive-shadow-302\n",
      "Loading dataset...\n",
      "H5 tree:\n",
      "[GROUP] Background_data\n",
      "  [GROUP] Test\n",
      "    DATA: shape=(284221, 33, 15), dtype=float32\n",
      "  [GROUP] Train\n",
      "    DATA: shape=(1136883, 33, 15), dtype=float32\n",
      "Train shape: (1136883, 33, 15), Test shape: (284221, 33, 15)\n",
      "Loss functions ready.\n",
      "Autoencoders are ready.\n",
      "Moving data to device...\n",
      "Data on device.\n",
      "Starting the training loop!\n",
      "[EPOCH 0/100] Loss=4.1854 Reco1(jets)=0.3334 Reco2(lep/photon/MET)=0.1381 DisCo=0.0333 Closure=0.383814(lambda_dis=100,flush=True)\n",
      "[EPOCH 2/100] Loss=0.8855 Reco1(jets)=0.3919 Reco2(lep/photon/MET)=0.1742 DisCo=0.0022 Closure=0.094937(lambda_dis=100,flush=True)\n",
      "[EPOCH 3/100] Loss=0.8535 Reco1(jets)=0.3742 Reco2(lep/photon/MET)=0.1707 DisCo=0.0021 Closure=0.102570(lambda_dis=100,flush=True)\n",
      "[EPOCH 4/100] Loss=0.8024 Reco1(jets)=0.3511 Reco2(lep/photon/MET)=0.1651 DisCo=0.0019 Closure=0.095530(lambda_dis=100,flush=True)\n",
      "[EPOCH 5/100] Loss=0.7601 Reco1(jets)=0.3230 Reco2(lep/photon/MET)=0.1670 DisCo=0.0018 Closure=0.093228(lambda_dis=100,flush=True)\n",
      "[EPOCH 6/100] Loss=0.7435 Reco1(jets)=0.3063 Reco2(lep/photon/MET)=0.1622 DisCo=0.0018 Closure=0.099825(lambda_dis=100,flush=True)\n",
      "[EPOCH 7/100] Loss=0.7136 Reco1(jets)=0.2944 Reco2(lep/photon/MET)=0.1591 DisCo=0.0017 Closure=0.088861(lambda_dis=100,flush=True)\n",
      "[EPOCH 8/100] Loss=0.7035 Reco1(jets)=0.2760 Reco2(lep/photon/MET)=0.1577 DisCo=0.0017 Closure=0.096848(lambda_dis=100,flush=True)\n",
      "[EPOCH 9/100] Loss=0.6801 Reco1(jets)=0.2643 Reco2(lep/photon/MET)=0.1551 DisCo=0.0017 Closure=0.089850(lambda_dis=100,flush=True)\n",
      "[EPOCH 10/100] Loss=0.6755 Reco1(jets)=0.2582 Reco2(lep/photon/MET)=0.1484 DisCo=0.0017 Closure=0.094897(lambda_dis=100,flush=True)\n",
      "[EPOCH 11/100] Loss=0.6670 Reco1(jets)=0.2559 Reco2(lep/photon/MET)=0.1472 DisCo=0.0017 Closure=0.089609(lambda_dis=100,flush=True)\n",
      "[EPOCH 12/100] Loss=0.6598 Reco1(jets)=0.2537 Reco2(lep/photon/MET)=0.1400 DisCo=0.0017 Closure=0.093054(lambda_dis=100,flush=True)\n",
      "[EPOCH 13/100] Loss=0.6522 Reco1(jets)=0.2444 Reco2(lep/photon/MET)=0.1446 DisCo=0.0017 Closure=0.090179(lambda_dis=100,flush=True)\n",
      "[EPOCH 14/100] Loss=0.6453 Reco1(jets)=0.2429 Reco2(lep/photon/MET)=0.1424 DisCo=0.0017 Closure=0.088922(lambda_dis=100,flush=True)\n",
      "[EPOCH 15/100] Loss=0.6436 Reco1(jets)=0.2390 Reco2(lep/photon/MET)=0.1427 DisCo=0.0017 Closure=0.091151(lambda_dis=100,flush=True)\n",
      "[EPOCH 16/100] Loss=0.6287 Reco1(jets)=0.2382 Reco2(lep/photon/MET)=0.1428 DisCo=0.0016 Closure=0.084936(lambda_dis=100,flush=True)\n",
      "[EPOCH 17/100] Loss=0.6307 Reco1(jets)=0.2338 Reco2(lep/photon/MET)=0.1455 DisCo=0.0016 Closure=0.087516(lambda_dis=100,flush=True)\n",
      "[EPOCH 18/100] Loss=0.6290 Reco1(jets)=0.2312 Reco2(lep/photon/MET)=0.1438 DisCo=0.0016 Closure=0.090285(lambda_dis=100,flush=True)\n",
      "[EPOCH 19/100] Loss=0.6220 Reco1(jets)=0.2331 Reco2(lep/photon/MET)=0.1360 DisCo=0.0016 Closure=0.090846(lambda_dis=100,flush=True)\n",
      "[EPOCH 20/100] Loss=0.6199 Reco1(jets)=0.2329 Reco2(lep/photon/MET)=0.1332 DisCo=0.0017 Closure=0.086004(lambda_dis=100,flush=True)\n",
      "[EPOCH 21/100] Loss=0.6045 Reco1(jets)=0.2282 Reco2(lep/photon/MET)=0.1320 DisCo=0.0016 Closure=0.079357(lambda_dis=100,flush=True)\n",
      "[EPOCH 22/100] Loss=0.6044 Reco1(jets)=0.2285 Reco2(lep/photon/MET)=0.1352 DisCo=0.0016 Closure=0.081898(lambda_dis=100,flush=True)\n",
      "[EPOCH 23/100] Loss=0.6155 Reco1(jets)=0.2283 Reco2(lep/photon/MET)=0.1300 DisCo=0.0017 Closure=0.089377(lambda_dis=100,flush=True)\n",
      "[EPOCH 24/100] Loss=0.6038 Reco1(jets)=0.2255 Reco2(lep/photon/MET)=0.1277 DisCo=0.0016 Closure=0.086571(lambda_dis=100,flush=True)\n",
      "[EPOCH 25/100] Loss=0.6024 Reco1(jets)=0.2242 Reco2(lep/photon/MET)=0.1304 DisCo=0.0017 Closure=0.082047(lambda_dis=100,flush=True)\n",
      "[EPOCH 26/100] Loss=0.6065 Reco1(jets)=0.2242 Reco2(lep/photon/MET)=0.1315 DisCo=0.0016 Closure=0.089373(lambda_dis=100,flush=True)\n",
      "[EPOCH 27/100] Loss=0.5989 Reco1(jets)=0.2230 Reco2(lep/photon/MET)=0.1328 DisCo=0.0016 Closure=0.087067(lambda_dis=100,flush=True)\n",
      "[EPOCH 28/100] Loss=0.6049 Reco1(jets)=0.2219 Reco2(lep/photon/MET)=0.1315 DisCo=0.0016 Closure=0.090535(lambda_dis=100,flush=True)\n",
      "[EPOCH 29/100] Loss=0.5859 Reco1(jets)=0.2222 Reco2(lep/photon/MET)=0.1308 DisCo=0.0015 Closure=0.080690(lambda_dis=100,flush=True)\n",
      "[EPOCH 30/100] Loss=0.5892 Reco1(jets)=0.2217 Reco2(lep/photon/MET)=0.1320 DisCo=0.0015 Closure=0.081657(lambda_dis=100,flush=True)\n",
      "[EPOCH 31/100] Loss=0.5949 Reco1(jets)=0.2221 Reco2(lep/photon/MET)=0.1304 DisCo=0.0016 Closure=0.084429(lambda_dis=100,flush=True)\n",
      "[EPOCH 32/100] Loss=0.5981 Reco1(jets)=0.2214 Reco2(lep/photon/MET)=0.1290 DisCo=0.0016 Closure=0.085926(lambda_dis=100,flush=True)\n",
      "[EPOCH 33/100] Loss=0.5924 Reco1(jets)=0.2212 Reco2(lep/photon/MET)=0.1283 DisCo=0.0016 Closure=0.081696(lambda_dis=100,flush=True)\n",
      "[EPOCH 34/100] Loss=0.5890 Reco1(jets)=0.2205 Reco2(lep/photon/MET)=0.1272 DisCo=0.0016 Closure=0.082977(lambda_dis=100,flush=True)\n",
      "[EPOCH 35/100] Loss=0.5839 Reco1(jets)=0.2199 Reco2(lep/photon/MET)=0.1279 DisCo=0.0016 Closure=0.079691(lambda_dis=100,flush=True)\n",
      "[EPOCH 36/100] Loss=0.5921 Reco1(jets)=0.2199 Reco2(lep/photon/MET)=0.1280 DisCo=0.0016 Closure=0.087188(lambda_dis=100,flush=True)\n",
      "[EPOCH 37/100] Loss=0.5905 Reco1(jets)=0.2202 Reco2(lep/photon/MET)=0.1287 DisCo=0.0016 Closure=0.085339(lambda_dis=100,flush=True)\n",
      "[EPOCH 38/100] Loss=0.5976 Reco1(jets)=0.2198 Reco2(lep/photon/MET)=0.1288 DisCo=0.0016 Closure=0.088711(lambda_dis=100,flush=True)\n",
      "[EPOCH 39/100] Loss=0.5936 Reco1(jets)=0.2195 Reco2(lep/photon/MET)=0.1285 DisCo=0.0016 Closure=0.085516(lambda_dis=100,flush=True)\n",
      "[EPOCH 40/100] Loss=0.5953 Reco1(jets)=0.2195 Reco2(lep/photon/MET)=0.1286 DisCo=0.0016 Closure=0.084504(lambda_dis=100,flush=True)\n",
      "[EPOCH 41/100] Loss=0.5827 Reco1(jets)=0.2193 Reco2(lep/photon/MET)=0.1286 DisCo=0.0016 Closure=0.079026(lambda_dis=100,flush=True)\n",
      "[EPOCH 42/100] Loss=0.6038 Reco1(jets)=0.2207 Reco2(lep/photon/MET)=0.1296 DisCo=0.0016 Closure=0.091855(lambda_dis=100,flush=True)\n",
      "[EPOCH 43/100] Loss=0.5948 Reco1(jets)=0.2218 Reco2(lep/photon/MET)=0.1294 DisCo=0.0016 Closure=0.083428(lambda_dis=100,flush=True)\n",
      "[EPOCH 44/100] Loss=0.5961 Reco1(jets)=0.2195 Reco2(lep/photon/MET)=0.1304 DisCo=0.0016 Closure=0.083525(lambda_dis=100,flush=True)\n",
      "[EPOCH 45/100] Loss=0.5945 Reco1(jets)=0.2182 Reco2(lep/photon/MET)=0.1279 DisCo=0.0016 Closure=0.085356(lambda_dis=100,flush=True)\n",
      "[EPOCH 46/100] Loss=0.5852 Reco1(jets)=0.2172 Reco2(lep/photon/MET)=0.1251 DisCo=0.0016 Closure=0.081959(lambda_dis=100,flush=True)\n",
      "[EPOCH 47/100] Loss=0.5930 Reco1(jets)=0.2172 Reco2(lep/photon/MET)=0.1273 DisCo=0.0016 Closure=0.089636(lambda_dis=100,flush=True)\n",
      "[EPOCH 48/100] Loss=0.5912 Reco1(jets)=0.2152 Reco2(lep/photon/MET)=0.1254 DisCo=0.0016 Closure=0.088642(lambda_dis=100,flush=True)\n",
      "[EPOCH 49/100] Loss=0.5809 Reco1(jets)=0.2132 Reco2(lep/photon/MET)=0.1252 DisCo=0.0016 Closure=0.083390(lambda_dis=100,flush=True)\n",
      "[EPOCH 50/100] Loss=0.5990 Reco1(jets)=0.2155 Reco2(lep/photon/MET)=0.1283 DisCo=0.0017 Closure=0.089987(lambda_dis=100,flush=True)\n",
      "[EPOCH 51/100] Loss=0.5746 Reco1(jets)=0.2146 Reco2(lep/photon/MET)=0.1237 DisCo=0.0015 Closure=0.082750(lambda_dis=100,flush=True)\n",
      "[EPOCH 53/100] Loss=0.5785 Reco1(jets)=0.2147 Reco2(lep/photon/MET)=0.1264 DisCo=0.0015 Closure=0.083210(lambda_dis=100,flush=True)\n",
      "[EPOCH 54/100] Loss=0.5881 Reco1(jets)=0.2132 Reco2(lep/photon/MET)=0.1256 DisCo=0.0016 Closure=0.087859(lambda_dis=100,flush=True)\n",
      "[EPOCH 55/100] Loss=0.5776 Reco1(jets)=0.2137 Reco2(lep/photon/MET)=0.1221 DisCo=0.0016 Closure=0.085969(lambda_dis=100,flush=True)\n",
      "[EPOCH 56/100] Loss=0.5770 Reco1(jets)=0.2119 Reco2(lep/photon/MET)=0.1182 DisCo=0.0016 Closure=0.085483(lambda_dis=100,flush=True)\n",
      "[EPOCH 57/100] Loss=0.5821 Reco1(jets)=0.2119 Reco2(lep/photon/MET)=0.1198 DisCo=0.0016 Closure=0.089385(lambda_dis=100,flush=True)\n",
      "[EPOCH 58/100] Loss=0.5883 Reco1(jets)=0.2091 Reco2(lep/photon/MET)=0.1192 DisCo=0.0017 Closure=0.092844(lambda_dis=100,flush=True)\n",
      "[EPOCH 59/100] Loss=0.5687 Reco1(jets)=0.2080 Reco2(lep/photon/MET)=0.1221 DisCo=0.0015 Closure=0.085103(lambda_dis=100,flush=True)\n",
      "[EPOCH 60/100] Loss=0.5691 Reco1(jets)=0.2073 Reco2(lep/photon/MET)=0.1208 DisCo=0.0015 Closure=0.087956(lambda_dis=100,flush=True)\n",
      "[EPOCH 62/100] Loss=0.5683 Reco1(jets)=0.2064 Reco2(lep/photon/MET)=0.1205 DisCo=0.0016 Closure=0.085179(lambda_dis=100,flush=True)\n",
      "[EPOCH 63/100] Loss=0.5706 Reco1(jets)=0.2062 Reco2(lep/photon/MET)=0.1203 DisCo=0.0016 Closure=0.085061(lambda_dis=100,flush=True)\n",
      "[EPOCH 64/100] Loss=0.5687 Reco1(jets)=0.2090 Reco2(lep/photon/MET)=0.1192 DisCo=0.0015 Closure=0.085767(lambda_dis=100,flush=True)\n",
      "[EPOCH 65/100] Loss=0.5587 Reco1(jets)=0.2068 Reco2(lep/photon/MET)=0.1198 DisCo=0.0015 Closure=0.080878(lambda_dis=100,flush=True)\n",
      "[EPOCH 66/100] Loss=0.5614 Reco1(jets)=0.2080 Reco2(lep/photon/MET)=0.1202 DisCo=0.0015 Closure=0.082242(lambda_dis=100,flush=True)\n",
      "[EPOCH 67/100] Loss=0.5644 Reco1(jets)=0.2050 Reco2(lep/photon/MET)=0.1220 DisCo=0.0015 Closure=0.085153(lambda_dis=100,flush=True)\n",
      "[EPOCH 68/100] Loss=0.5597 Reco1(jets)=0.2047 Reco2(lep/photon/MET)=0.1208 DisCo=0.0015 Closure=0.082198(lambda_dis=100,flush=True)\n",
      "[EPOCH 69/100] Loss=0.5613 Reco1(jets)=0.2047 Reco2(lep/photon/MET)=0.1172 DisCo=0.0015 Closure=0.086201(lambda_dis=100,flush=True)\n",
      "[EPOCH 71/100] Loss=0.5555 Reco1(jets)=0.2016 Reco2(lep/photon/MET)=0.1185 DisCo=0.0015 Closure=0.082217(lambda_dis=100,flush=True)\n",
      "[EPOCH 72/100] Loss=0.5559 Reco1(jets)=0.2016 Reco2(lep/photon/MET)=0.1207 DisCo=0.0015 Closure=0.084402(lambda_dis=100,flush=True)\n",
      "[EPOCH 73/100] Loss=0.5525 Reco1(jets)=0.2012 Reco2(lep/photon/MET)=0.1193 DisCo=0.0015 Closure=0.084069(lambda_dis=100,flush=True)\n",
      "[EPOCH 74/100] Loss=0.5497 Reco1(jets)=0.2002 Reco2(lep/photon/MET)=0.1172 DisCo=0.0015 Closure=0.083070(lambda_dis=100,flush=True)\n",
      "[EPOCH 75/100] Loss=0.5531 Reco1(jets)=0.2005 Reco2(lep/photon/MET)=0.1186 DisCo=0.0015 Closure=0.081637(lambda_dis=100,flush=True)\n",
      "[EPOCH 76/100] Loss=0.5540 Reco1(jets)=0.1992 Reco2(lep/photon/MET)=0.1173 DisCo=0.0015 Closure=0.088317(lambda_dis=100,flush=True)\n",
      "[EPOCH 77/100] Loss=0.5545 Reco1(jets)=0.1988 Reco2(lep/photon/MET)=0.1157 DisCo=0.0015 Closure=0.085530(lambda_dis=100,flush=True)\n",
      "[EPOCH 78/100] Loss=0.5502 Reco1(jets)=0.1991 Reco2(lep/photon/MET)=0.1172 DisCo=0.0015 Closure=0.083392(lambda_dis=100,flush=True)\n",
      "[EPOCH 79/100] Loss=0.5507 Reco1(jets)=0.1985 Reco2(lep/photon/MET)=0.1155 DisCo=0.0015 Closure=0.082934(lambda_dis=100,flush=True)\n",
      "[EPOCH 80/100] Loss=0.5468 Reco1(jets)=0.1979 Reco2(lep/photon/MET)=0.1167 DisCo=0.0015 Closure=0.083772(lambda_dis=100,flush=True)\n",
      "[EPOCH 81/100] Loss=0.5547 Reco1(jets)=0.1981 Reco2(lep/photon/MET)=0.1186 DisCo=0.0015 Closure=0.086510(lambda_dis=100,flush=True)\n",
      "[EPOCH 82/100] Loss=0.5503 Reco1(jets)=0.1973 Reco2(lep/photon/MET)=0.1192 DisCo=0.0015 Closure=0.085205(lambda_dis=100,flush=True)\n",
      "[EPOCH 83/100] Loss=0.5509 Reco1(jets)=0.1969 Reco2(lep/photon/MET)=0.1199 DisCo=0.0015 Closure=0.085930(lambda_dis=100,flush=True)\n",
      "[EPOCH 84/100] Loss=0.5492 Reco1(jets)=0.1968 Reco2(lep/photon/MET)=0.1212 DisCo=0.0015 Closure=0.085655(lambda_dis=100,flush=True)\n",
      "[EPOCH 85/100] Loss=0.5452 Reco1(jets)=0.1968 Reco2(lep/photon/MET)=0.1197 DisCo=0.0015 Closure=0.083039(lambda_dis=100,flush=True)\n",
      "[EPOCH 86/100] Loss=0.5421 Reco1(jets)=0.1968 Reco2(lep/photon/MET)=0.1182 DisCo=0.0015 Closure=0.080139(lambda_dis=100,flush=True)\n",
      "[EPOCH 87/100] Loss=0.5457 Reco1(jets)=0.1962 Reco2(lep/photon/MET)=0.1183 DisCo=0.0015 Closure=0.082782(lambda_dis=100,flush=True)\n",
      "[EPOCH 88/100] Loss=0.5441 Reco1(jets)=0.1962 Reco2(lep/photon/MET)=0.1181 DisCo=0.0015 Closure=0.082223(lambda_dis=100,flush=True)\n",
      "[EPOCH 90/100] Loss=0.5419 Reco1(jets)=0.1964 Reco2(lep/photon/MET)=0.1171 DisCo=0.0014 Closure=0.084612(lambda_dis=100,flush=True)\n",
      "[EPOCH 91/100] Loss=0.5357 Reco1(jets)=0.1963 Reco2(lep/photon/MET)=0.1170 DisCo=0.0014 Closure=0.078897(lambda_dis=100,flush=True)\n",
      "[EPOCH 92/100] Loss=0.5528 Reco1(jets)=0.1959 Reco2(lep/photon/MET)=0.1158 DisCo=0.0015 Closure=0.086911(lambda_dis=100,flush=True)\n",
      "[EPOCH 93/100] Loss=0.5389 Reco1(jets)=0.1956 Reco2(lep/photon/MET)=0.1158 DisCo=0.0014 Closure=0.083734(lambda_dis=100,flush=True)\n",
      "[EPOCH 94/100] Loss=0.5411 Reco1(jets)=0.1954 Reco2(lep/photon/MET)=0.1160 DisCo=0.0015 Closure=0.083089(lambda_dis=100,flush=True)\n",
      "[EPOCH 95/100] Loss=0.5408 Reco1(jets)=0.1956 Reco2(lep/photon/MET)=0.1158 DisCo=0.0015 Closure=0.079768(lambda_dis=100,flush=True)\n",
      "[EPOCH 96/100] Loss=0.5411 Reco1(jets)=0.1951 Reco2(lep/photon/MET)=0.1158 DisCo=0.0015 Closure=0.084234(lambda_dis=100,flush=True)\n",
      "[EPOCH 97/100] Loss=0.5449 Reco1(jets)=0.1952 Reco2(lep/photon/MET)=0.1164 DisCo=0.0014 Closure=0.088670(lambda_dis=100,flush=True)\n",
      "[EPOCH 98/100] Loss=0.5358 Reco1(jets)=0.1952 Reco2(lep/photon/MET)=0.1164 DisCo=0.0014 Closure=0.081372(lambda_dis=100,flush=True)\n",
      "[EPOCH 99/100] Loss=0.5368 Reco1(jets)=0.1954 Reco2(lep/photon/MET)=0.1164 DisCo=0.0014 Closure=0.083766(lambda_dis=100,flush=True)\n",
      "Finished training.\n",
      "Optimized percentiles: percent_1=0.850, percent_2=0.770\n",
      "Optimized thresholds: threshold 1=0.511447, threshold  2=0.250961\n",
      "ABCD counts: A=9806, B=32827, C=55565, D=186023\n",
      "Predicted A: N_A_hat=9805.413\n",
      "Non-closure: 0.01%  ((A - A_hat)/A_hat)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#run and store the returned stuff here\n",
    "training_vars = run(config)\n",
    "\n",
    "#store things we need\n",
    "ae_1 = training_vars[\"ae_1\"]\n",
    "ae_2 = training_vars[\"ae_2\"]\n",
    "X1_test_z = training_vars[\"X1_test_z\"]\n",
    "X2_test_z = training_vars[\"X2_test_z\"]\n",
    "reco_loss_fn1 = training_vars[\"reco_loss_fn1\"]\n",
    "reco_loss_fn2 = training_vars[\"reco_loss_fn2\"]\n",
    "device = training_vars[\"device\"]\n",
    "\n",
    "#inference of test set\n",
    "def inference(ae, Xz, loss_fn, device, batch_size=4096):\n",
    "    ae.eval()\n",
    "    n = Xz.shape[0]\n",
    "    out = np.empty(n, dtype=np.float32)\n",
    "    with torch.no_grad():\n",
    "        for i0 in range(0, n, batch_size):\n",
    "            i1 = min(i0 + batch_size, n)\n",
    "            xb = torch.tensor(Xz[i0:i1], dtype=torch.float32, device=device)\n",
    "            # forward pass\n",
    "            recon, _ = ae(xb)                         \n",
    "            loss_b = loss_fn(recon, xb)\n",
    "            out[i0:i1] = loss_b.detach().cpu().numpy()\n",
    "    return out\n",
    "\n",
    "#reco losses after running on test set\n",
    "reco_test_1 = inference(ae_1, X1_test_z, reco_loss_fn1, device)\n",
    "reco_test_2 = inference(ae_2, X2_test_z, reco_loss_fn2, device)\n",
    "\n",
    "#divides events into ABCD based on their losses compared to threshold\n",
    "def abcd_counts(loss_1, loss_2, percent_1, percent_2):\n",
    "    thresh_1 = np.quantile(loss_1, percent_1)\n",
    "    thresh_2 = np.quantile(loss_2, percent_2)\n",
    "    A = int(((loss_1 > thresh_1) & (loss_2 > thresh_2)).sum())     \n",
    "    B = int(((loss_1 > thresh_1) & (loss_2 <= thresh_2)).sum())    \n",
    "    C = int(((loss_1 <= thresh_1) & (loss_2 > thresh_2)).sum())\n",
    "    D = int(((loss_1 <= thresh_1) & (loss_2 <= thresh_2)).sum()) \n",
    "    return thresh_1, thresh_2, A, B, C, D\n",
    "\n",
    "#calculate nonclosure in region A\n",
    "def nonclosure_A(A, B, C, D, eps=1e-8):\n",
    "    #predicted background\n",
    "    A_hat = (B * C) / max(D, eps)\n",
    "    if A_hat <= 0:\n",
    "        return np.inf, A_hat\n",
    "    return (A - A_hat) / A_hat, A_hat\n",
    "\n",
    "#scan thresholds to minimize nonclosure\n",
    "percent_1s = np.linspace(0.75, 0.98, 24)\n",
    "percent_2s = np.linspace(0.75, 0.98, 24)\n",
    "\n",
    "#dict to store best combo of thresholds in ABCD scan\n",
    "best = {\n",
    "    \"percent_1\": None, \"percent_2\": None, \"t1\": None, \"t2\": None,\n",
    "    \"A\": None, \"B\": None, \"C\": None, \"D\": None,\n",
    "    \"nonclosure\": np.inf, \"A_hat\": None\n",
    "}\n",
    "#require at least 200 events in A\n",
    "min_A = 200 \n",
    "#at least 1000 in D\n",
    "min_D = 1000 \n",
    "\n",
    "#loop over thresholds (scan) and update best dict\n",
    "for percent_1 in percent_1s:\n",
    "    for percent_2 in percent_2s:\n",
    "        thresh_1, thresh_2, A, B, C, D = abcd_counts(reco_test_1, reco_test_2, percent_1, percent_2)\n",
    "        if (A < min_A) or (D < min_D):\n",
    "            continue\n",
    "        nc, A_hat = nonclosure_A(A, B, C, D)\n",
    "        if np.isfinite(nc) and abs(nc) < abs(best[\"nonclosure\"]):\n",
    "            best.update({\n",
    "                \"percent_1\": percent_1, \"percent_2\": percent_2, \"t1\": thresh_1, \"t2\": thresh_2,\n",
    "                \"A\": A, \"B\": B, \"C\": C, \"D\": D,\n",
    "                \"nonclosure\": nc, \"A_hat\": A_hat\n",
    "            })\n",
    "\n",
    "#take optimized parameters from scan\n",
    "t1_opt = best[\"t1\"]\n",
    "t2_opt = best[\"t2\"]\n",
    "percent_1_opt = best[\"percent_1\"]\n",
    "percent_2_opt = best[\"percent_2\"]\n",
    "N_A = best[\"A\"] \n",
    "N_B = best[\"B\"] \n",
    "N_C = best[\"C\"] \n",
    "N_D = best[\"D\"]\n",
    "N_A_hat = best[\"A_hat\"] \n",
    "nonclosure_A = best[\"nonclosure\"]\n",
    "\n",
    "#print them\n",
    "print(f\"Optimized percentiles: percent_1={percent_1_opt:.3f}, percent_2={percent_2_opt:.3f}\", flush=True)\n",
    "print(f\"Optimized thresholds: threshold 1={t1_opt:.6g}, threshold  2={t2_opt:.6g}\", flush=True)\n",
    "print(f\"ABCD counts: A={N_A}, B={N_B}, C={N_C}, D={N_D}\", flush=True)\n",
    "print(f\"Predicted A: N_A_hat={N_A_hat:.3f}\", flush=True)\n",
    "print(f\"Non-closure: {(100.0*nonclosure_A):.2f}%  ((A - A_hat)/A_hat)\", flush=True)\n",
    "\n",
    "#make plot dir\n",
    "plot_dir = \"plots/\"  \n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "#scan over percentiles\n",
    "p_list = np.linspace(0.75, 0.98, 24)\n",
    "#init selection efficiency list\n",
    "effs = []\n",
    "#init predicted/true background list\n",
    "bkg_ratio_ae = []\n",
    "#init poisson error band list\n",
    "bkg_ae_poisson_unc = []\n",
    "\n",
    "#total number test background events\n",
    "Ntot = float(len(reco_test_1))\n",
    "\n",
    "#loop over percentiles\n",
    "for p in p_list:\n",
    "    #find ABCD counts at (p, p)\n",
    "    thresh_1, thresh_2, A, B, C, D = abcd_counts(reco_test_1, reco_test_2, p, p)\n",
    "    A_hat = (B * C) / max(D, 1e-8)\n",
    "    ratio = A_hat / max(A, 1e-8)        \n",
    "\n",
    "    #poisson error propagation\n",
    "    invA = 0.0 if A == 0 else 1.0/A\n",
    "    invB = 0.0 if B == 0 else 1.0/B\n",
    "    invC = 0.0 if C == 0 else 1.0/C\n",
    "    invD = 0.0 if D == 0 else 1.0/D\n",
    "    rel_var = invA + invB + invC + invD\n",
    "    sigma = abs(ratio) * np.sqrt(rel_var) if rel_var > 0 else 0.0\n",
    "\n",
    "    #selection efficiency for region A\n",
    "    effs.append(A / max(Ntot, 1.0))      \n",
    "    bkg_ratio_ae.append(ratio)\n",
    "    bkg_ae_poisson_unc.append(sigma)\n",
    "\n",
    "#sort by efficiency\n",
    "effs = np.array(effs)\n",
    "bkg_ratio_ae = np.array(bkg_ratio_ae)\n",
    "bkg_ae_poisson_unc = np.array(bkg_ae_poisson_unc)\n",
    "order = np.argsort(effs)\n",
    "effs = effs[order] \n",
    "bkg_ratio_ae = bkg_ratio_ae[order]\n",
    "bkg_ae_poisson_unc = bkg_ae_poisson_unc[order]\n",
    "\n",
    "#colors and size like TNT code\n",
    "colors = ['g', 'b']  \n",
    "fig_size = (8, 6)\n",
    "fs = 28\n",
    "fs_leg = 24\n",
    "\n",
    "plt.figure(figsize=fig_size)\n",
    "fig, ax = plt.subplots(figsize=fig_size)\n",
    "\n",
    "#main AE curve plot\n",
    "ax.plot(effs, bkg_ratio_ae, c=colors[0], label=\"Autoencoders\")\n",
    "\n",
    "#error band plot\n",
    "alpha = 0.5\n",
    "ae_low = bkg_ratio_ae - bkg_ae_poisson_unc\n",
    "ae_high = bkg_ratio_ae + bkg_ae_poisson_unc\n",
    "ax.fill_between(effs, ae_low, ae_high, facecolor=colors[0], alpha=alpha, interpolate=True)\n",
    "\n",
    "#reference lines at 1 (perfect closure) and +/-5%\n",
    "one = np.ones_like(effs)\n",
    "one_m = np.full_like(effs, 0.95)\n",
    "one_p = np.full_like(effs, 1.05)\n",
    "ax.plot(effs, one,   linestyle='-',  color='black')\n",
    "ax.plot(effs, one_m, linestyle='--', color='black')\n",
    "ax.plot(effs, one_p, linestyle='--', color='black')\n",
    "\n",
    "#plot the optimized (percent_1_opt, percent_2_opt) operating point\n",
    "eff_opt = N_A / max(Ntot, 1.0)\n",
    "ax.plot([eff_opt], [N_A_hat / max(N_A, 1e-8)], marker='o', c='red', label='Optimized (bkg)')\n",
    "\n",
    "#labels and style like TNT plot code\n",
    "ax.set_xlabel('Selection Efficiency', fontsize=fs)\n",
    "ax.set_ylabel('Predicted Bkg. / True Bkg.', fontsize=fs)\n",
    "plt.ylim([0.0, 1.5])\n",
    "plt.xscale('log')\n",
    "plt.tick_params(axis='x', labelsize=fs_leg)\n",
    "plt.tick_params(axis='y', labelsize=fs_leg)\n",
    "plt.legend(loc=\"lower right\", fontsize=fs_leg)\n",
    "\n",
    "#save plot\n",
    "out_path = os.path.join(plot_dir, \"cut_and_count_bkg_check.png\")\n",
    "plt.savefig(out_path, dpi=200, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "#log closure plot into wandb\n",
    "wandb.log({\"Closure/plot\": wandb.Image(out_path)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2761078-2f9b-4392-949d-43621fcfdcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d8dc93-7bd0-4d9c-ae29-c645824cb908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
