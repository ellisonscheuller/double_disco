{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a5269f0-7f2e-40fd-917b-c3062e30516e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import gc\n",
    "import argparse\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "\n",
    "from models.autoencoder import Autoencoder \n",
    "#from losses.cyl_ptpz_mae import CylPtPzMAE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LogNorm\n",
    "from scipy.stats import binned_statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0252ca81-bc87-49de-bad4-be27f0f4ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting a seed like in ae_legacy\n",
    "def set_seed(seed=123):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33be852-c54f-4652-b770-402d2add7e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_corr(var_1,var_2,normedweight,power=1):\n",
    "    \"\"\"var_1: First variable to decorrelate (eg mass)\n",
    "    var_2: Second variable to decorrelate (eg classifier output)\n",
    "    normedweight: Per-example weight. Sum of weights should add up to N (where N is the number of examples)\n",
    "    power: Exponent used in calculating the distance correlation\n",
    "    \n",
    "    va1_1, var_2 and normedweight should all be 1D torch tensors with the same number of entries\n",
    "    \n",
    "    Usage: Add to your loss function. total_loss = BCE_loss + lambda * distance_corr\n",
    "    \"\"\" \n",
    "    \n",
    "    xx = var_1.view(-1, 1).repeat(1, len(var_1)).view(len(var_1),len(var_1))\n",
    "    yy = var_1.repeat(len(var_1),1).view(len(var_1),len(var_1))\n",
    "    amat = (xx-yy).abs()\n",
    "\n",
    "    xx = var_2.view(-1, 1).repeat(1, len(var_2)).view(len(var_2),len(var_2))\n",
    "    yy = var_2.repeat(len(var_2),1).view(len(var_2),len(var_2))\n",
    "    bmat = (xx-yy).abs()\n",
    "\n",
    "    amatavg = torch.mean(amat*normedweight,dim=1)\n",
    "    Amat=amat-amatavg.repeat(len(var_1),1).view(len(var_1),len(var_1))\\\n",
    "        -amatavg.view(-1, 1).repeat(1, len(var_1)).view(len(var_1),len(var_1))\\\n",
    "        +torch.mean(amatavg*normedweight)\n",
    "\n",
    "    bmatavg = torch.mean(bmat*normedweight,dim=1)\n",
    "    Bmat=bmat-bmatavg.repeat(len(var_2),1).view(len(var_2),len(var_2))\\\n",
    "        -bmatavg.view(-1, 1).repeat(1, len(var_2)).view(len(var_2),len(var_2))\\\n",
    "        +torch.mean(bmatavg*normedweight)\n",
    "\n",
    "    ABavg = torch.mean(Amat*Bmat*normedweight,dim=1)\n",
    "    AAavg = torch.mean(Amat*Amat*normedweight,dim=1)\n",
    "    BBavg = torch.mean(Bmat*Bmat*normedweight,dim=1)\n",
    "\n",
    "    if(power==1):\n",
    "        dCorr=(torch.mean(ABavg*normedweight))/torch.sqrt((torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight)))\n",
    "    elif(power==2):\n",
    "        dCorr=(torch.mean(ABavg*normedweight))**2/(torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight))\n",
    "    else:\n",
    "        dCorr=((torch.mean(ABavg*normedweight))/torch.sqrt((torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight))))**power\n",
    "    \n",
    "    return dCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "320ba38b-4c09-4ea0-b883-6f7011de2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates anomaly score like in ae legacy\n",
    "def distance_pt(model_ae, data_np, device):\n",
    "    x = torch.tensor(data_np, dtype=torch.float32, device=device)\n",
    "    z_mean, z_logvar, _ = model_ae.encoder(x)\n",
    "    score = torch.sum(z_mean**2, dim=1)\n",
    "    return score.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df9cf7a6-624b-491c-ad49-5eca472a676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to make 2D histograms\n",
    "def make_2D_hist(x, y, xlabel, ylabel, title, wandb_key, bins=40):\n",
    "    fig = plt.figure(figsize=(5,4))\n",
    "    plt.hist2d(x, y, bins=bins)\n",
    "    plt.colorbar(label='Counts')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    wandb.log({wandb_key: wandb.Image(fig)})\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3855723b-e71a-4348-9d28-4edcadec73c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerSampleMSE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction='none')\n",
    "    def forward(self, recon, target):\n",
    "        per_feat = self.mse(recon, target)\n",
    "        return per_feat.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8c2e8ec-dd88-46fb-91a0-71c7048af7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_standard_scaler(X, eps=1e-8):\n",
    "    mu  = X.mean(axis=0).astype(np.float32)\n",
    "    std = X.std(axis=0).astype(np.float32)\n",
    "    std = np.where(std < eps, 1.0, std)\n",
    "    return mu, std\n",
    "\n",
    "def transform_standard(X, mu, std):\n",
    "    return (X - mu) / (std + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70f9494b-d8c8-4bf6-b3a0-1d2850958bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints h5 tree to examine\n",
    "def print_h5_tree(h, prefix=\"\"):\n",
    "    for k in h.keys():\n",
    "        item = h[k]\n",
    "        if hasattr(item, 'keys'):\n",
    "            print(prefix + f\"[GROUP] {k}\")\n",
    "            print_h5_tree(item, prefix + \"  \")\n",
    "        else:\n",
    "            try:\n",
    "                print(prefix + f\"{k}: shape={item.shape}, dtype={item.dtype}\")\n",
    "            except Exception:\n",
    "                print(prefix + f\"{k}: <dataset>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dd000d9-98e3-4fcb-a254-131f09b88bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_plot(ax, x, y, nbins=30, logx=False, min_per_bin=20, label=\"mean Â± SE\"):\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    m = np.isfinite(x) & np.isfinite(y)\n",
    "    if logx:\n",
    "        m &= (x > 0)\n",
    "\n",
    "    x = x[m]\n",
    "    y = y[m]\n",
    "\n",
    "    # bin along x (linear or log space)\n",
    "    if logx:\n",
    "        xu = np.log10(x)\n",
    "    else:\n",
    "        xu = x\n",
    "\n",
    "    # uniform bins over the chosen coordinate\n",
    "    lo = float(xu.min())\n",
    "    hi = float(xu.max())\n",
    "    if lo == hi:\n",
    "        hi = np.nextafter(hi, np.inf)\n",
    "\n",
    "    edges = np.linspace(lo, hi, nbins + 1)\n",
    "    centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "\n",
    "    # stats per bin\n",
    "    mean, _, _ = binned_statistic(xu, y, statistic=\"mean\", bins=edges)\n",
    "    std,  _, _ = binned_statistic(xu, y, statistic=\"std\",  bins=edges)\n",
    "    cnt,  _, _ = binned_statistic(xu, y, statistic=\"count\", bins=edges)\n",
    "\n",
    "    sem = std / np.sqrt(np.maximum(cnt, 1))\n",
    "\n",
    "    # keep well-populated bins\n",
    "    good = cnt >= min_per_bin\n",
    "    xc = centers[good]\n",
    "    ym   = mean[good]\n",
    "    ye   = sem[good]\n",
    "\n",
    "    # convert x-axis back from log if needed\n",
    "    if logx:\n",
    "        xplot = 10.0 ** xc\n",
    "        ax.set_xscale(\"log\")\n",
    "    else:\n",
    "        xplot = xc\n",
    "\n",
    "    ax.errorbar(xplot, ym, yerr=ye, fmt=\"o\", ms=3, lw=1, capsize=2, label=label)\n",
    "    ax.grid(alpha=0.3)\n",
    "    return {\"x\": xplot, \"mean\": ym, \"sem\": ye, \"count\": cnt[good]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1df498a0-5325-4e58-be19-cea785549dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_loss_plots(\n",
    "    ae_1,\n",
    "    ae_2,\n",
    "    dataset_test,                # single array OR (X1_test, X2_test)\n",
    "    reco_loss_fn1=None,\n",
    "    reco_loss_fn2=None,\n",
    "    device=None,\n",
    "    batch_size=2048,\n",
    "    outdir=\"plots\",\n",
    "    bins=200,\n",
    "    logy=False,\n",
    "    ae1_input_slicer=None,\n",
    "    ae2_input_slicer=None,\n",
    "    ae1_loss_slicer=None,\n",
    "    ae2_loss_slicer=None\n",
    "):\n",
    "    import os, numpy as np, torch, matplotlib.pyplot as plt\n",
    "    from matplotlib.colors import LogNorm\n",
    "\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    assert device is not None\n",
    "\n",
    "    do_ae1 = ae_1 is not None\n",
    "    do_ae2 = ae_2 is not None\n",
    "    assert do_ae1 or do_ae2\n",
    "\n",
    "    if do_ae1:\n",
    "        ae_1.eval(); assert reco_loss_fn1 is not None\n",
    "    if do_ae2:\n",
    "        ae_2.eval(); assert reco_loss_fn2 is not None\n",
    "\n",
    "    # Allow (X1_test, X2_test) OR a single dataset for both\n",
    "    if isinstance(dataset_test, (tuple, list)) and len(dataset_test) == 2:\n",
    "        ds1, ds2 = dataset_test\n",
    "    else:\n",
    "        ds1 = dataset_test\n",
    "        ds2 = dataset_test\n",
    "\n",
    "    losses_1, losses_2 = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if do_ae1 and ds1 is not None:\n",
    "            X1_full = torch.tensor(ds1, dtype=torch.float32, device=device)\n",
    "            for i in range(0, X1_full.size(0), batch_size):\n",
    "                xb_full = X1_full[i:i + batch_size]\n",
    "                xb1 = ae1_input_slicer(xb_full) if ae1_input_slicer else xb_full\n",
    "                recon1, _ = ae_1(xb1)\n",
    "                y1_pred, y1_true = recon1, xb1\n",
    "                if ae1_loss_slicer:\n",
    "                    y1_pred = ae1_loss_slicer(y1_pred); y1_true = ae1_loss_slicer(y1_true)\n",
    "                l1 = reco_loss_fn1(y1_pred, y1_true).detach().cpu().numpy().reshape(-1)\n",
    "                l1 = np.nan_to_num(l1, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                losses_1.append(l1)\n",
    "\n",
    "        if do_ae2 and ds2 is not None:\n",
    "            X2_full = torch.tensor(ds2, dtype=torch.float32, device=device)\n",
    "            for i in range(0, X2_full.size(0), batch_size):\n",
    "                xb_full = X2_full[i:i + batch_size]\n",
    "                xb2 = ae2_input_slicer(xb_full) if ae2_input_slicer else xb_full\n",
    "                recon2, _ = ae_2(xb2)\n",
    "                y2_pred, y2_true = recon2, xb2\n",
    "                if ae2_loss_slicer:\n",
    "                    y2_pred = ae2_loss_slicer(y2_pred); y2_true = ae2_loss_slicer(y2_true)\n",
    "                l2 = reco_loss_fn2(y2_pred, y2_true).detach().cpu().numpy().reshape(-1)\n",
    "                l2 = np.nan_to_num(l2, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                losses_2.append(l2)\n",
    "\n",
    "    losses_1_all = np.concatenate(losses_1, axis=0) if (do_ae1 and losses_1) else None\n",
    "    losses_2_all = np.concatenate(losses_2, axis=0) if (do_ae2 and losses_2) else None\n",
    "\n",
    "    # Save raw arrays\n",
    "    if losses_1_all is not None:\n",
    "        np.save(os.path.join(outdir, \"test_reco_loss_ae1.npy\"), losses_1_all)\n",
    "    if losses_2_all is not None:\n",
    "        np.save(os.path.join(outdir, \"test_reco_loss_ae2.npy\"), losses_2_all)\n",
    "\n",
    "    # Helper to plot one histogram\n",
    "    def _one_hist(arr, tag):\n",
    "        if arr is None or arr.size == 0:\n",
    "            print(f\"[WARN] No losses for {tag}; skipping histogram.\")\n",
    "            return None\n",
    "        mask = arr > 0\n",
    "        x = arr[mask]\n",
    "        if x.size == 0:\n",
    "            print(f\"[WARN] No positive losses for {tag}; skipping histogram.\")\n",
    "            return None\n",
    "        x_max = np.nextafter(float(x.max()), np.inf)\n",
    "        x_min = max(min(x.min(), x_max/1e6), 1e-12)\n",
    "        edges = np.logspace(np.log10(x_min), np.log10(x_max), bins + 1)\n",
    "        plt.figure(figsize=(7, 4))\n",
    "        plt.hist(x, bins=edges)\n",
    "        plt.xscale(\"log\")\n",
    "        if logy: plt.yscale(\"log\")\n",
    "        plt.xlabel(f\"{tag} reconstruction loss\"); plt.ylabel(\"Counts\")\n",
    "        plt.title(f\"{tag} reconstruction loss (Test)\")\n",
    "        plt.xlim(edges[0], edges[-1])\n",
    "        plt.tight_layout()\n",
    "        out_png = os.path.join(outdir, f\"hist_{tag}.png\")\n",
    "        plt.savefig(out_png, dpi=150); plt.close()\n",
    "        try:\n",
    "            import wandb; wandb.log({f\"Eval/hist_{tag}\": wandb.Image(out_png)})\n",
    "        except Exception:\n",
    "            pass\n",
    "        return edges\n",
    "\n",
    "    # Per-AE histograms\n",
    "    edges1 = _one_hist(losses_1_all, \"AE1\") if do_ae1 else None\n",
    "    edges2 = _one_hist(losses_2_all, \"AE2\") if do_ae2 else None\n",
    "\n",
    "    # Cross-AE plots only if BOTH exist and are aligned in length\n",
    "    if (losses_1_all is not None) and (losses_2_all is not None) and \\\n",
    "       (losses_1_all.shape[0] == losses_2_all.shape[0]):\n",
    "\n",
    "        mask = (losses_1_all > 0) & (losses_2_all > 0)\n",
    "        x1 = losses_1_all[mask]; x2 = losses_2_all[mask]\n",
    "\n",
    "        # Choose edges robustly if None (fallback)\n",
    "        if edges1 is None and x1.size:\n",
    "            x1_max = np.nextafter(float(x1.max()), np.inf)\n",
    "            x1_min = max(min(x1.min(), x1_max/1e6), 1e-12)\n",
    "            edges1 = np.logspace(np.log10(x1_min), np.log10(x1_max), bins + 1)\n",
    "        if edges2 is None and x2.size:\n",
    "            x2_max = np.nextafter(float(x2.max()), np.inf)\n",
    "            x2_min = max(min(x2.min(), x2_max/1e6), 1e-12)\n",
    "            edges2 = np.logspace(np.log10(x2_min), np.log10(x2_max), bins + 1)\n",
    "\n",
    "        # 2D hist\n",
    "        if x1.size and x2.size:\n",
    "            plt.figure(figsize=(6, 5))\n",
    "            plt.hist2d(x1, x2, bins=[edges1, edges2], norm=LogNorm(vmin=1), cmin=1)\n",
    "            plt.xscale(\"log\"); plt.yscale(\"log\")\n",
    "            plt.xlabel(\"AE1 reconstruction loss\"); plt.ylabel(\"AE2 reconstruction loss\")\n",
    "            plt.title(\"AE1 vs AE2 reconstruction loss (Test) â log-log\")\n",
    "            plt.xlim(edges1[0], edges1[-1]); plt.ylim(edges2[0], edges2[-1])\n",
    "            plt.colorbar(label=\"Counts (log)\")\n",
    "            plt.tight_layout()\n",
    "            out_png = os.path.join(outdir, \"hist2d_AE1_vs_AE2.png\")\n",
    "            plt.savefig(out_png, dpi=150); plt.close()\n",
    "            try:\n",
    "                import wandb; wandb.log({\"Eval/hist2d_AE1_vs_AE2\": wandb.Image(out_png)})\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Profile plots\n",
    "    if (losses_1_all is not None) and (losses_2_all is not None):\n",
    "        n = min(len(losses_1_all), len(losses_2_all))\n",
    "        x1 = losses_1_all[:n]\n",
    "        x2 = losses_2_all[:n]\n",
    "    \n",
    "        fig, ax = plt.subplots(figsize=(6.2, 4.6))\n",
    "        profile_plot(ax, x=x2, y=x1, nbins=100, logx=True, min_per_bin=50, label=\"mean Â± SE\")\n",
    "        ax.set_title(\"Profile: â¨AE1 lossâ© vs AE2 loss (Test)\")\n",
    "        ax.set_xlabel(\"AE2 reconstruction loss\")\n",
    "        ax.set_ylabel(\"â¨AE1 reconstruction lossâ©\")\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.legend()\n",
    "        fig.tight_layout()\n",
    "        out_png = os.path.join(outdir, \"profile_AE1_vs_AE2.png\")\n",
    "        fig.savefig(out_png, dpi=150)\n",
    "        plt.close(fig)\n",
    "        try:\n",
    "            import wandb; wandb.log({\"Profiles/AE1_vs_AE2\": wandb.Image(out_png)})\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "        fig, ax = plt.subplots(figsize=(6.2, 4.6))\n",
    "        profile_plot(ax, x=x1, y=x2, nbins=100, logx=True, min_per_bin=50, label=\"mean Â± SE\")\n",
    "        ax.set_title(\"Profile: â¨AE2 lossâ© vs AE1 loss (Test)\")\n",
    "        ax.set_xlabel(\"AE1 reconstruction loss\")\n",
    "        ax.set_ylabel(\"â¨AE2 reconstruction lossâ©\")\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.legend()\n",
    "        fig.tight_layout()\n",
    "        out_png = os.path.join(outdir, \"profile_AE2_vs_AE1.png\")\n",
    "        fig.savefig(out_png, dpi=150)\n",
    "        plt.close(fig)\n",
    "        try:\n",
    "            import wandb; wandb.log({\"Profiles/AE2_vs_AE1\": wandb.Image(out_png)})\n",
    "        except Exception:\n",
    "            pass\n",
    "                \n",
    "    return losses_1_all, losses_2_all\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "540a813e-5d61-4ce4-9593-d2ef7e03bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config):\n",
    "    seed = 123\n",
    "    set_seed(seed)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using: {device}\", flush=True)\n",
    "\n",
    "    print(\"Logging in to wandb...\", flush=True)\n",
    "    wandb.login(key=\"24d1d60ce26563c74d290d7b487cb104fc251271\")\n",
    "    wandb.init(project=\"Double Disco Axo Training\",\n",
    "               settings=wandb.Settings(_disable_stats=True),\n",
    "               config=config)\n",
    "    run_name = wandb.run.name\n",
    "    print(f\"Run name: {run_name}\", flush=True)\n",
    "\n",
    "    alpha = float(config['alpha'])\n",
    "    ae_lr = float(config['ae_lr'])\n",
    "    latent_dim = int(config['ae_latent'])\n",
    "    enc_nodes = list(config['ae_nodes'])\n",
    "    dec_nodes_template = [24, 32, 64, 128]\n",
    "\n",
    "    #load data\n",
    "    data_path = '/axovol/HLT_data_oct_14.h5'\n",
    "    print(\"Loading dataset...\", flush=True)\n",
    "    with h5.File(data_path, 'r') as f:\n",
    "        r = f['data'] if 'data' in f else f\n",
    "\n",
    "        print(\"H5 tree:\")\n",
    "        print_h5_tree(r)\n",
    "\n",
    "        x_train = r['Background_data']['Train']['DATA'][:]  \n",
    "        x_test  = r['Background_data']['Test']['DATA'][:]   \n",
    "        scale = r['Normalization']['norm_scale'][:]     \n",
    "        bias = r['Normalization']['norm_bias'][:]        \n",
    "\n",
    "        print(f\"Train shape: {x_train.shape}, Test shape: {x_test.shape}\", flush=True)\n",
    "\n",
    "        #undo normalization (next time you do preprocessing just remove it)\n",
    "        Xtr_raw = x_train * scale + bias\n",
    "        Xte_raw = x_test  * scale + bias\n",
    "        \n",
    "        #padding\n",
    "        pad_tr = (x_train == 0.0).all(axis=-1)\n",
    "        pad_te = (x_test  == 0.0).all(axis=-1)\n",
    "        Xtr_raw[pad_tr] = 0.0\n",
    "        Xte_raw[pad_te] = 0.0\n",
    "        \n",
    "        #splitting AE datasets by object\n",
    "        SLOTS = {\n",
    "            \"ELECTRONS\": (0, 4),\n",
    "            \"MUONS\":     (4, 8),\n",
    "            \"PHOTONS\":   (8, 12),\n",
    "            \"JETS\":      (12, 22),\n",
    "            \"FATJETS\":   (22, 32),\n",
    "            \"MET\":       (32, 33),\n",
    "        }\n",
    "\n",
    "        def _slice_slots(x, start, end):\n",
    "            n, _, fdim = x.shape\n",
    "            return x[:, start:end, :].reshape(n, (end - start) * fdim)\n",
    "\n",
    "        def _take_groups(x, groups):\n",
    "            parts = [_slice_slots(x, *SLOTS[g]) for g in groups]\n",
    "            return np.concatenate(parts, axis=1) if len(parts) > 1 else parts[0]\n",
    "\n",
    "        # AE-1 = jets (+ fatjets), AE-2 = leptons + photons + MET \n",
    "        X1_train = _take_groups(Xtr_raw, [\"JETS\", \"FATJETS\", \"ELECTRONS\", \"MUONS\", \"PHOTONS\", \"MET\"])\n",
    "        X1_test = _take_groups(Xte_raw, [\"JETS\", \"FATJETS\", \"ELECTRONS\", \"MUONS\", \"PHOTONS\", \"MET\"])\n",
    "        X2_train = X1_train\n",
    "        X2_test = X1_test\n",
    "        # X2_train = _take_groups(Xtr_raw, [\"ELECTRONS\", \"MUONS\", \"PHOTONS\", \"MET\"])\n",
    "        # X2_test = _take_groups(Xte_raw, [\"ELECTRONS\", \"MUONS\", \"PHOTONS\", \"MET\"])\n",
    "\n",
    "\n",
    "        # keep raw copies for plots/inference\n",
    "        X1_train_raw, X1_test_raw = X1_train, X1_test\n",
    "        X2_train_raw, X2_test_raw = X2_train, X2_test\n",
    "\n",
    "        mu1, std1 = fit_standard_scaler(X1_train_raw)\n",
    "        # mu2, std2 = fit_standard_scaler(X2_train_raw)\n",
    "        mu2, std2 = mu1, std1\n",
    "        \n",
    "        X1_train_z = transform_standard(X1_train_raw, mu1, std1)\n",
    "        X1_test_z = transform_standard(X1_test_raw,  mu1, std1)\n",
    "        X2_train_z = transform_standard(X2_train_raw, mu2, std2)\n",
    "        X2_test_z = transform_standard(X2_test_raw,  mu2, std2)\n",
    "\n",
    "    feat1 = X1_train.shape[1]\n",
    "    feat2 = X2_train.shape[1]\n",
    "\n",
    "    reco_loss_fn1 = PerSampleMSE().to(device)\n",
    "    reco_loss_fn2 = PerSampleMSE().to(device)\n",
    "    print(\"Loss functions ready.\", flush=True)\n",
    "\n",
    "    ae1_cfg = {\n",
    "        \"features\": feat1,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"encoder_config\": {\"nodes\": enc_nodes},\n",
    "        \"decoder_config\": {\"nodes\": dec_nodes_template + [feat1]},\n",
    "        \"alpha\": alpha\n",
    "    }\n",
    "    ae2_cfg = {\n",
    "        \"features\": feat2,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"encoder_config\": {\"nodes\": enc_nodes},\n",
    "        \"decoder_config\": {\"nodes\": dec_nodes_template + [feat2]},\n",
    "        \"alpha\": alpha\n",
    "    }\n",
    "\n",
    "    ae_1 = Autoencoder(ae1_cfg).to(device)\n",
    "    ae_2 = Autoencoder(ae2_cfg).to(device)\n",
    "    print(\"Autoencoders are ready.\", flush=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(ae_1.parameters()) + list(ae_2.parameters()),\n",
    "        lr=ae_lr\n",
    "    )\n",
    "\n",
    "    warmup_epochs = 10\n",
    "    cos = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=32, T_mult=2, eta_min=0.0\n",
    "    )\n",
    "\n",
    "    def set_lr(lr):\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "\n",
    "    Epochs_AE = 100\n",
    "    Batch_size = 1024\n",
    "    lambda_disco = float(config.get(\"lambda_disco\", 1.0))\n",
    "\n",
    "    print(\"Moving data to device...\", flush=True)\n",
    "    X1 = torch.tensor(X1_train_z, dtype=torch.float32, device=device)\n",
    "    X2 = torch.tensor(X2_train_z, dtype=torch.float32, device=device)\n",
    "    print(\"Data on device.\", flush=True)\n",
    "\n",
    "    def _check_finite(name, t):\n",
    "        ok = torch.isfinite(t).all().item()\n",
    "        print(f\"[finite-check] {name}: {ok} | min={t.min().item():.3g}, max={t.max().item():.3g}\")\n",
    "        if not ok:\n",
    "            bad = (~torch.isfinite(t)).nonzero(as_tuple=False)[:5]\n",
    "            print(f\"[finite-check] {name} bad idx (first 5): {bad}\")\n",
    "            raise RuntimeError(f\"{name} contains non-finite values\")\n",
    "\n",
    "    _check_finite(\"X1_train\", torch.tensor(X1_train[:1000], device=device))\n",
    "    _check_finite(\"X2_train\", torch.tensor(X2_train[:1000], device=device))\n",
    "\n",
    "    #training\n",
    "    print(\"Starting the training loop!\", flush=True)\n",
    "    N1 = X1.size(0)\n",
    "    N2 = X2.size(0)\n",
    "    Nmin = min(N1, N2)\n",
    "\n",
    "    for epoch in range(Epochs_AE):\n",
    "        ae1_reco_loss = []\n",
    "        ae2_reco_loss = []\n",
    "\n",
    "        if epoch < warmup_epochs:\n",
    "            lr = ae_lr * (epoch + 1) / warmup_epochs\n",
    "            set_lr(lr)\n",
    "        else:\n",
    "            cos.step(epoch - warmup_epochs)\n",
    "\n",
    "        perm1 = torch.randperm(N1, device=device)\n",
    "        perm2 = torch.randperm(N2, device=device)\n",
    "\n",
    "        num_batches = 0\n",
    "        total_loss = total_reco1 = total_reco2 = total_disco = 0.0\n",
    "\n",
    "        for i0 in range(0, Nmin, Batch_size):\n",
    "            i1 = min(i0 + Batch_size, Nmin)\n",
    "            bsz = i1 - i0\n",
    "\n",
    "            idx1 = perm1[i0:i1]\n",
    "            idx2 = perm2[i0:i1]\n",
    "\n",
    "            xb1 = X1[idx1]\n",
    "            xb2 = X2[idx2]\n",
    "\n",
    "            recon1, z1 = ae_1(xb1)\n",
    "            recon2, z2 = ae_2(xb2)\n",
    "\n",
    "            reco1_per = reco_loss_fn1(recon1, xb1)\n",
    "            reco2_per = reco_loss_fn2(recon2, xb2)\n",
    "\n",
    "            w = torch.ones(bsz, device=device, dtype=reco1_per.dtype)\n",
    "            disco = distance_corr(reco1_per, reco2_per, w, power=1)\n",
    "\n",
    "            reco1 = ae_1.alpha * reco1_per.mean()\n",
    "            reco2 = ae_2.alpha * reco2_per.mean()\n",
    "\n",
    "            loss = reco1 + reco2 + lambda_disco * disco\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(ae_1.parameters()) + list(ae_2.parameters()), max_norm=5.0\n",
    "            )\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss  += loss.item()\n",
    "            total_reco1 += reco1.item()\n",
    "            total_reco2 += reco2.item()\n",
    "            total_disco += disco.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            ae1_reco_loss.append(reco1.item())\n",
    "            ae2_reco_loss.append(reco2.item())\n",
    "\n",
    "            avg_loss  = total_loss  / max(1, num_batches)\n",
    "            avg_reco1 = total_reco1 / max(1, num_batches)\n",
    "            avg_reco2 = total_reco2 / max(1, num_batches)\n",
    "            avg_disco = total_disco / max(1, num_batches)\n",
    "\n",
    "        print(f\"[EPOCH {epoch}/{Epochs_AE}] \"\n",
    "              f\"Loss={avg_loss:.4f} \"\n",
    "              f\"Reco1(jets)={avg_reco1:.4f} Reco2(lep/photon/MET)={avg_reco2:.4f} \"\n",
    "              f\"DisCo={avg_disco:.4f}\", flush=True)\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"TotalLoss\": avg_loss,\n",
    "            \"RecoLoss_AE1_jets\": avg_reco1,\n",
    "            \"RecoLoss_AE2_lepPhotMET\": avg_reco2,\n",
    "            \"DisCoLoss\": avg_disco,\n",
    "            \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "        })\n",
    "\n",
    "        ae1_reco_np = np.array(ae1_reco_loss)\n",
    "        ae2_reco_np = np.array(ae2_reco_loss)\n",
    "\n",
    "        make_2D_hist(ae1_reco_np, ae2_reco_np,\n",
    "                     \"Reco Loss (jets)\", \"Reco Loss (lep/photon/MET)\",\n",
    "                     f\"Epoch {epoch}: Reco jets vs Reco lep/photon/MET\",\n",
    "                     wandb_key=\"Hists2D/Reco_jets_vs_Reco_lepPhotMET\")\n",
    "\n",
    "    print(\"Finished training.\", flush=True)\n",
    "\n",
    "    #save model\n",
    "    torch.save(ae_1.state_dict(), \"ae1_trained_jets.pth\")\n",
    "    torch.save(ae_2.state_dict(), \"ae2_trained_lepPhotMET.pth\")\n",
    "\n",
    "    mu1_t = torch.tensor(mu1, dtype=torch.float32, device=device)\n",
    "    std1_t = torch.tensor(std1, dtype=torch.float32, device=device)\n",
    "    mu2_t = torch.tensor(mu2, dtype=torch.float32, device=device)\n",
    "    std2_t = torch.tensor(std2, dtype=torch.float32, device=device)\n",
    "    \n",
    "    def ae1_input_from_raw(t_flat: torch.Tensor):\n",
    "        return (t_flat - mu1_t) / (std1_t + 1e-8)\n",
    "    \n",
    "    def ae2_input_from_raw(t_flat: torch.Tensor):\n",
    "        return (t_flat - mu2_t) / (std2_t + 1e-8)\n",
    "\n",
    "    _ae1_losses, _ae2_losses = inference_loss_plots(\n",
    "        ae_1=ae_1,\n",
    "        ae_2=ae_2,\n",
    "        dataset_test=(X1_test_raw, X2_test_raw),\n",
    "        reco_loss_fn1=reco_loss_fn1,\n",
    "        reco_loss_fn2=reco_loss_fn2,\n",
    "        device=device,\n",
    "        batch_size=2048,\n",
    "        outdir=\"plots_profiles_same_events\",\n",
    "        bins=200,\n",
    "        logy=False,\n",
    "        ae1_input_slicer=ae1_input_from_raw,\n",
    "        ae2_input_slicer=ae2_input_from_raw,\n",
    "        ae1_loss_slicer=None,\n",
    "        ae2_loss_slicer=None\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8160a1fc-439a-429f-a154-7c134e0ecfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n",
      "Logging in to wandb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mescheuller\u001b[0m (\u001b[33mescheuller-uc-san-diego\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20251022_070558-qqn5dvps</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/qqn5dvps' target=\"_blank\">winter-sun-259</a></strong> to <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training' target=\"_blank\">https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/qqn5dvps' target=\"_blank\">https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/qqn5dvps</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: winter-sun-259\n",
      "Loading dataset...\n",
      "H5 tree:\n",
      "[GROUP] Background_data\n",
      "  [GROUP] Test\n",
      "    DATA: shape=(255856, 33, 15), dtype=float32\n",
      "  [GROUP] Train\n",
      "    DATA: shape=(1023420, 33, 15), dtype=float32\n",
      "[GROUP] Normalization\n",
      "  norm_bias: shape=(33, 15), dtype=float32\n",
      "  norm_scale: shape=(33, 15), dtype=float32\n",
      "Train shape: (1023420, 33, 15), Test shape: (255856, 33, 15)\n",
      "Loss functions ready.\n",
      "Moving data to device...\n",
      "Data on device.\n",
      "[finite-check] X1_train: True | min=-3.14, max=588\n",
      "[finite-check] X2_train: True | min=-3.14, max=588\n",
      "Starting the training loop!\n",
      "[EPOCH 0/100] Loss=3.8588 Reco1(jets)=0.2482 Reco2(lep/photon/MET)=0.2473 DisCo=0.0034\n",
      "[EPOCH 1/100] Loss=4.1258 Reco1(jets)=0.2454 Reco2(lep/photon/MET)=0.2416 DisCo=0.0036\n",
      "[EPOCH 2/100] Loss=3.8061 Reco1(jets)=0.2490 Reco2(lep/photon/MET)=0.2416 DisCo=0.0033\n",
      "[EPOCH 3/100] Loss=3.6073 Reco1(jets)=0.2654 Reco2(lep/photon/MET)=0.2534 DisCo=0.0031\n",
      "[EPOCH 4/100] Loss=3.3571 Reco1(jets)=0.2809 Reco2(lep/photon/MET)=0.2701 DisCo=0.0028\n",
      "[EPOCH 5/100] Loss=3.2011 Reco1(jets)=0.3063 Reco2(lep/photon/MET)=0.3045 DisCo=0.0026\n",
      "[EPOCH 6/100] Loss=2.9638 Reco1(jets)=0.3144 Reco2(lep/photon/MET)=0.3450 DisCo=0.0023\n",
      "[EPOCH 7/100] Loss=2.7621 Reco1(jets)=0.3368 Reco2(lep/photon/MET)=0.3804 DisCo=0.0020\n",
      "[EPOCH 8/100] Loss=2.7376 Reco1(jets)=0.3562 Reco2(lep/photon/MET)=0.4287 DisCo=0.0020\n",
      "[EPOCH 9/100] Loss=2.5494 Reco1(jets)=0.3580 Reco2(lep/photon/MET)=0.4465 DisCo=0.0017\n",
      "[EPOCH 10/100] Loss=2.5805 Reco1(jets)=0.3960 Reco2(lep/photon/MET)=0.4259 DisCo=0.0018\n",
      "[EPOCH 11/100] Loss=2.5370 Reco1(jets)=0.4196 Reco2(lep/photon/MET)=0.4142 DisCo=0.0017\n",
      "[EPOCH 12/100] Loss=2.5941 Reco1(jets)=0.4210 Reco2(lep/photon/MET)=0.4067 DisCo=0.0018\n",
      "[EPOCH 13/100] Loss=2.3579 Reco1(jets)=0.4075 Reco2(lep/photon/MET)=0.4205 DisCo=0.0015\n",
      "[EPOCH 14/100] Loss=2.3126 Reco1(jets)=0.4160 Reco2(lep/photon/MET)=0.3948 DisCo=0.0015\n",
      "[EPOCH 15/100] Loss=2.3623 Reco1(jets)=0.4208 Reco2(lep/photon/MET)=0.3927 DisCo=0.0015\n",
      "[EPOCH 16/100] Loss=2.3234 Reco1(jets)=0.3987 Reco2(lep/photon/MET)=0.3881 DisCo=0.0015\n",
      "[EPOCH 17/100] Loss=2.4845 Reco1(jets)=0.4023 Reco2(lep/photon/MET)=0.3778 DisCo=0.0017\n",
      "[EPOCH 18/100] Loss=2.2641 Reco1(jets)=0.4030 Reco2(lep/photon/MET)=0.3734 DisCo=0.0015\n",
      "[EPOCH 19/100] Loss=2.3145 Reco1(jets)=0.4101 Reco2(lep/photon/MET)=0.3732 DisCo=0.0015\n",
      "[EPOCH 20/100] Loss=2.2533 Reco1(jets)=0.4150 Reco2(lep/photon/MET)=0.3637 DisCo=0.0015\n",
      "[EPOCH 21/100] Loss=2.4027 Reco1(jets)=0.4074 Reco2(lep/photon/MET)=0.3653 DisCo=0.0016\n",
      "[EPOCH 22/100] Loss=2.2197 Reco1(jets)=0.4135 Reco2(lep/photon/MET)=0.3531 DisCo=0.0015\n",
      "[EPOCH 23/100] Loss=2.1358 Reco1(jets)=0.4093 Reco2(lep/photon/MET)=0.3485 DisCo=0.0014\n",
      "[EPOCH 24/100] Loss=2.1483 Reco1(jets)=0.4156 Reco2(lep/photon/MET)=0.3461 DisCo=0.0014\n",
      "[EPOCH 25/100] Loss=2.3956 Reco1(jets)=0.4066 Reco2(lep/photon/MET)=0.3493 DisCo=0.0016\n",
      "[EPOCH 26/100] Loss=2.2377 Reco1(jets)=0.4073 Reco2(lep/photon/MET)=0.3478 DisCo=0.0015\n",
      "[EPOCH 27/100] Loss=2.3069 Reco1(jets)=0.4037 Reco2(lep/photon/MET)=0.3505 DisCo=0.0016\n",
      "[EPOCH 28/100] Loss=2.4597 Reco1(jets)=0.3941 Reco2(lep/photon/MET)=0.3451 DisCo=0.0017\n",
      "[EPOCH 29/100] Loss=2.2196 Reco1(jets)=0.3959 Reco2(lep/photon/MET)=0.3418 DisCo=0.0015\n",
      "[EPOCH 30/100] Loss=2.2366 Reco1(jets)=0.3962 Reco2(lep/photon/MET)=0.3423 DisCo=0.0015\n",
      "[EPOCH 31/100] Loss=2.1867 Reco1(jets)=0.3869 Reco2(lep/photon/MET)=0.3411 DisCo=0.0015\n",
      "[EPOCH 32/100] Loss=2.0956 Reco1(jets)=0.3926 Reco2(lep/photon/MET)=0.3325 DisCo=0.0014\n",
      "[EPOCH 33/100] Loss=1.9944 Reco1(jets)=0.3906 Reco2(lep/photon/MET)=0.3369 DisCo=0.0013\n",
      "[EPOCH 34/100] Loss=2.2563 Reco1(jets)=0.3923 Reco2(lep/photon/MET)=0.3392 DisCo=0.0015\n",
      "[EPOCH 35/100] Loss=2.0178 Reco1(jets)=0.3900 Reco2(lep/photon/MET)=0.3356 DisCo=0.0013\n",
      "[EPOCH 36/100] Loss=2.0267 Reco1(jets)=0.3920 Reco2(lep/photon/MET)=0.3317 DisCo=0.0013\n",
      "[EPOCH 37/100] Loss=2.1828 Reco1(jets)=0.3958 Reco2(lep/photon/MET)=0.3321 DisCo=0.0015\n",
      "[EPOCH 38/100] Loss=2.0835 Reco1(jets)=0.3933 Reco2(lep/photon/MET)=0.3302 DisCo=0.0014\n",
      "[EPOCH 39/100] Loss=2.3635 Reco1(jets)=0.3964 Reco2(lep/photon/MET)=0.3318 DisCo=0.0016\n",
      "[EPOCH 40/100] Loss=2.0439 Reco1(jets)=0.3953 Reco2(lep/photon/MET)=0.3326 DisCo=0.0013\n",
      "[EPOCH 41/100] Loss=2.1736 Reco1(jets)=0.3932 Reco2(lep/photon/MET)=0.3326 DisCo=0.0014\n",
      "[EPOCH 42/100] Loss=2.3035 Reco1(jets)=0.3939 Reco2(lep/photon/MET)=0.3328 DisCo=0.0016\n",
      "[EPOCH 43/100] Loss=2.2259 Reco1(jets)=0.3883 Reco2(lep/photon/MET)=0.3330 DisCo=0.0015\n",
      "[EPOCH 44/100] Loss=2.1713 Reco1(jets)=0.3893 Reco2(lep/photon/MET)=0.3295 DisCo=0.0015\n",
      "[EPOCH 45/100] Loss=2.0091 Reco1(jets)=0.3866 Reco2(lep/photon/MET)=0.3318 DisCo=0.0013\n",
      "[EPOCH 46/100] Loss=2.0554 Reco1(jets)=0.3867 Reco2(lep/photon/MET)=0.3335 DisCo=0.0013\n",
      "[EPOCH 47/100] Loss=2.1554 Reco1(jets)=0.3802 Reco2(lep/photon/MET)=0.3298 DisCo=0.0014\n",
      "[EPOCH 48/100] Loss=2.1027 Reco1(jets)=0.3697 Reco2(lep/photon/MET)=0.3348 DisCo=0.0014\n",
      "[EPOCH 49/100] Loss=2.2021 Reco1(jets)=0.3656 Reco2(lep/photon/MET)=0.3333 DisCo=0.0015\n",
      "[EPOCH 50/100] Loss=2.6189 Reco1(jets)=0.3661 Reco2(lep/photon/MET)=0.3327 DisCo=0.0019\n",
      "[EPOCH 51/100] Loss=2.1740 Reco1(jets)=0.3586 Reco2(lep/photon/MET)=0.3269 DisCo=0.0015\n",
      "[EPOCH 52/100] Loss=2.2280 Reco1(jets)=0.3500 Reco2(lep/photon/MET)=0.3201 DisCo=0.0016\n",
      "[EPOCH 53/100] Loss=2.1190 Reco1(jets)=0.3528 Reco2(lep/photon/MET)=0.3225 DisCo=0.0014\n",
      "[EPOCH 54/100] Loss=3.1701 Reco1(jets)=0.3577 Reco2(lep/photon/MET)=0.3249 DisCo=0.0025\n",
      "[EPOCH 55/100] Loss=2.4937 Reco1(jets)=0.3566 Reco2(lep/photon/MET)=0.3187 DisCo=0.0018\n",
      "[EPOCH 56/100] Loss=2.2437 Reco1(jets)=0.3594 Reco2(lep/photon/MET)=0.3227 DisCo=0.0016\n",
      "[EPOCH 57/100] Loss=2.5174 Reco1(jets)=0.3611 Reco2(lep/photon/MET)=0.3148 DisCo=0.0018\n",
      "[EPOCH 58/100] Loss=2.1038 Reco1(jets)=0.3591 Reco2(lep/photon/MET)=0.3224 DisCo=0.0014\n",
      "[EPOCH 59/100] Loss=2.2130 Reco1(jets)=0.3551 Reco2(lep/photon/MET)=0.3179 DisCo=0.0015\n",
      "[EPOCH 60/100] Loss=2.2438 Reco1(jets)=0.3629 Reco2(lep/photon/MET)=0.3155 DisCo=0.0016\n",
      "[EPOCH 61/100] Loss=2.0845 Reco1(jets)=0.3641 Reco2(lep/photon/MET)=0.3176 DisCo=0.0014\n",
      "[EPOCH 62/100] Loss=2.0541 Reco1(jets)=0.3636 Reco2(lep/photon/MET)=0.3159 DisCo=0.0014\n",
      "[EPOCH 63/100] Loss=2.0642 Reco1(jets)=0.3619 Reco2(lep/photon/MET)=0.3135 DisCo=0.0014\n",
      "[EPOCH 64/100] Loss=2.0169 Reco1(jets)=0.3684 Reco2(lep/photon/MET)=0.3094 DisCo=0.0013\n",
      "[EPOCH 65/100] Loss=2.0767 Reco1(jets)=0.3678 Reco2(lep/photon/MET)=0.3092 DisCo=0.0014\n",
      "[EPOCH 66/100] Loss=2.1127 Reco1(jets)=0.3599 Reco2(lep/photon/MET)=0.3098 DisCo=0.0014\n",
      "[EPOCH 67/100] Loss=1.8892 Reco1(jets)=0.3611 Reco2(lep/photon/MET)=0.3072 DisCo=0.0012\n",
      "[EPOCH 68/100] Loss=2.1376 Reco1(jets)=0.3610 Reco2(lep/photon/MET)=0.3125 DisCo=0.0015\n",
      "[EPOCH 69/100] Loss=1.9764 Reco1(jets)=0.3609 Reco2(lep/photon/MET)=0.3033 DisCo=0.0013\n",
      "[EPOCH 70/100] Loss=2.0384 Reco1(jets)=0.3587 Reco2(lep/photon/MET)=0.3091 DisCo=0.0014\n",
      "[EPOCH 71/100] Loss=2.1395 Reco1(jets)=0.3597 Reco2(lep/photon/MET)=0.3119 DisCo=0.0015\n",
      "[EPOCH 72/100] Loss=1.8583 Reco1(jets)=0.3589 Reco2(lep/photon/MET)=0.3048 DisCo=0.0012\n",
      "[EPOCH 73/100] Loss=1.9403 Reco1(jets)=0.3621 Reco2(lep/photon/MET)=0.3025 DisCo=0.0013\n",
      "[EPOCH 74/100] Loss=1.9750 Reco1(jets)=0.3582 Reco2(lep/photon/MET)=0.3009 DisCo=0.0013\n",
      "[EPOCH 75/100] Loss=2.0039 Reco1(jets)=0.3602 Reco2(lep/photon/MET)=0.3064 DisCo=0.0013\n",
      "[EPOCH 76/100] Loss=2.1359 Reco1(jets)=0.3565 Reco2(lep/photon/MET)=0.3060 DisCo=0.0015\n",
      "[EPOCH 77/100] Loss=2.0149 Reco1(jets)=0.3572 Reco2(lep/photon/MET)=0.3045 DisCo=0.0014\n",
      "[EPOCH 78/100] Loss=2.8505 Reco1(jets)=0.3538 Reco2(lep/photon/MET)=0.3030 DisCo=0.0022\n",
      "[EPOCH 79/100] Loss=2.1932 Reco1(jets)=0.3530 Reco2(lep/photon/MET)=0.2998 DisCo=0.0015\n",
      "[EPOCH 80/100] Loss=2.1054 Reco1(jets)=0.3581 Reco2(lep/photon/MET)=0.3050 DisCo=0.0014\n",
      "[EPOCH 81/100] Loss=2.0221 Reco1(jets)=0.3588 Reco2(lep/photon/MET)=0.3022 DisCo=0.0014\n",
      "[EPOCH 82/100] Loss=2.0340 Reco1(jets)=0.3537 Reco2(lep/photon/MET)=0.2991 DisCo=0.0014\n",
      "[EPOCH 83/100] Loss=2.0360 Reco1(jets)=0.3535 Reco2(lep/photon/MET)=0.3010 DisCo=0.0014\n",
      "[EPOCH 84/100] Loss=2.9167 Reco1(jets)=0.3538 Reco2(lep/photon/MET)=0.2990 DisCo=0.0023\n",
      "[EPOCH 85/100] Loss=2.1868 Reco1(jets)=0.3535 Reco2(lep/photon/MET)=0.3009 DisCo=0.0015\n",
      "[EPOCH 86/100] Loss=1.8413 Reco1(jets)=0.3531 Reco2(lep/photon/MET)=0.2971 DisCo=0.0012\n",
      "[EPOCH 87/100] Loss=2.1788 Reco1(jets)=0.3525 Reco2(lep/photon/MET)=0.3025 DisCo=0.0015\n",
      "[EPOCH 88/100] Loss=1.9016 Reco1(jets)=0.3537 Reco2(lep/photon/MET)=0.3005 DisCo=0.0012\n",
      "[EPOCH 89/100] Loss=2.0240 Reco1(jets)=0.3553 Reco2(lep/photon/MET)=0.2976 DisCo=0.0014\n",
      "[EPOCH 90/100] Loss=3.0249 Reco1(jets)=0.3555 Reco2(lep/photon/MET)=0.3008 DisCo=0.0024\n",
      "[EPOCH 91/100] Loss=3.5281 Reco1(jets)=0.3550 Reco2(lep/photon/MET)=0.2986 DisCo=0.0029\n",
      "[EPOCH 92/100] Loss=1.9221 Reco1(jets)=0.3576 Reco2(lep/photon/MET)=0.2933 DisCo=0.0013\n",
      "[EPOCH 93/100] Loss=2.3415 Reco1(jets)=0.3538 Reco2(lep/photon/MET)=0.2973 DisCo=0.0017\n",
      "[EPOCH 94/100] Loss=2.0431 Reco1(jets)=0.3511 Reco2(lep/photon/MET)=0.2989 DisCo=0.0014\n",
      "[EPOCH 95/100] Loss=1.8464 Reco1(jets)=0.3537 Reco2(lep/photon/MET)=0.2972 DisCo=0.0012\n",
      "[EPOCH 96/100] Loss=2.1384 Reco1(jets)=0.3536 Reco2(lep/photon/MET)=0.2986 DisCo=0.0015\n",
      "[EPOCH 97/100] Loss=2.0989 Reco1(jets)=0.3488 Reco2(lep/photon/MET)=0.2972 DisCo=0.0015\n",
      "[EPOCH 98/100] Loss=3.2034 Reco1(jets)=0.3552 Reco2(lep/photon/MET)=0.2964 DisCo=0.0026\n",
      "[EPOCH 99/100] Loss=2.1253 Reco1(jets)=0.3527 Reco2(lep/photon/MET)=0.2975 DisCo=0.0015\n",
      "Finished training.\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'ae_lr': 1e-4,\n",
    "    'alpha': 0.5,\n",
    "    'ae_latent': 8,\n",
    "    'ae_nodes': [28, 14],\n",
    "    'lambda_disco':1000.0\n",
    "}\n",
    "\n",
    "run(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2761078-2f9b-4392-949d-43621fcfdcf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d8dc93-7bd0-4d9c-ae29-c645824cb908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
