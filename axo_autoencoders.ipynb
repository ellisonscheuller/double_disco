{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a5269f0-7f2e-40fd-917b-c3062e30516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gc\n",
    "import argparse\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "from models.autoencoder import Autoencoder \n",
    "from losses.cyl_ptpz_mae import CylPtPzMAE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0252ca81-bc87-49de-bad4-be27f0f4ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting a seed like in ae_legacy\n",
    "def set_seed(seed=123):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a33be852-c54f-4652-b770-402d2add7e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_corr(var_1,var_2,normedweight,power=1):\n",
    "    \"\"\"var_1: First variable to decorrelate (eg mass)\n",
    "    var_2: Second variable to decorrelate (eg classifier output)\n",
    "    normedweight: Per-example weight. Sum of weights should add up to N (where N is the number of examples)\n",
    "    power: Exponent used in calculating the distance correlation\n",
    "    \n",
    "    va1_1, var_2 and normedweight should all be 1D torch tensors with the same number of entries\n",
    "    \n",
    "    Usage: Add to your loss function. total_loss = BCE_loss + lambda * distance_corr\n",
    "    \"\"\" \n",
    "    \n",
    "    xx = var_1.view(-1, 1).repeat(1, len(var_1)).view(len(var_1),len(var_1))\n",
    "    yy = var_1.repeat(len(var_1),1).view(len(var_1),len(var_1))\n",
    "    amat = (xx-yy).abs()\n",
    "\n",
    "    xx = var_2.view(-1, 1).repeat(1, len(var_2)).view(len(var_2),len(var_2))\n",
    "    yy = var_2.repeat(len(var_2),1).view(len(var_2),len(var_2))\n",
    "    bmat = (xx-yy).abs()\n",
    "\n",
    "    amatavg = torch.mean(amat*normedweight,dim=1)\n",
    "    Amat=amat-amatavg.repeat(len(var_1),1).view(len(var_1),len(var_1))\\\n",
    "        -amatavg.view(-1, 1).repeat(1, len(var_1)).view(len(var_1),len(var_1))\\\n",
    "        +torch.mean(amatavg*normedweight)\n",
    "\n",
    "    bmatavg = torch.mean(bmat*normedweight,dim=1)\n",
    "    Bmat=bmat-bmatavg.repeat(len(var_2),1).view(len(var_2),len(var_2))\\\n",
    "        -bmatavg.view(-1, 1).repeat(1, len(var_2)).view(len(var_2),len(var_2))\\\n",
    "        +torch.mean(bmatavg*normedweight)\n",
    "\n",
    "    ABavg = torch.mean(Amat*Bmat*normedweight,dim=1)\n",
    "    AAavg = torch.mean(Amat*Amat*normedweight,dim=1)\n",
    "    BBavg = torch.mean(Bmat*Bmat*normedweight,dim=1)\n",
    "\n",
    "    if(power==1):\n",
    "        dCorr=(torch.mean(ABavg*normedweight))/torch.sqrt((torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight)))\n",
    "    elif(power==2):\n",
    "        dCorr=(torch.mean(ABavg*normedweight))**2/(torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight))\n",
    "    else:\n",
    "        dCorr=((torch.mean(ABavg*normedweight))/torch.sqrt((torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight))))**power\n",
    "    \n",
    "    return dCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "320ba38b-4c09-4ea0-b883-6f7011de2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates anomaly score like in ae legacy\n",
    "def distance_pt(model_ae, data_np, device):\n",
    "    x = torch.tensor(data_np, dtype=torch.float32, device=device)\n",
    "    z_mean, z_logvar, _ = model_ae.encoder(x)\n",
    "    score = torch.sum(z_mean**2, dim=1)\n",
    "    return score.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df9cf7a6-624b-491c-ad49-5eca472a676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to make 2D histograms\n",
    "def make_2D_hist(x, y, xlabel, ylabel, title, wandb_key, bins=40):\n",
    "    fig = plt.figure(figsize=(5,4))\n",
    "    plt.hist2d(x, y, bins=bins)\n",
    "    plt.colorbar(label='Counts')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    wandb.log({wandb_key: wandb.Image(fig)})\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70f9494b-d8c8-4bf6-b3a0-1d2850958bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints h5 tree to examine\n",
    "def print_h5_tree(h, prefix=\"\"):\n",
    "    for k in h.keys():\n",
    "        item = h[k]\n",
    "        if hasattr(item, 'keys'):\n",
    "            print(prefix + f\"[GROUP] {k}\")\n",
    "            print_h5_tree(item, prefix + \"  \")\n",
    "        else:\n",
    "            try:\n",
    "                print(prefix + f\"{k}: shape={item.shape}, dtype={item.dtype}\")\n",
    "            except Exception:\n",
    "                print(prefix + f\"{k}: <dataset>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "540a813e-5d61-4ce4-9593-d2ef7e03bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config):\n",
    "    #set seed\n",
    "    seed = 123\n",
    "    set_seed(seed)\n",
    "\n",
    "    #move to gpu if avail\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using: {device}\")\n",
    "\n",
    "    #login to wandb\n",
    "    print(\"Logging in to wandb...\")\n",
    "    wandb.login(key=\"24d1d60ce26563c74d290d7b487cb104fc251271\")\n",
    "    wandb.init(project=\"Double Disco Axo Training\",\n",
    "               settings=wandb.Settings(_disable_stats=True),\n",
    "               config=config)\n",
    "    run_name = wandb.run.name\n",
    "    print(f\"Run name: {run_name}\")\n",
    "\n",
    "    #scaling\n",
    "    alpha = float(config['alpha'])\n",
    "    ae_lr = float(config['ae_lr'])\n",
    "\n",
    "    #load data\n",
    "    print(\"Loading dataset...\")\n",
    "    fpath = '/axovol/training/v5/conditionsupdate_apr25.h5'\n",
    "    with h5.File(fpath, 'r') as f:\n",
    "        root = f['data'] if 'data' in f else f\n",
    "\n",
    "        #print h5 tree to view\n",
    "        # print(\"Printing h5 tree...\")\n",
    "        # print_h5_tree(root)\n",
    "    \n",
    "        x_train = root['Background_data']['Train']['DATA'][:]\n",
    "        x_test = root['Background_data']['Test']['DATA'][:]\n",
    "        print(f\"Train shape: {x_train.shape}, Test shape: {x_test.shape}\")\n",
    "    \n",
    "        #flatten per event\n",
    "        x_train_bkg = x_train.reshape(x_train.shape[0], -1)\n",
    "        x_test_bkg  = x_test.reshape(x_test.shape[0], -1)\n",
    "    \n",
    "        scale = root['Normalisation']['norm_scale'][:]\n",
    "        bias = root['Normalisation']['norm_bias'][:]\n",
    "    \n",
    "        l1_bits_bkg_test = root['Background_data']['Test']['L1bits'][:]\n",
    "    \n",
    "        #load signal data\n",
    "        SIGNAL_NAMES = list(root['Signal_data'].keys())\n",
    "        signal_data_dict = {}\n",
    "        signal_l1_dict = {}\n",
    "        for sname in SIGNAL_NAMES:\n",
    "            x_sig = root['Signal_data'][sname]['DATA'][:]\n",
    "            x_sig = x_sig.reshape(x_sig.shape[0], -1)\n",
    "            l1_bits = root['Signal_data'][sname]['L1bits'][:]\n",
    "            signal_data_dict[sname] = x_sig\n",
    "            signal_l1_dict[sname] = l1_bits\n",
    "\n",
    "    print(\"Data finished loading.\")\n",
    "    \n",
    "    dataset = x_train_bkg\n",
    "    dataset_test = x_test_bkg\n",
    "\n",
    "    #should be 57\n",
    "    features = dataset.shape[1]\n",
    "\n",
    "\n",
    "    print(\"Building mask...\")\n",
    "    # same mask from ae legacy\n",
    "    mask_dict = {\n",
    "        \"MET\":[True],\n",
    "        \"EGAMMA\":[True,True,True,True,False,False,False,False,False,False,False,False],\n",
    "        \"MUON\":[True,True,True,True,False,False,False,False],\n",
    "        \"JET\":[True,True,True,True,True,True,True,True,True,True,False,False]\n",
    "    }\n",
    "    \n",
    "    #build cyl_ptpz_mae mask (input scales and biases)\n",
    "    reco_loss_fn = CylPtPzMAE(scale, bias).to(device)\n",
    "    print(\"Mask is ready.\")\n",
    "\n",
    "\n",
    "    #ae config\n",
    "    latent_dim = int(config['ae_latent'])\n",
    "    enc_nodes = list(config['ae_nodes'])\n",
    "    dec_nodes = [24, 32, 64, 128, features] \n",
    "\n",
    "    ae_cfg = {\n",
    "        \"features\": features,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"encoder_config\": {\"nodes\": enc_nodes},\n",
    "        \"decoder_config\": {\"nodes\": dec_nodes},\n",
    "        \"alpha\": alpha\n",
    "    }\n",
    "\n",
    "    #put ae on device/init\n",
    "    ae_1 = Autoencoder(ae_cfg).to(device)\n",
    "    ae_2 = Autoencoder(ae_cfg).to(device)\n",
    "    print(\"Autoencoders are ready.\")\n",
    "\n",
    "    #optimizer (adam)\n",
    "    optimizer = torch.optim.Adam(list(ae_1.parameters()) + list(ae_2.parameters()), lr=ae_lr)\n",
    "\n",
    "    #cosine restarts\n",
    "    warmup_epochs = 10\n",
    "    cos = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=32, T_mult=2, eta_min=0.0)\n",
    "\n",
    "    #sets learning rate\n",
    "    def set_lr(lr):\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "\n",
    "    #hyperparameters\n",
    "    Epochs_AE = 50\n",
    "    Batch_size = 16384\n",
    "\n",
    "    #get disco param from config, else set to 1\n",
    "    lambda_disco = float(config.get(\"lambda_disco\", 1))\n",
    "\n",
    "    print(\"Moving data to device...\")\n",
    "    X = torch.tensor(dataset, dtype=torch.float32, device=device)\n",
    "    print(\"Data on device.\")\n",
    "\n",
    "    #training loop\n",
    "    print(\"Starting the training loop!\")\n",
    "    N = X.size(0)\n",
    "    for epoch in range(Epochs_AE):\n",
    "\n",
    "        #init lists for 2D plotting\n",
    "        ae1_reco_loss = []\n",
    "        ae2_reco_loss = []\n",
    "        \n",
    "        #cosine warmup step (sets the learning rate based on cosine schedule)\n",
    "        if epoch < warmup_epochs:\n",
    "            lr = ae_lr * (epoch + 1) / warmup_epochs\n",
    "            set_lr(lr)\n",
    "        else:\n",
    "            cos.step(epoch - warmup_epochs)\n",
    "\n",
    "        #shuffles data incides for each epoch\n",
    "        perm = torch.randperm(N, device=device)\n",
    "\n",
    "        #init losses\n",
    "        total_loss = total_reco1 = total_reco2 = total_disco = 0.0\n",
    "\n",
    "        #loops over dataset in steps of batch sizze\n",
    "        for i in range(0, N, Batch_size):\n",
    "            #picks indices for current batch\n",
    "            idx = perm[i:i+Batch_size]\n",
    "\n",
    "            #selects batch of samples from data set X\n",
    "            xb = X[idx]\n",
    "\n",
    "            #ae 1\n",
    "            recon1, z1 = ae_1(xb)\n",
    "            \n",
    "            #ae 2\n",
    "            recon2, z2 = ae_2(xb)\n",
    "\n",
    "            #get reco loss from custom func\n",
    "            reco1_per = reco_loss_fn(recon1, xb)\n",
    "            reco2_per = reco_loss_fn(recon2, xb)\n",
    "\n",
    "            #from paper code weight\n",
    "            B = xb.shape[0]\n",
    "            w = torch.ones(B, device=reco1_per.device, dtype=reco1_per.dtype)\n",
    "\n",
    "            #disco loss (ask Melissa about since using mu instead of z)\n",
    "            #disco = disco_loss(mu1, mu2)\n",
    "            disco = distance_corr((reco1_per), (reco2_per), w, power=1)\n",
    "\n",
    "            #same scaling from ae legacy\n",
    "            reco1 = ae_1.alpha*reco1_per.mean()\n",
    "            reco2 = ae_2.alpha*reco2_per.mean()\n",
    "\n",
    "            #calc total loss (scalar)\n",
    "            loss = reco1 + reco2 + lambda_disco * disco\n",
    "\n",
    "            #zero grads\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            #backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            #do some gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(list(ae_1.parameters()) + list(ae_2.parameters()), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            #add losses to list for wandb plotting\n",
    "            total_loss+=loss.item()\n",
    "            total_reco1+=reco1.item()\n",
    "            total_reco2+=reco2.item()\n",
    "            total_disco+=disco.item()\n",
    "\n",
    "            #add loss to list for 2d plotting\n",
    "            ae1_reco_loss.append(reco1.item())\n",
    "            ae2_reco_loss.append(reco2.item())\n",
    "\n",
    "        print(f\"[EPOCH {epoch}/{Epochs_ae}] \"\n",
    "          f\"Loss={total_loss:.4f} \"\n",
    "          f\"Reco1={total_reco1:.4f} Reco2={total_reco2:.4f} \"\n",
    "          f\"DisCo={total_disco:.4f}\")\n",
    "\n",
    "        #log in wandb\n",
    "        wandb.log({\n",
    "            \"Epochae\": epoch,\n",
    "            \"TotalLossae\": total_loss,\n",
    "            \"RecoLossae1\": total_reco1,\n",
    "            \"RecoLossae2\": total_reco2,\n",
    "            \"DisCoLoss\": total_disco,\n",
    "        })\n",
    "\n",
    "        #convert to np array for plot\n",
    "        ae1_reco_np = np.array(ae1_reco_loss)\n",
    "        ae2_reco_np = np.array(ae2_reco_loss)\n",
    "\n",
    "        make_2D_hist(ae1_reco_np, ae2_reco_np,\n",
    "                    \"Reco Loss (ae1)\", \"Reco Loss (ae2)\",\n",
    "                    f\"Epoch {epoch}: Reco ae1 vs Reco ae2\",\n",
    "                    wandb_key=\"Hists2D/Reco_ae1_vs_Reco_ae2\")\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"Finished training.\")\n",
    "\n",
    "    #save models\n",
    "    torch.save(ae_1.state_dict(), \"ae1_trained.pth\")\n",
    "    torch.save(ae_2.state_dict(), \"ae2_trained.pth\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8160a1fc-439a-429f-a154-7c134e0ecfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n",
      "Logging in to wandb...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">proud-haze-129</strong> at: <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/dya1j3sw' target=\"_blank\">https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/dya1j3sw</a><br> View project at: <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training' target=\"_blank\">https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250829_151151-dya1j3sw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20250829_152045-be6rby98</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/be6rby98' target=\"_blank\">fanciful-smoke-130</a></strong> to <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training' target=\"_blank\">https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/be6rby98' target=\"_blank\">https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/be6rby98</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: fanciful-smoke-130\n",
      "Loading dataset...\n",
      "Train shape: (1999965, 19, 3), Test shape: (4511092, 19, 3)\n",
      "Data finished loading.\n",
      "Building mask...\n",
      "Mask is ready.\n",
      "Autoencoders are ready.\n",
      "Moving data to device...\n",
      "Data on device.\n",
      "Starting the training loop!\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 179.25 MiB is free. Process 614511 has 10.72 GiB memory in use. Of the allocated memory 10.08 GiB is allocated by PyTorch, and 500.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mae_lr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1e-4\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda_disco\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10000.0\u001b[39m\n\u001b[1;32m      7\u001b[0m }\n\u001b[0;32m----> 9\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 167\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    163\u001b[0m w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(B, device\u001b[38;5;241m=\u001b[39mreco1_per\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mreco1_per\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m#disco loss (ask Melissa about since using mu instead of z)\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m#disco = disco_loss(mu1, mu2)\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m disco \u001b[38;5;241m=\u001b[39m \u001b[43mdistance_corr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreco1_per\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mreco2_per\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m#same scaling from ae legacy\u001b[39;00m\n\u001b[1;32m    170\u001b[0m reco1 \u001b[38;5;241m=\u001b[39m ae_1\u001b[38;5;241m.\u001b[39malpha\u001b[38;5;241m*\u001b[39mreco1_per\u001b[38;5;241m.\u001b[39mmean()\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36mdistance_corr\u001b[0;34m(var_1, var_2, normedweight, power)\u001b[0m\n\u001b[1;32m     16\u001b[0m xx \u001b[38;5;241m=\u001b[39m var_2\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(var_2))\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mlen\u001b[39m(var_2),\u001b[38;5;28mlen\u001b[39m(var_2))\n\u001b[1;32m     17\u001b[0m yy \u001b[38;5;241m=\u001b[39m var_2\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mlen\u001b[39m(var_2),\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mlen\u001b[39m(var_2),\u001b[38;5;28mlen\u001b[39m(var_2))\n\u001b[0;32m---> 18\u001b[0m bmat \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mxx\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43myy\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m amatavg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(amat\u001b[38;5;241m*\u001b[39mnormedweight,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m Amat\u001b[38;5;241m=\u001b[39mamat\u001b[38;5;241m-\u001b[39mamatavg\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mlen\u001b[39m(var_1),\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mlen\u001b[39m(var_1),\u001b[38;5;28mlen\u001b[39m(var_1))\\\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;241m-\u001b[39mamatavg\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(var_1))\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mlen\u001b[39m(var_1),\u001b[38;5;28mlen\u001b[39m(var_1))\\\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;241m+\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(amatavg\u001b[38;5;241m*\u001b[39mnormedweight)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 179.25 MiB is free. Process 614511 has 10.72 GiB memory in use. Of the allocated memory 10.08 GiB is allocated by PyTorch, and 500.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'ae_lr': 1e-4,\n",
    "    'alpha': 0.5,\n",
    "    'ae_latent': 8,\n",
    "    'ae_nodes': [28, 14],\n",
    "    'lambda_disco': 10000.0\n",
    "}\n",
    "\n",
    "run(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6994c81-b5dc-48a3-9a76-930bd037059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload data same way as in traiining\n",
    "print(\"Loading dataset...\")\n",
    "fpath = '/axovol/training/v5/conditionsupdate_apr25.h5'\n",
    "with h5.File(fpath, 'r') as f:\n",
    "    root = f['data'] if 'data' in f else f\n",
    "\n",
    "    x_train = root['Background_data']['Train']['DATA'][:]\n",
    "    x_test  = root['Background_data']['Test']['DATA'][:]\n",
    "    print(f\"Train shape: {x_train.shape}, Test shape: {x_test.shape}\")\n",
    "\n",
    "    #flatten per event\n",
    "    x_train_bkg = x_train.reshape(x_train.shape[0], -1)\n",
    "    x_test_bkg  = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "    scale = root['Normalisation']['norm_scale'][:]\n",
    "    bias  = root['Normalisation']['norm_bias'][:]\n",
    "\n",
    "    l1_bits_bkg_test = root['Background_data']['Test']['L1bits'][:]\n",
    "\n",
    "    print(\"Available signal names:\")\n",
    "    print(list(root['Signal_data'].keys()))\n",
    "\n",
    "    #load signal data\n",
    "    x_sig = root['Signal_data']['GluGluHToTauTau']['DATA'][:]\n",
    "    x_sig = x_sig.reshape(x_sig.shape[0], -1)\n",
    "    l1_bits_sig = root['Signal_data']['GluGluHToTauTau']['L1bits'][:]\n",
    "\n",
    "print(\"Data finished loading.\")\n",
    "\n",
    "features = x_test_bkg.shape[1]\n",
    "\n",
    "# reload config based on training\n",
    "cfg = {\n",
    "    \"features\": features,\n",
    "    \"latent_dim\": 8,         \n",
    "    \"encoder_config\": {\"nodes\": [28, 14]}, \n",
    "    \"decoder_config\": {\"nodes\": [24, 32, 64, 128, features]}, \n",
    "    \"alpha\": 0.5,\n",
    "    \"beta\": 0.5\n",
    "}\n",
    "\n",
    "ae_1 = ae(cfg).to(device)\n",
    "ae_2 = ae(cfg).to(device)\n",
    "\n",
    "#load model weights\n",
    "ae_1.load_state_dict(torch.load(\"ae1_trained.pth\", map_location=device))\n",
    "ae_2.load_state_dict(torch.load(\"ae2_trained.pth\", map_location=device))\n",
    "ae_1.eval()\n",
    "ae_2.eval()\n",
    "\n",
    "#compute anomaly scores\n",
    "bkg_scores_1 = distance_pt(ae_1, x_test_bkg, device)\n",
    "sig_scores_1 = distance_pt(ae_1, x_sig, device)\n",
    "\n",
    "bkg_scores_2 = distance_pt(ae_2, x_test_bkg, device)\n",
    "sig_scores_2 = distance_pt(ae_2, x_sig, device)\n",
    "\n",
    "#ae 1 anomaly score plot\n",
    "plt.figure(figsize=(6,4))\n",
    "bins = np.linspace(np.percentile(bkg_scores_1, 0.5), np.percentile(bkg_scores_1, 99.5), 60)\n",
    "plt.hist(bkg_scores_1, bins=bins, alpha=0.5, label='Background')\n",
    "plt.hist(sig_scores_1, bins=bins, alpha=0.5, label=\"GluGluHToTauTau\")\n",
    "plt.xlabel(\"Anomaly score\")\n",
    "plt.ylabel(\"Events\")\n",
    "plt.legend()\n",
    "plt.title(\"ae 1 Anomaly Scores\")\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "#ae 1 ROC curve\n",
    "y_true  = np.concatenate([np.zeros_like(bkg_scores_1), np.ones_like(sig_scores_1)])\n",
    "y_score = np.concatenate([bkg_scores_1, sig_scores_1])\n",
    "fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(fpr, tpr, lw=2, label=f\"AUC = {roc_auc:.3f}\")\n",
    "plt.plot([0,1],[0,1],'--', lw=1, color='gray')\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(f\"ROC ae 1\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#ae 2 anomaly score plot\n",
    "plt.figure(figsize=(6,4))\n",
    "bins = np.linspace(np.percentile(bkg_scores_2, 0.5), np.percentile(bkg_scores_2, 99.5), 60)\n",
    "plt.hist(bkg_scores_2, bins=bins, alpha=0.6, label='Background')\n",
    "plt.hist(sig_scores_2, bins=bins,alpha=0.6, label=\"GluGluHToTauTau\")\n",
    "plt.xlabel(\"Anomaly score\")\n",
    "plt.ylabel(\"Events\")\n",
    "plt.legend()\n",
    "plt.title(\"ae 2 anomaly scores\")\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "#ae 2 ROC curve plot\n",
    "y_true  = np.concatenate([np.zeros_like(bkg_scores_2), np.ones_like(sig_scores_2)])\n",
    "y_score = np.concatenate([bkg_scores_2, sig_scores_2])\n",
    "fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(fpr, tpr, lw=2, label=f\"AUC = {roc_auc:.3f}\")\n",
    "plt.plot([0,1],[0,1],'--', lw=1, color='gray')\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(f\"ROC ae 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2761078-2f9b-4392-949d-43621fcfdcf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
