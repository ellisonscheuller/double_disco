{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a5269f0-7f2e-40fd-917b-c3062e30516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gc\n",
    "import argparse\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "from models.vae import VAE \n",
    "from losses.cyl_ptpz_mae import CylPtPzMAE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0252ca81-bc87-49de-bad4-be27f0f4ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting a seed like in vae_legacy\n",
    "def set_seed(seed=123):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89562028-a18a-4aa5-8f3a-ad75bc4468ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculated disco loss based on paper\n",
    "def disco_loss(z1, z2):\n",
    "    #center around 0\n",
    "    x = z1 - z1.mean(0)\n",
    "    y = z2 - z2.mean(0)\n",
    "    #build euclidean distance matrices\n",
    "    a = torch.cdist(x, x)\n",
    "    b = torch.cdist(y, y)\n",
    "    #doublce center distance matrices\n",
    "    A = a - a.mean(0) - a.mean(1, keepdim=True) + a.mean()\n",
    "    B = b - b.mean(0) - b.mean(1, keepdim=True) + b.mean()\n",
    "    #find covariance\n",
    "    dcov = (A * B).mean()\n",
    "    #find variance and take square root (in paper it is not squared)\n",
    "    dvar_x = (A * A).mean().sqrt()\n",
    "    dvar_y = (B * B).mean().sqrt()\n",
    "    #return paper formula for disco\n",
    "    return dcov / (dvar_x * dvar_y + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a33be852-c54f-4652-b770-402d2add7e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_corr(var_1,var_2,normedweight,power=1):\n",
    "    \"\"\"var_1: First variable to decorrelate (eg mass)\n",
    "    var_2: Second variable to decorrelate (eg classifier output)\n",
    "    normedweight: Per-example weight. Sum of weights should add up to N (where N is the number of examples)\n",
    "    power: Exponent used in calculating the distance correlation\n",
    "    \n",
    "    va1_1, var_2 and normedweight should all be 1D torch tensors with the same number of entries\n",
    "    \n",
    "    Usage: Add to your loss function. total_loss = BCE_loss + lambda * distance_corr\n",
    "    \"\"\" \n",
    "    \n",
    "    xx = var_1.view(-1, 1).repeat(1, len(var_1)).view(len(var_1),len(var_1))\n",
    "    yy = var_1.repeat(len(var_1),1).view(len(var_1),len(var_1))\n",
    "    amat = (xx-yy).abs()\n",
    "\n",
    "    xx = var_2.view(-1, 1).repeat(1, len(var_2)).view(len(var_2),len(var_2))\n",
    "    yy = var_2.repeat(len(var_2),1).view(len(var_2),len(var_2))\n",
    "    bmat = (xx-yy).abs()\n",
    "\n",
    "    amatavg = torch.mean(amat*normedweight,dim=1)\n",
    "    Amat=amat-amatavg.repeat(len(var_1),1).view(len(var_1),len(var_1))\\\n",
    "        -amatavg.view(-1, 1).repeat(1, len(var_1)).view(len(var_1),len(var_1))\\\n",
    "        +torch.mean(amatavg*normedweight)\n",
    "\n",
    "    bmatavg = torch.mean(bmat*normedweight,dim=1)\n",
    "    Bmat=bmat-bmatavg.repeat(len(var_2),1).view(len(var_2),len(var_2))\\\n",
    "        -bmatavg.view(-1, 1).repeat(1, len(var_2)).view(len(var_2),len(var_2))\\\n",
    "        +torch.mean(bmatavg*normedweight)\n",
    "\n",
    "    ABavg = torch.mean(Amat*Bmat*normedweight,dim=1)\n",
    "    AAavg = torch.mean(Amat*Amat*normedweight,dim=1)\n",
    "    BBavg = torch.mean(Bmat*Bmat*normedweight,dim=1)\n",
    "\n",
    "    if(power==1):\n",
    "        dCorr=(torch.mean(ABavg*normedweight))/torch.sqrt((torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight)))\n",
    "    elif(power==2):\n",
    "        dCorr=(torch.mean(ABavg*normedweight))**2/(torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight))\n",
    "    else:\n",
    "        dCorr=((torch.mean(ABavg*normedweight))/torch.sqrt((torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight))))**power\n",
    "    \n",
    "    return dCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "320ba38b-4c09-4ea0-b883-6f7011de2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates anomaly score like in vae legacy\n",
    "def distance_pt(model_vae, data_np, device):\n",
    "    x = torch.tensor(data_np, dtype=torch.float32, device=device)\n",
    "    z_mean, z_logvar, _ = model_vae.encoder(x)\n",
    "    score = torch.sum(z_mean**2, dim=1)\n",
    "    return score.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df9cf7a6-624b-491c-ad49-5eca472a676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to make 2D histograms\n",
    "def make_2D_hist(x, y, xlabel, ylabel, title, wandb_key, bins=40):\n",
    "    fig = plt.figure(figsize=(5,4))\n",
    "    plt.hist2d(x, y, bins=bins)\n",
    "    plt.colorbar(label='Counts')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    wandb.log({wandb_key: wandb.Image(fig)})\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70f9494b-d8c8-4bf6-b3a0-1d2850958bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints h5 tree to examine\n",
    "def print_h5_tree(h, prefix=\"\"):\n",
    "    for k in h.keys():\n",
    "        item = h[k]\n",
    "        if hasattr(item, 'keys'):\n",
    "            print(prefix + f\"[GROUP] {k}\")\n",
    "            print_h5_tree(item, prefix + \"  \")\n",
    "        else:\n",
    "            try:\n",
    "                print(prefix + f\"{k}: shape={item.shape}, dtype={item.dtype}\")\n",
    "            except Exception:\n",
    "                print(prefix + f\"{k}: <dataset>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "540a813e-5d61-4ce4-9593-d2ef7e03bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config):\n",
    "    #set seed\n",
    "    seed = 123\n",
    "    set_seed(seed)\n",
    "\n",
    "    #move to gpu if avail\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using: {device}\")\n",
    "\n",
    "    #login to wandb\n",
    "    print(\"Logging in to wandb...\")\n",
    "    wandb.login(key=\"24d1d60ce26563c74d290d7b487cb104fc251271\")\n",
    "    wandb.init(project=\"Double Disco Axo Training\",\n",
    "               settings=wandb.Settings(_disable_stats=True),\n",
    "               config=config)\n",
    "    run_name = wandb.run.name\n",
    "    print(f\"Run name: {run_name}\")\n",
    "\n",
    "    #scaling\n",
    "    beta = float(config['beta'])\n",
    "    alpha = float(config['alpha'])\n",
    "    vae_lr = float(config['vae_lr'])\n",
    "\n",
    "    #load data\n",
    "    print(\"Loading dataset...\")\n",
    "    fpath = '/axovol/training/v5/conditionsupdate_apr25.h5'\n",
    "    with h5.File(fpath, 'r') as f:\n",
    "        root = f['data'] if 'data' in f else f\n",
    "\n",
    "        #print h5 tree to view\n",
    "        # print(\"Printing h5 tree...\")\n",
    "        # print_h5_tree(root)\n",
    "    \n",
    "        x_train = root['Background_data']['Train']['DATA'][:]\n",
    "        x_test = root['Background_data']['Test']['DATA'][:]\n",
    "        print(f\"Train shape: {x_train.shape}, Test shape: {x_test.shape}\")\n",
    "    \n",
    "        #flatten per event\n",
    "        x_train_bkg = x_train.reshape(x_train.shape[0], -1)\n",
    "        x_test_bkg  = x_test.reshape(x_test.shape[0], -1)\n",
    "    \n",
    "        scale = root['Normalisation']['norm_scale'][:]\n",
    "        bias = root['Normalisation']['norm_bias'][:]\n",
    "    \n",
    "        l1_bits_bkg_test = root['Background_data']['Test']['L1bits'][:]\n",
    "    \n",
    "        #load signal data\n",
    "        SIGNAL_NAMES = list(root['Signal_data'].keys())\n",
    "        signal_data_dict = {}\n",
    "        signal_l1_dict = {}\n",
    "        for sname in SIGNAL_NAMES:\n",
    "            x_sig = root['Signal_data'][sname]['DATA'][:]\n",
    "            x_sig = x_sig.reshape(x_sig.shape[0], -1)\n",
    "            l1_bits = root['Signal_data'][sname]['L1bits'][:]\n",
    "            signal_data_dict[sname] = x_sig\n",
    "            signal_l1_dict[sname] = l1_bits\n",
    "\n",
    "    print(\"Data finished loading.\")\n",
    "    \n",
    "    dataset = x_train_bkg\n",
    "    dataset_test = x_test_bkg\n",
    "\n",
    "    #should be 57\n",
    "    features = dataset.shape[1]\n",
    "\n",
    "\n",
    "    print(\"Building mask...\")\n",
    "    # same mask from vae legacy\n",
    "    mask_dict = {\n",
    "        \"MET\":[True],\n",
    "        \"EGAMMA\":[True,True,True,True,False,False,False,False,False,False,False,False],\n",
    "        \"MUON\":[True,True,True,True,False,False,False,False],\n",
    "        \"JET\":[True,True,True,True,True,True,True,True,True,True,False,False]\n",
    "    }\n",
    "    \n",
    "    #build cyl_ptpz_mae mask (input scales and biases)\n",
    "    reco_loss_fn = CylPtPzMAE(scale, bias).to(device)\n",
    "    print(\"Mask is ready.\")\n",
    "\n",
    "\n",
    "    #vae config\n",
    "    latent_dim = int(config['vae_latent'])\n",
    "    enc_nodes = list(config['vae_nodes'])\n",
    "    dec_nodes = [24, 32, 64, 128, features] \n",
    "\n",
    "    vae_cfg = {\n",
    "        \"features\": features,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"encoder_config\": {\"nodes\": enc_nodes},\n",
    "        \"decoder_config\": {\"nodes\": dec_nodes},\n",
    "        \"alpha\": alpha,\n",
    "        \"beta\":  beta,\n",
    "    }\n",
    "\n",
    "    #put vae on device/init\n",
    "    vae_1 = VAE(vae_cfg).to(device)\n",
    "    vae_2 = VAE(vae_cfg).to(device)\n",
    "    print(\"VAEs are ready.\")\n",
    "\n",
    "    #optimizer (adam)\n",
    "    optimizer = torch.optim.Adam(list(vae_1.parameters()) + list(vae_2.parameters()), lr=vae_lr)\n",
    "\n",
    "    #cosine restarts\n",
    "    warmup_epochs = 10\n",
    "    cos = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=32, T_mult=2, eta_min=0.0)\n",
    "\n",
    "    #sets learning rate\n",
    "    def set_lr(lr):\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "\n",
    "    #hyperparameters\n",
    "    Epochs_VAE = 200\n",
    "    Batch_size = 16384\n",
    "\n",
    "    #get disco param from config, else set to 1\n",
    "    lambda_disco = float(config.get(\"lambda_disco\", 1))\n",
    "\n",
    "    print(\"Moving data to device...\")\n",
    "    X = torch.tensor(dataset, dtype=torch.float32, device=device)\n",
    "    print(\"Data on device.\")\n",
    "\n",
    "    #training loop\n",
    "    print(\"Starting the training loop!\")\n",
    "    N = X.size(0)\n",
    "    for epoch in range(Epochs_VAE):\n",
    "\n",
    "        #init lists for 2D plotting\n",
    "        vae1_total_loss = []\n",
    "        vae2_total_loss = []\n",
    "        vae1_kl_loss = []\n",
    "        vae2_kl_loss = []\n",
    "        vae1_reco_loss = []\n",
    "        vae2_reco_loss = []\n",
    "        \n",
    "        #cosine warmup step (sets the learning rate based on cosine schedule)\n",
    "        if epoch < warmup_epochs:\n",
    "            lr = vae_lr * (epoch + 1) / warmup_epochs\n",
    "            set_lr(lr)\n",
    "        else:\n",
    "            cos.step(epoch - warmup_epochs)\n",
    "\n",
    "        #shuffles data incides for each epoch\n",
    "        perm = torch.randperm(N, device=device)\n",
    "\n",
    "        #init losses\n",
    "        total_loss = total_reco1 = total_reco2 = total_kl1 = total_kl2 = total_disco = 0.0\n",
    "\n",
    "        #loops over dataset in steps of batch sizze\n",
    "        for i in range(0, N, Batch_size):\n",
    "            #picks indices for current batch\n",
    "            idx = perm[i:i+Batch_size]\n",
    "\n",
    "            #selects batch of samples from data set X\n",
    "            xb = X[idx]\n",
    "\n",
    "            #vae 1\n",
    "            recon1, mu1, logvar1, z1 = vae_1(xb)\n",
    "            \n",
    "            #vae 2\n",
    "            recon2, mu2, logvar2, z2 = vae_2(xb)\n",
    "\n",
    "            #get reco loss from custom func\n",
    "            reco1_per = reco_loss_fn(recon1, xb)\n",
    "            reco2_per = reco_loss_fn(recon2, xb)\n",
    "\n",
    "            #get kl div per sample\n",
    "            kl1_per = VAE.kl_divergence(mu1, logvar1)\n",
    "            kl2_per = VAE.kl_divergence(mu2, logvar2)\n",
    "\n",
    "            tot1_per = reco1_per + kl1_per\n",
    "            tot2_per = reco2_per + kl2_per\n",
    "\n",
    "            #from paper code weight\n",
    "            B = xb.shape[0]\n",
    "            w = torch.ones(B, device=tot1_per.device, dtype=tot1_per.dtype)\n",
    "\n",
    "            #disco loss (ask Melissa about since using mu instead of z)\n",
    "            #disco = disco_loss(mu1, mu2)\n",
    "            disco = distance_corr((tot1_per), (tot2_per), w, power=1)\n",
    "\n",
    "            #same scaling from vae legacy\n",
    "            reco1 = vae_1.reco_scale*reco1_per.mean()\n",
    "            reco2 = vae_2.reco_scale*reco2_per.mean()\n",
    "            kl1 = vae_1.kl_scale*kl1_per.mean()\n",
    "            kl2 = vae_2.kl_scale*kl2_per.mean() \n",
    "\n",
    "            #calc total loss (scalar)\n",
    "            loss = (reco1 + kl1) + (reco2 + kl2) + lambda_disco * disco\n",
    "\n",
    "            #zero grads\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            #backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            #do some gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(list(vae_1.parameters()) + list(vae_2.parameters()), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            #add losses to list for wandb plotting\n",
    "            total_loss+=loss.item()\n",
    "            total_reco1+=reco1.item()\n",
    "            total_reco2+=reco2.item()\n",
    "            total_kl1+=kl1.item()\n",
    "            total_kl2+=kl2.item()\n",
    "            total_disco+=disco.item()\n",
    "\n",
    "            #add loss to list for plotting\n",
    "            vae1_total_loss.append((reco1 + kl1).item())\n",
    "            vae2_total_loss.append((reco2 + kl2).item())\n",
    "            vae1_kl_loss.append(kl1.item())\n",
    "            vae2_kl_loss.append(kl2.item())\n",
    "            vae1_reco_loss.append(reco1.item())\n",
    "            vae2_reco_loss.append(reco2.item())\n",
    "\n",
    "        print(f\"[EPOCH {epoch}/{Epochs_VAE}] \"\n",
    "          f\"Loss={total_loss:.4f} \"\n",
    "          f\"Reco1={total_reco1:.4f} Reco2={total_reco2:.4f} \"\n",
    "          f\"KL1={total_kl1:.4f} KL2={total_kl2:.4f} \"\n",
    "          f\"DisCo={total_disco:.4f}\")\n",
    "\n",
    "        #log in wandb\n",
    "        wandb.log({\n",
    "            \"EpochVae\": epoch,\n",
    "            \"TotalLossVae\": total_loss,\n",
    "            \"RecoLossVae1\": total_reco1,\n",
    "            \"RecoLossVae2\": total_reco2,\n",
    "            \"KLLossVae1\": total_kl1,\n",
    "            \"KLLossVae2\": total_kl2,\n",
    "            \"DisCoLoss\": total_disco,\n",
    "        })\n",
    "\n",
    "        #convert to np array for plot\n",
    "        vae1_total_np = np.array(vae1_total_loss)\n",
    "        vae2_total_np = np.array(vae2_total_loss)\n",
    "        vae1_kl_np = np.array(vae1_kl_loss)\n",
    "        vae2_kl_np = np.array(vae2_kl_loss)\n",
    "        vae1_reco_np = np.array(vae1_reco_loss)\n",
    "        vae2_reco_np = np.array(vae2_reco_loss)\n",
    "\n",
    "        make_2D_hist(vae1_total_np, vae2_total_np,\n",
    "                    \"Total Loss (VAE1)\", \"Total Loss (VAE2)\",\n",
    "                    f\"Epoch {epoch}: Total VAE1 vs Total VAE2\",\n",
    "                    wandb_key=\"Hists2D/Total_VAE1_vs_Total_VAE2\")\n",
    "\n",
    "        make_2D_hist(vae1_kl_np, vae2_kl_np,\n",
    "                    \"KL Loss (VAE1)\", \"KL Loss (VAE2)\",\n",
    "                    f\"Epoch {epoch}: KL VAE1 vs KL VAE2\",\n",
    "                    wandb_key=\"Hists2D/KL_VAE1_vs_KL_VAE2\")\n",
    "\n",
    "        make_2D_hist(vae1_reco_np, vae2_reco_np,\n",
    "                    \"Reco Loss (VAE1)\", \"Reco Loss (VAE2)\",\n",
    "                    f\"Epoch {epoch}: Reco VAE1 vs Reco VAE2\",\n",
    "                    wandb_key=\"Hists2D/Reco_VAE1_vs_Reco_VAE2\")\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"Finished training.\")\n",
    "\n",
    "    #save models\n",
    "    torch.save(vae_1.state_dict(), \"vae1_trained.pth\")\n",
    "    torch.save(vae_2.state_dict(), \"vae2_trained.pth\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160a1fc-439a-429f-a154-7c134e0ecfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n",
      "Logging in to wandb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jovyan/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mescheuller\u001b[0m (\u001b[33mescheuller-uc-san-diego\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20250828_170824-mra3f2yc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/mra3f2yc' target=\"_blank\">cerulean-plant-112</a></strong> to <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training' target=\"_blank\">https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/mra3f2yc' target=\"_blank\">https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/mra3f2yc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: cerulean-plant-112\n",
      "Loading dataset...\n",
      "Train shape: (1999965, 19, 3), Test shape: (4511092, 19, 3)\n",
      "Data finished loading.\n",
      "Building mask...\n",
      "Mask is ready.\n",
      "VAEs are ready.\n",
      "Moving data to device...\n",
      "Data on device.\n",
      "Starting the training loop!\n",
      "[EPOCH 0/200] Loss=1290795.1670 Reco1=576476.6011 Reco2=605858.6553 KL1=12.9962 KL2=10.1063 DisCo=10.8437\n",
      "[EPOCH 1/200] Loss=1132302.5967 Reco1=517005.2517 Reco2=541898.9507 KL1=12.8030 KL2=9.7886 DisCo=7.3376\n",
      "[EPOCH 2/200] Loss=929862.0220 Reco1=437709.2729 Reco2=439387.9341 KL1=12.3216 KL2=9.5976 DisCo=5.2743\n",
      "[EPOCH 3/200] Loss=800417.3481 Reco1=376455.8564 Reco2=363596.2126 KL1=12.0074 KL2=9.1479 DisCo=6.0344\n",
      "[EPOCH 4/200] Loss=749928.3989 Reco1=355741.7114 Reco2=335449.4966 KL1=13.6179 KL2=8.3368 DisCo=5.8715\n",
      "[EPOCH 5/200] Loss=727171.5278 Reco1=348609.5286 Reco2=323337.2295 KL1=18.2463 KL2=8.5593 DisCo=5.5198\n",
      "[EPOCH 6/200] Loss=707480.8955 Reco1=351581.1536 Reco2=307733.9170 KL1=27.3539 KL2=18.4448 DisCo=4.8120\n",
      "[EPOCH 7/200] Loss=683859.2002 Reco1=340427.7166 Reco2=304692.5879 KL1=52.4098 KL2=67.4452 DisCo=3.8619\n",
      "[EPOCH 8/200] Loss=666635.0244 Reco1=323584.3096 Reco2=310041.6553 KL1=97.5426 KL2=180.5867 DisCo=3.2731\n",
      "[EPOCH 9/200] Loss=646249.5996 Reco1=309420.9888 Reco2=306258.9736 KL1=205.3552 KL2=507.3112 DisCo=2.9857\n",
      "[EPOCH 10/200] Loss=613518.4380 Reco1=293857.3977 Reco2=291032.9502 KL1=454.3322 KL2=1100.5239 DisCo=2.7073\n",
      "[EPOCH 11/200] Loss=582145.7222 Reco1=280207.8994 Reco2=273603.8081 KL1=746.5869 KL2=1753.2812 DisCo=2.5834\n",
      "[EPOCH 12/200] Loss=549494.0098 Reco1=260236.9023 Reco2=261128.8745 KL1=1143.9595 KL2=2291.1879 DisCo=2.4693\n",
      "[EPOCH 13/200] Loss=524041.3181 Reco1=244375.0908 Reco2=251443.5175 KL1=1434.1132 KL2=2544.1768 DisCo=2.4244\n",
      "[EPOCH 14/200] Loss=498397.1692 Reco1=229168.6284 Reco2=239929.1638 KL1=1683.2807 KL2=2764.8781 DisCo=2.4851\n",
      "[EPOCH 15/200] Loss=468398.1570 Reco1=213921.9236 Reco2=225824.8340 KL1=1794.8180 KL2=3148.3725 DisCo=2.3708\n",
      "[EPOCH 16/200] Loss=443376.9373 Reco1=202306.5483 Reco2=213581.1108 KL1=1846.5719 KL2=3401.3285 DisCo=2.2241\n",
      "[EPOCH 17/200] Loss=422412.4768 Reco1=192270.5376 Reco2=202818.8939 KL1=1939.8658 KL2=3413.4858 DisCo=2.1970\n",
      "[EPOCH 18/200] Loss=404640.0061 Reco1=184683.8307 Reco2=191698.2041 KL1=2006.9805 KL2=3470.2666 DisCo=2.2781\n",
      "[EPOCH 19/200] Loss=390104.4619 Reco1=178973.1125 Reco2=182624.7437 KL1=2048.3178 KL2=3491.5349 DisCo=2.2967\n",
      "[EPOCH 20/200] Loss=379155.0598 Reco1=174841.4194 Reco2=175650.1915 KL1=2091.4171 KL2=3526.3549 DisCo=2.3046\n",
      "[EPOCH 21/200] Loss=371140.7766 Reco1=171706.7585 Reco2=170921.4614 KL1=2141.9682 KL2=3525.5711 DisCo=2.2845\n",
      "[EPOCH 22/200] Loss=365241.5798 Reco1=169305.3654 Reco2=167597.0540 KL1=2182.9261 KL2=3513.9171 DisCo=2.2642\n",
      "[EPOCH 23/200] Loss=361158.3723 Reco1=167542.0083 Reco2=165226.6837 KL1=2215.5566 KL2=3493.5590 DisCo=2.2681\n",
      "[EPOCH 24/200] Loss=358135.8252 Reco1=166344.3059 Reco2=163411.5184 KL1=2251.4735 KL2=3473.5676 DisCo=2.2655\n",
      "[EPOCH 25/200] Loss=355892.6853 Reco1=165609.5842 Reco2=161906.1700 KL1=2280.5475 KL2=3454.6279 DisCo=2.2642\n",
      "[EPOCH 26/200] Loss=353707.1245 Reco1=164674.8615 Reco2=160613.8890 KL1=2298.1873 KL2=3422.4797 DisCo=2.2698\n",
      "[EPOCH 27/200] Loss=351792.2908 Reco1=164085.6482 Reco2=159626.9158 KL1=2313.1244 KL2=3403.5814 DisCo=2.2363\n",
      "[EPOCH 28/200] Loss=350508.6702 Reco1=163514.2874 Reco2=158861.8883 KL1=2327.8426 KL2=3378.0895 DisCo=2.2427\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'vae_lr': 1e-4,\n",
    "    'beta': 0.5,\n",
    "    'alpha': 0.5,\n",
    "    'vae_latent': 8,\n",
    "    'vae_nodes': [28, 14],\n",
    "    'lambda_disco': 10000.0\n",
    "}\n",
    "\n",
    "run(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6994c81-b5dc-48a3-9a76-930bd037059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload data same way as in traiining\n",
    "print(\"Loading dataset...\")\n",
    "fpath = '/axovol/training/v5/conditionsupdate_apr25.h5'\n",
    "with h5.File(fpath, 'r') as f:\n",
    "    root = f['data'] if 'data' in f else f\n",
    "\n",
    "    x_train = root['Background_data']['Train']['DATA'][:]\n",
    "    x_test  = root['Background_data']['Test']['DATA'][:]\n",
    "    print(f\"Train shape: {x_train.shape}, Test shape: {x_test.shape}\")\n",
    "\n",
    "    #flatten per event\n",
    "    x_train_bkg = x_train.reshape(x_train.shape[0], -1)\n",
    "    x_test_bkg  = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "    scale = root['Normalisation']['norm_scale'][:]\n",
    "    bias  = root['Normalisation']['norm_bias'][:]\n",
    "\n",
    "    l1_bits_bkg_test = root['Background_data']['Test']['L1bits'][:]\n",
    "\n",
    "    print(\"Available signal names:\")\n",
    "    print(list(root['Signal_data'].keys()))\n",
    "\n",
    "    #load signal data\n",
    "    x_sig = root['Signal_data']['GluGluHToTauTau']['DATA'][:]\n",
    "    x_sig = x_sig.reshape(x_sig.shape[0], -1)\n",
    "    l1_bits_sig = root['Signal_data']['GluGluHToTauTau']['L1bits'][:]\n",
    "\n",
    "print(\"Data finished loading.\")\n",
    "\n",
    "features = x_test_bkg.shape[1]\n",
    "\n",
    "# reload config based on training\n",
    "cfg = {\n",
    "    \"features\": features,\n",
    "    \"latent_dim\": 8,         \n",
    "    \"encoder_config\": {\"nodes\": [28, 14]}, \n",
    "    \"decoder_config\": {\"nodes\": [24, 32, 64, 128, features]}, \n",
    "    \"alpha\": 0.5,\n",
    "    \"beta\": 0.5\n",
    "}\n",
    "\n",
    "vae_1 = VAE(cfg).to(device)\n",
    "vae_2 = VAE(cfg).to(device)\n",
    "\n",
    "#load model weights\n",
    "vae_1.load_state_dict(torch.load(\"vae1_trained.pth\", map_location=device))\n",
    "vae_2.load_state_dict(torch.load(\"vae2_trained.pth\", map_location=device))\n",
    "vae_1.eval()\n",
    "vae_2.eval()\n",
    "\n",
    "#compute anomaly scores\n",
    "bkg_scores_1 = distance_pt(vae_1, x_test_bkg, device)\n",
    "sig_scores_1 = distance_pt(vae_1, x_sig, device)\n",
    "\n",
    "bkg_scores_2 = distance_pt(vae_2, x_test_bkg, device)\n",
    "sig_scores_2 = distance_pt(vae_2, x_sig, device)\n",
    "\n",
    "#vae 1 anomaly score plot\n",
    "plt.figure(figsize=(6,4))\n",
    "bins = np.linspace(np.percentile(bkg_scores_1, 0.5), np.percentile(bkg_scores_1, 99.5), 60)\n",
    "plt.hist(bkg_scores_1, bins=bins, alpha=0.5, label='Background')\n",
    "plt.hist(sig_scores_1, bins=bins, alpha=0.5, label=\"GluGluHToTauTau\")\n",
    "plt.xlabel(\"Anomaly score\")\n",
    "plt.ylabel(\"Events\")\n",
    "plt.legend()\n",
    "plt.title(\"VAE 1 Anomaly Scores\")\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "#vae 1 ROC curve\n",
    "y_true  = np.concatenate([np.zeros_like(bkg_scores_1), np.ones_like(sig_scores_1)])\n",
    "y_score = np.concatenate([bkg_scores_1, sig_scores_1])\n",
    "fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(fpr, tpr, lw=2, label=f\"AUC = {roc_auc:.3f}\")\n",
    "plt.plot([0,1],[0,1],'--', lw=1, color='gray')\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(f\"ROC VAE 1\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#vae 2 anomaly score plot\n",
    "plt.figure(figsize=(6,4))\n",
    "bins = np.linspace(np.percentile(bkg_scores_2, 0.5), np.percentile(bkg_scores_2, 99.5), 60)\n",
    "plt.hist(bkg_scores_2, bins=bins, alpha=0.6, label='Background')\n",
    "plt.hist(sig_scores_2, bins=bins,alpha=0.6, label=\"GluGluHToTauTau\")\n",
    "plt.xlabel(\"Anomaly score\")\n",
    "plt.ylabel(\"Events\")\n",
    "plt.legend()\n",
    "plt.title(\"VAE 2 anomaly scores\")\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "#vae 2 ROC curve plot\n",
    "y_true  = np.concatenate([np.zeros_like(bkg_scores_2), np.ones_like(sig_scores_2)])\n",
    "y_score = np.concatenate([bkg_scores_2, sig_scores_2])\n",
    "fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(fpr, tpr, lw=2, label=f\"AUC = {roc_auc:.3f}\")\n",
    "plt.plot([0,1],[0,1],'--', lw=1, color='gray')\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(f\"ROC VAE 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2761078-2f9b-4392-949d-43621fcfdcf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
