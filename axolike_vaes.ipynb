{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a5269f0-7f2e-40fd-917b-c3062e30516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gc\n",
    "import argparse\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "from models.vae import VAE \n",
    "from losses.cyl_ptpz_mae import CylPtPzMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0252ca81-bc87-49de-bad4-be27f0f4ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting a seed like in vae_legacy\n",
    "def set_seed(seed=123):\n",
    "    import random, os, numpy as np, torch\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89562028-a18a-4aa5-8f3a-ad75bc4468ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculated disco loss based on paper\n",
    "def disco_loss(z1, z2):\n",
    "    x = z1 - z1.mean(0)\n",
    "    y = z2 - z2.mean(0)\n",
    "    a = torch.cdist(x, x)\n",
    "    b = torch.cdist(y, y)\n",
    "    A = a - a.mean(0) - a.mean(1, keepdim=True) + a.mean()\n",
    "    B = b - b.mean(0) - b.mean(1, keepdim=True) + b.mean()\n",
    "    dcov = (A * B).mean()\n",
    "    dvar_x = (A * A).mean().sqrt()\n",
    "    dvar_y = (B * B).mean().sqrt()\n",
    "    return dcov / (dvar_x * dvar_y + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "320ba38b-4c09-4ea0-b883-6f7011de2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates anomaly score like in vae legacy\n",
    "def distance_pt(model_vae, data_np, device):\n",
    "    x = torch.tensor(data_np, dtype=torch.float32, device=device)\n",
    "    z_mean, z_logvar, _ = model_vae.encoder(x)\n",
    "    score = torch.sum(z_mean**2, dim=1)\n",
    "    return score.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "540a813e-5d61-4ce4-9593-d2ef7e03bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config):\n",
    "    #set seed\n",
    "    seed = 123\n",
    "    set_seed(seed)\n",
    "\n",
    "    #move to gpu if avail\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using: {device}\")\n",
    "\n",
    "    #login to wandb\n",
    "    print(\"Logging in to wandb...\")\n",
    "    wandb.login(key=\"24d1d60ce26563c74d290d7b487cb104fc251271\")\n",
    "    wandb.init(project=\"Double Disco Axo Training\",\n",
    "               settings=wandb.Settings(_disable_stats=True),\n",
    "               config=config)\n",
    "    run_name = wandb.run.name\n",
    "    print(f\"Run name: {run_name}\")\n",
    "\n",
    "    #scaling\n",
    "    beta  = float(config['beta'])\n",
    "    alpha = float(config['alpha'])\n",
    "    vae_lr = float(config['vae_lr'])\n",
    "\n",
    "    #load data\n",
    "    print(\"Loading dataset...\")\n",
    "    fpath = '/axovol/training/v5/conditionsupdate_apr25.h5'\n",
    "    with h5.File(fpath, 'r') as f:\n",
    "        root = f['data'] if 'data' in f else f\n",
    "    \n",
    "        x_train = root['Background_data']['Train']['DATA'][:]\n",
    "        x_test  = root['Background_data']['Test']['DATA'][:]\n",
    "        print(f\"Train shape: {x_train.shape}, Test shape: {x_test.shape}\")\n",
    "    \n",
    "        #flatten per event\n",
    "        x_train_bkg = x_train.reshape(x_train.shape[0], -1)\n",
    "        x_test_bkg  = x_test.reshape(x_test.shape[0], -1)\n",
    "    \n",
    "        scale = root['Normalisation']['norm_scale'][:]\n",
    "        bias  = root['Normalisation']['norm_bias'][:]\n",
    "    \n",
    "        l1_bits_bkg_test = root['Background_data']['Test']['L1bits'][:]\n",
    "    \n",
    "        #load signal data\n",
    "        SIGNAL_NAMES = list(root['Signal_data'].keys())\n",
    "        signal_data_dict = {}\n",
    "        signal_l1_dict   = {}\n",
    "        for sname in SIGNAL_NAMES:\n",
    "            x_sig = root['Signal_data'][sname]['DATA'][:]\n",
    "            x_sig = x_sig.reshape(x_sig.shape[0], -1)\n",
    "            l1_bits = root['Signal_data'][sname]['L1bits'][:]\n",
    "            signal_data_dict[sname] = x_sig\n",
    "            signal_l1_dict[sname] = l1_bits\n",
    "\n",
    "    print(\"Data finished loading.\")\n",
    "    \n",
    "    dataset = x_train_bkg\n",
    "    dataset_test = x_test_bkg\n",
    "\n",
    "    #should be 57\n",
    "    features = dataset.shape[1]\n",
    "\n",
    "\n",
    "    print(\"Building mask...\")\n",
    "    # same mask from vae legacy\n",
    "    mask_dict = {\n",
    "        \"MET\":[True],\n",
    "        \"EGAMMA\":[True,True,True,True,False,False,False,False,False,False,False,False],\n",
    "        \"MUON\":[True,True,True,True,False,False,False,False],\n",
    "        \"JET\":[True,True,True,True,True,True,True,True,True,True,False,False]\n",
    "    }\n",
    "    \n",
    "    #build cyl_ptpz_mae mask (input scales and biases)\n",
    "    reco_loss_fn = CylPtPzMAE(scale, bias).to(device)\n",
    "    print(\"Mask is ready.\")\n",
    "\n",
    "\n",
    "    #vae config\n",
    "    latent_dim = int(config['vae_latent'])\n",
    "    enc_nodes = list(config['vae_nodes'])\n",
    "    dec_nodes = [24, 32, 64, 128, features] \n",
    "\n",
    "    vae_cfg = {\n",
    "        \"features\": features,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"encoder_config\": {\"nodes\": enc_nodes},\n",
    "        \"decoder_config\": {\"nodes\": dec_nodes},\n",
    "        \"alpha\": alpha,\n",
    "        \"beta\":  beta,\n",
    "    }\n",
    "\n",
    "    #put vae on device/init\n",
    "    vae_1 = VAE(vae_cfg).to(device)\n",
    "    vae_2 = VAE(vae_cfg).to(device)\n",
    "    print(\"VAEs are ready.\")\n",
    "\n",
    "    #optimizer (adam)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(vae_1.parameters()) + list(vae_2.parameters()),\n",
    "        lr=vae_lr\n",
    "    )\n",
    "\n",
    "    #cosine restarts\n",
    "    warmup_epochs = 10\n",
    "    cos = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=32, T_mult=2, eta_min=0.0\n",
    "    )\n",
    "\n",
    "    #sets learning rate\n",
    "    def set_lr(lr):\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "\n",
    "    #hyperparameters\n",
    "    Epochs_VAE = 50\n",
    "    Batch_size = 16384\n",
    "\n",
    "    #get disco param from config, else set to 1\n",
    "    lambda_disco = float(config.get(\"lambda_disco\", 1))\n",
    "\n",
    "    print(\"Moving data to device...\")\n",
    "    X = torch.tensor(dataset, dtype=torch.float32, device=device)\n",
    "    print(\"Data on device.\")\n",
    "\n",
    "    #training loop\n",
    "    print(\"Starting the training loop!\")\n",
    "    N = X.size(0)\n",
    "    for epoch in range(Epochs_VAE):\n",
    "        #cosine warmup step\n",
    "        if epoch < warmup_epochs:\n",
    "            lr = vae_lr * (epoch + 1) / warmup_epochs\n",
    "            set_lr(lr)\n",
    "        else:\n",
    "            cos.step(epoch - warmup_epochs)\n",
    "\n",
    "        #shuffles data incides for each epoch\n",
    "        perm = torch.randperm(N, device=device)\n",
    "\n",
    "        #init losses\n",
    "        total_loss = total_reco1 = total_reco2 = total_kl1 = total_kl2 = total_disco = 0.0\n",
    "\n",
    "        #loops over dataset in steps of batch sizze\n",
    "        for i in range(0, N, Batch_size):\n",
    "            #picks indices for current batch\n",
    "            idx = perm[i:i+Batch_size]\n",
    "\n",
    "            #selects batch of samples from data set X\n",
    "            xb  = X[idx]\n",
    "\n",
    "            #vae 1\n",
    "            recon1, mu1, logvar1, z1 = vae_1(xb)\n",
    "            \n",
    "            #vae 2\n",
    "            recon2, mu2, logvar2, z2 = vae_2(xb)\n",
    "\n",
    "            #get reco loss from custom func\n",
    "            reco1_per = reco_loss_fn(recon1, xb)\n",
    "            reco2_per = reco_loss_fn(recon2, xb)\n",
    "\n",
    "            #get kl div per sample\n",
    "            kl1_per = VAE.kl_divergence(mu1, logvar1)\n",
    "            kl2_per = VAE.kl_divergence(mu2, logvar2)\n",
    "\n",
    "            #same scaling from vae legacy\n",
    "            reco1 = vae_1.reco_scale*reco1_per.mean()\n",
    "            reco2 = vae_2.reco_scale*reco2_per.mean()\n",
    "            kl1 = vae_1.kl_scale*kl1_per.mean()\n",
    "            kl2 = vae_2.kl_scale*kl2_per.mean()\n",
    "\n",
    "            #disco loss (ask Melissa about since using mu instead of z)\n",
    "            disco = disco_loss(mu1, mu2)\n",
    "\n",
    "            #calc total loss\n",
    "            loss = (reco1 + kl1) + (reco2 + kl2) + lambda_disco * disco\n",
    "\n",
    "            #zero grads\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            #backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            #do some gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(vae_1.parameters()) + list(vae_2.parameters()), max_norm=5.0\n",
    "            )\n",
    "            optimizer.step()\n",
    "\n",
    "            #add losses to list for wandb plotting\n",
    "            total_loss+=loss.item()\n",
    "            total_reco1+=reco1.item()\n",
    "            total_reco2+=reco2.item()\n",
    "            total_kl1+=kl1.item()\n",
    "            total_kl2+=kl2.item()\n",
    "            total_disco+=disco.item()\n",
    "\n",
    "        print(f\"[EPOCH {epoch+1}/{Epochs_VAE}] \"\n",
    "          f\"Loss={total_loss:.4f} \"\n",
    "          f\"Reco1={total_reco1:.4f} Reco2={total_reco2:.4f} \"\n",
    "          f\"KL1={total_kl1:.4f} KL2={total_kl2:.4f} \"\n",
    "          f\"DisCo={total_disco:.4f}\")\n",
    "\n",
    "        #log in wandb\n",
    "        wandb.log({\n",
    "            \"EpochVae\": epoch,\n",
    "            \"TotalLossVae\": total_loss,\n",
    "            \"RecoLossVae1\": total_reco1,\n",
    "            \"RecoLossVae2\": total_reco2,\n",
    "            \"KLLossVae1\": total_kl1,\n",
    "            \"KLLossVae2\": total_kl2,\n",
    "            \"DisCoLoss\": total_disco,\n",
    "        })\n",
    "\n",
    "    print(\"Finished training.\")\n",
    "\n",
    "    #save models\n",
    "    torch.save(vae_1.state_dict(), \"vae1_trained.pth\")\n",
    "    torch.save(vae_2.state_dict(), \"vae2_trained.pth\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8160a1fc-439a-429f-a154-7c134e0ecfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n",
      "Logging in to wandb...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>DisCoLoss</td><td>▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▅▇██▇▆▅▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃</td></tr><tr><td>EpochVae</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>KLLossVae1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▄▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>KLLossVae2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▄▅▆▆▆▇▇██████████████████</td></tr><tr><td>RecoLossVae1</td><td>█▇▅▄▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>RecoLossVae2</td><td>█▆▅▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>TotalLossVae</td><td>█▇▅▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>DisCoLoss</td><td>4.84205</td></tr><tr><td>EpochVae</td><td>49</td></tr><tr><td>KLLossVae1</td><td>726.35436</td></tr><tr><td>KLLossVae2</td><td>807.71241</td></tr><tr><td>RecoLossVae1</td><td>334349.03101</td></tr><tr><td>RecoLossVae2</td><td>364526.07422</td></tr><tr><td>TotalLossVae</td><td>705251.21631</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">major-resonance-42</strong> at: <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/3pcoszv7' target=\"_blank\">https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/3pcoszv7</a><br> View project at: <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training' target=\"_blank\">https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250812_162521-3pcoszv7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (1.5s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20250812_164832-xbi9kvhe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/xbi9kvhe' target=\"_blank\">upbeat-smoke-43</a></strong> to <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training' target=\"_blank\">https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/xbi9kvhe' target=\"_blank\">https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/xbi9kvhe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: upbeat-smoke-43\n",
      "Loading dataset...\n",
      "Train shape: (1999965, 19, 3), Test shape: (4511092, 19, 3)\n",
      "Data finished loading.\n",
      "Building mask...\n",
      "Mask is ready.\n",
      "VAEs are ready.\n",
      "Moving data to device...\n",
      "Data on device.\n",
      "Starting the training loop!\n",
      "[EPOCH 1/50] Loss=1167370.3242 Reco1=565457.0747 Reco2=593604.0068 KL1=12.9752 KL2=9.8977 DisCo=8.2864\n",
      "[EPOCH 2/50] Loss=961973.6006 Reco1=469967.3000 Reco2=487856.6082 KL1=12.7728 KL2=9.4035 DisCo=4.1275\n",
      "[EPOCH 3/50] Loss=709944.9980 Reco1=352153.2539 Reco2=355844.3545 KL1=12.4245 KL2=9.5742 DisCo=1.9254\n",
      "[EPOCH 4/50] Loss=546902.1616 Reco1=272716.1890 Reco2=273405.7744 KL1=11.8002 KL2=9.4450 DisCo=0.7590\n",
      "[EPOCH 5/50] Loss=492213.0071 Reco1=246153.7438 Reco2=245768.2845 KL1=10.8266 KL2=8.6775 DisCo=0.2715\n",
      "[EPOCH 6/50] Loss=483904.0430 Reco1=241920.6785 Reco2=241830.1709 KL1=9.6611 KL2=7.5763 DisCo=0.1360\n",
      "[EPOCH 7/50] Loss=482455.0964 Reco1=241187.0255 Reco2=241162.5233 KL1=8.3369 KL2=6.4642 DisCo=0.0907\n",
      "[EPOCH 8/50] Loss=482012.9807 Reco1=240973.6554 Reco2=240959.2858 KL1=6.9675 KL2=5.4220 DisCo=0.0676\n",
      "[EPOCH 9/50] Loss=481891.4480 Reco1=240919.7933 Reco2=240907.7101 KL1=5.6855 KL2=4.4578 DisCo=0.0538\n",
      "[EPOCH 10/50] Loss=481543.0615 Reco1=240750.7262 Reco2=240739.1184 KL1=4.5971 KL2=3.6732 DisCo=0.0449\n",
      "[EPOCH 11/50] Loss=481581.0449 Reco1=240771.7152 Reco2=240760.9508 KL1=3.7469 KL2=3.0572 DisCo=0.0416\n",
      "[EPOCH 12/50] Loss=481355.8523 Reco1=240657.7545 Reco2=240650.4121 KL1=3.1620 KL2=2.6459 DisCo=0.0419\n",
      "[EPOCH 13/50] Loss=481601.5901 Reco1=240781.3727 Reco2=240778.1921 KL1=2.7244 KL2=2.3535 DisCo=0.0369\n",
      "[EPOCH 14/50] Loss=481393.8333 Reco1=240676.5120 Reco2=240674.9897 KL1=2.4430 KL2=2.1409 DisCo=0.0377\n",
      "[EPOCH 15/50] Loss=481335.0962 Reco1=240647.8966 Reco2=240647.9232 KL1=2.2888 KL2=1.9724 DisCo=0.0350\n",
      "[EPOCH 16/50] Loss=481371.0591 Reco1=240666.7548 Reco2=240669.7501 KL1=2.2430 KL2=1.8598 DisCo=0.0305\n",
      "[EPOCH 17/50] Loss=481247.8223 Reco1=240586.9758 Reco2=240615.8010 KL1=2.9757 KL2=1.7841 DisCo=0.0403\n",
      "[EPOCH 18/50] Loss=480561.6584 Reco1=239893.7063 Reco2=240616.5115 KL1=8.2282 KL2=1.7615 DisCo=0.0415\n",
      "[EPOCH 19/50] Loss=477155.4595 Reco1=236432.2305 Reco2=240624.3862 KL1=35.3551 KL2=1.7766 DisCo=0.0617\n",
      "[EPOCH 20/50] Loss=472454.4351 Reco1=231694.9183 Reco2=240612.7833 KL1=75.2116 KL2=1.8227 DisCo=0.0697\n",
      "[EPOCH 21/50] Loss=465856.8008 Reco1=225014.3278 Reco2=240621.7714 KL1=142.3210 KL2=1.8398 DisCo=0.0765\n",
      "[EPOCH 22/50] Loss=457491.0269 Reco1=216583.5292 Reco2=240547.6260 KL1=259.4871 KL2=1.8625 DisCo=0.0985\n",
      "[EPOCH 23/50] Loss=447828.0876 Reco1=206630.8184 Reco2=240627.8813 KL1=437.8903 KL2=1.7871 DisCo=0.1297\n",
      "[EPOCH 24/50] Loss=436793.2578 Reco1=195411.1694 Reco2=240588.0555 KL1=665.9342 KL2=1.7789 DisCo=0.1263\n",
      "[EPOCH 25/50] Loss=427097.4871 Reco1=185506.9548 Reco2=240598.7360 KL1=861.1974 KL2=1.7869 DisCo=0.1288\n",
      "[EPOCH 26/50] Loss=419232.2380 Reco1=177423.7606 Reco2=240636.5809 KL1=1032.4904 KL2=1.6902 DisCo=0.1377\n",
      "[EPOCH 27/50] Loss=414302.0132 Reco1=172335.1959 Reco2=240621.7037 KL1=1206.3987 KL2=1.5641 DisCo=0.1372\n",
      "[EPOCH 28/50] Loss=410883.6538 Reco1=168775.8949 Reco2=240623.5873 KL1=1353.0565 KL2=1.4797 DisCo=0.1296\n",
      "[EPOCH 29/50] Loss=407348.3459 Reco1=165194.4131 Reco2=240552.5114 KL1=1475.8943 KL2=1.4229 DisCo=0.1241\n",
      "[EPOCH 30/50] Loss=404448.4829 Reco1=162104.3533 Reco2=240664.2650 KL1=1568.9651 KL2=1.3857 DisCo=0.1095\n",
      "[EPOCH 31/50] Loss=401020.1968 Reco1=158575.9888 Reco2=240693.1464 KL1=1649.9444 KL2=1.3523 DisCo=0.0998\n",
      "[EPOCH 32/50] Loss=398090.1013 Reco1=155610.5947 Reco2=240665.2517 KL1=1719.4667 KL2=1.3310 DisCo=0.0935\n",
      "[EPOCH 33/50] Loss=395247.4841 Reco1=152735.5991 Reco2=240644.8032 KL1=1776.5547 KL2=1.3108 DisCo=0.0892\n",
      "[EPOCH 34/50] Loss=392858.8811 Reco1=150373.9164 Reco2=240576.0291 KL1=1822.3084 KL2=1.3063 DisCo=0.0853\n",
      "[EPOCH 35/50] Loss=391167.1711 Reco1=148618.6400 Reco2=240603.1799 KL1=1857.9955 KL2=1.2977 DisCo=0.0861\n",
      "[EPOCH 36/50] Loss=390121.7971 Reco1=147440.5126 Reco2=240711.3307 KL1=1886.6700 KL2=1.2946 DisCo=0.0820\n",
      "[EPOCH 37/50] Loss=389193.7087 Reco1=146504.2029 Reco2=240698.5745 KL1=1906.7537 KL2=1.2890 DisCo=0.0829\n",
      "[EPOCH 38/50] Loss=388461.0964 Reco1=145786.9828 Reco2=240667.2958 KL1=1921.9445 KL2=1.2879 DisCo=0.0836\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvae_lr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1e-4\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda_disco\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1000.0\u001b[39m\n\u001b[1;32m      8\u001b[0m }\n\u001b[0;32m---> 10\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 169\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    164\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mlist\u001b[39m(vae_1\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(vae_2\u001b[38;5;241m.\u001b[39mparameters()), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5.0\u001b[39m\n\u001b[1;32m    166\u001b[0m )\n\u001b[1;32m    167\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 169\u001b[0m total_loss  \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m total_reco1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reco1\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    171\u001b[0m total_reco2 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reco2\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'vae_lr': 1e-4,\n",
    "    'beta': 0.5,\n",
    "    'alpha': 0.5,\n",
    "    'vae_latent': 8,\n",
    "    'vae_nodes': [28, 14],\n",
    "    'lambda_disco': 1000.0\n",
    "}\n",
    "\n",
    "run(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ffb118-6d9a-484c-ac86-8e3e701f4c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2761078-2f9b-4392-949d-43621fcfdcf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
