{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a5269f0-7f2e-40fd-917b-c3062e30516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gc\n",
    "import argparse\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "from models.vae import VAE \n",
    "from losses.cyl_ptpz_mae import CylPtPzMAE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0252ca81-bc87-49de-bad4-be27f0f4ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting a seed like in vae_legacy\n",
    "def set_seed(seed=123):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89562028-a18a-4aa5-8f3a-ad75bc4468ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculated disco loss based on paper\n",
    "def disco_loss(z1, z2):\n",
    "    #center around 0\n",
    "    x = z1 - z1.mean(0)\n",
    "    y = z2 - z2.mean(0)\n",
    "    #build euclidean distance matrices\n",
    "    a = torch.cdist(x, x)\n",
    "    b = torch.cdist(y, y)\n",
    "    #doublce center distance matrices\n",
    "    A = a - a.mean(0) - a.mean(1, keepdim=True) + a.mean()\n",
    "    B = b - b.mean(0) - b.mean(1, keepdim=True) + b.mean()\n",
    "    #find covariance\n",
    "    dcov = (A * B).mean()\n",
    "    #find variance and take square root (in paper it is not squared)\n",
    "    dvar_x = (A * A).mean().sqrt()\n",
    "    dvar_y = (B * B).mean().sqrt()\n",
    "    #return paper formula for disco\n",
    "    return dcov / (dvar_x * dvar_y + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a33be852-c54f-4652-b770-402d2add7e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_corr(var_1,var_2,normedweight,power=1):\n",
    "    \"\"\"var_1: First variable to decorrelate (eg mass)\n",
    "    var_2: Second variable to decorrelate (eg classifier output)\n",
    "    normedweight: Per-example weight. Sum of weights should add up to N (where N is the number of examples)\n",
    "    power: Exponent used in calculating the distance correlation\n",
    "    \n",
    "    va1_1, var_2 and normedweight should all be 1D torch tensors with the same number of entries\n",
    "    \n",
    "    Usage: Add to your loss function. total_loss = BCE_loss + lambda * distance_corr\n",
    "    \"\"\" \n",
    "    \n",
    "    xx = var_1.view(-1, 1).repeat(1, len(var_1)).view(len(var_1),len(var_1))\n",
    "    yy = var_1.repeat(len(var_1),1).view(len(var_1),len(var_1))\n",
    "    amat = (xx-yy).abs()\n",
    "\n",
    "    xx = var_2.view(-1, 1).repeat(1, len(var_2)).view(len(var_2),len(var_2))\n",
    "    yy = var_2.repeat(len(var_2),1).view(len(var_2),len(var_2))\n",
    "    bmat = (xx-yy).abs()\n",
    "\n",
    "    amatavg = torch.mean(amat*normedweight,dim=1)\n",
    "    Amat=amat-amatavg.repeat(len(var_1),1).view(len(var_1),len(var_1))\\\n",
    "        -amatavg.view(-1, 1).repeat(1, len(var_1)).view(len(var_1),len(var_1))\\\n",
    "        +torch.mean(amatavg*normedweight)\n",
    "\n",
    "    bmatavg = torch.mean(bmat*normedweight,dim=1)\n",
    "    Bmat=bmat-bmatavg.repeat(len(var_2),1).view(len(var_2),len(var_2))\\\n",
    "        -bmatavg.view(-1, 1).repeat(1, len(var_2)).view(len(var_2),len(var_2))\\\n",
    "        +torch.mean(bmatavg*normedweight)\n",
    "\n",
    "    ABavg = torch.mean(Amat*Bmat*normedweight,dim=1)\n",
    "    AAavg = torch.mean(Amat*Amat*normedweight,dim=1)\n",
    "    BBavg = torch.mean(Bmat*Bmat*normedweight,dim=1)\n",
    "\n",
    "    if(power==1):\n",
    "        dCorr=(torch.mean(ABavg*normedweight))/torch.sqrt((torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight)))\n",
    "    elif(power==2):\n",
    "        dCorr=(torch.mean(ABavg*normedweight))**2/(torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight))\n",
    "    else:\n",
    "        dCorr=((torch.mean(ABavg*normedweight))/torch.sqrt((torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight))))**power\n",
    "    \n",
    "    return dCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "320ba38b-4c09-4ea0-b883-6f7011de2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates anomaly score like in vae legacy\n",
    "def distance_pt(model_vae, data_np, device):\n",
    "    x = torch.tensor(data_np, dtype=torch.float32, device=device)\n",
    "    z_mean, z_logvar, _ = model_vae.encoder(x)\n",
    "    score = torch.sum(z_mean**2, dim=1)\n",
    "    return score.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df9cf7a6-624b-491c-ad49-5eca472a676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to make 2D histograms\n",
    "def make_2D_hist(x, y, xlabel, ylabel, title, wandb_key, bins=40):\n",
    "    fig = plt.figure(figsize=(5,4))\n",
    "    plt.hist2d(x, y, bins=bins)\n",
    "    plt.colorbar(label='Counts')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    wandb.log({wandb_key: wandb.Image(fig)})\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70f9494b-d8c8-4bf6-b3a0-1d2850958bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints h5 tree to examine\n",
    "def print_h5_tree(h, prefix=\"\"):\n",
    "    for k in h.keys():\n",
    "        item = h[k]\n",
    "        if hasattr(item, 'keys'):\n",
    "            print(prefix + f\"[GROUP] {k}\")\n",
    "            print_h5_tree(item, prefix + \"  \")\n",
    "        else:\n",
    "            try:\n",
    "                print(prefix + f\"{k}: shape={item.shape}, dtype={item.dtype}\")\n",
    "            except Exception:\n",
    "                print(prefix + f\"{k}: <dataset>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "540a813e-5d61-4ce4-9593-d2ef7e03bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config):\n",
    "    #set seed\n",
    "    seed = 123\n",
    "    set_seed(seed)\n",
    "\n",
    "    #move to gpu if avail\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using: {device}\")\n",
    "\n",
    "    #login to wandb\n",
    "    print(\"Logging in to wandb...\")\n",
    "    wandb.login(key=\"24d1d60ce26563c74d290d7b487cb104fc251271\")\n",
    "    wandb.init(project=\"Double Disco Axo Training\",\n",
    "               settings=wandb.Settings(_disable_stats=True),\n",
    "               config=config)\n",
    "    run_name = wandb.run.name\n",
    "    print(f\"Run name: {run_name}\")\n",
    "\n",
    "    #scaling\n",
    "    beta = float(config['beta'])\n",
    "    alpha = float(config['alpha'])\n",
    "    vae_lr = float(config['vae_lr'])\n",
    "\n",
    "    #load data\n",
    "    print(\"Loading dataset...\")\n",
    "    fpath = '/axovol/training/v5/conditionsupdate_apr25.h5'\n",
    "    with h5.File(fpath, 'r') as f:\n",
    "        root = f['data'] if 'data' in f else f\n",
    "\n",
    "        #print h5 tree to view\n",
    "        print(\"Printing h5 tree...\")\n",
    "        print_h5_tree(root)\n",
    "    \n",
    "        x_train = root['Background_data']['Train']['DATA'][:]\n",
    "        x_test = root['Background_data']['Test']['DATA'][:]\n",
    "        print(f\"Train shape: {x_train.shape}, Test shape: {x_test.shape}\")\n",
    "    \n",
    "        #flatten per event\n",
    "        x_train_bkg = x_train.reshape(x_train.shape[0], -1)\n",
    "        x_test_bkg  = x_test.reshape(x_test.shape[0], -1)\n",
    "    \n",
    "        scale = root['Normalisation']['norm_scale'][:]\n",
    "        bias = root['Normalisation']['norm_bias'][:]\n",
    "    \n",
    "        l1_bits_bkg_test = root['Background_data']['Test']['L1bits'][:]\n",
    "    \n",
    "        #load signal data\n",
    "        SIGNAL_NAMES = list(root['Signal_data'].keys())\n",
    "        signal_data_dict = {}\n",
    "        signal_l1_dict = {}\n",
    "        for sname in SIGNAL_NAMES:\n",
    "            x_sig = root['Signal_data'][sname]['DATA'][:]\n",
    "            x_sig = x_sig.reshape(x_sig.shape[0], -1)\n",
    "            l1_bits = root['Signal_data'][sname]['L1bits'][:]\n",
    "            signal_data_dict[sname] = x_sig\n",
    "            signal_l1_dict[sname] = l1_bits\n",
    "\n",
    "    print(\"Data finished loading.\")\n",
    "    \n",
    "    dataset = x_train_bkg\n",
    "    dataset_test = x_test_bkg\n",
    "\n",
    "    #should be 57\n",
    "    features = dataset.shape[1]\n",
    "\n",
    "\n",
    "    print(\"Building mask...\")\n",
    "    # same mask from vae legacy\n",
    "    mask_dict = {\n",
    "        \"MET\":[True],\n",
    "        \"EGAMMA\":[True,True,True,True,False,False,False,False,False,False,False,False],\n",
    "        \"MUON\":[True,True,True,True,False,False,False,False],\n",
    "        \"JET\":[True,True,True,True,True,True,True,True,True,True,False,False]\n",
    "    }\n",
    "    \n",
    "    #build cyl_ptpz_mae mask (input scales and biases)\n",
    "    reco_loss_fn = CylPtPzMAE(scale, bias).to(device)\n",
    "    print(\"Mask is ready.\")\n",
    "\n",
    "\n",
    "    #vae config\n",
    "    latent_dim = int(config['vae_latent'])\n",
    "    enc_nodes = list(config['vae_nodes'])\n",
    "    dec_nodes = [24, 32, 64, 128, features] \n",
    "\n",
    "    vae_cfg = {\n",
    "        \"features\": features,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"encoder_config\": {\"nodes\": enc_nodes},\n",
    "        \"decoder_config\": {\"nodes\": dec_nodes},\n",
    "        \"alpha\": alpha,\n",
    "        \"beta\":  beta,\n",
    "    }\n",
    "\n",
    "    #put vae on device/init\n",
    "    vae_1 = VAE(vae_cfg).to(device)\n",
    "    vae_2 = VAE(vae_cfg).to(device)\n",
    "    print(\"VAEs are ready.\")\n",
    "\n",
    "    #optimizer (adam)\n",
    "    optimizer = torch.optim.Adam(list(vae_1.parameters()) + list(vae_2.parameters()), lr=vae_lr)\n",
    "\n",
    "    #cosine restarts\n",
    "    warmup_epochs = 10\n",
    "    cos = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=32, T_mult=2, eta_min=0.0)\n",
    "\n",
    "    #sets learning rate\n",
    "    def set_lr(lr):\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "\n",
    "    #hyperparameters\n",
    "    Epochs_VAE = 200\n",
    "    Batch_size = 16384\n",
    "\n",
    "    #get disco param from config, else set to 1\n",
    "    lambda_disco = float(config.get(\"lambda_disco\", 1))\n",
    "\n",
    "    print(\"Moving data to device...\")\n",
    "    X = torch.tensor(dataset, dtype=torch.float32, device=device)\n",
    "    print(\"Data on device.\")\n",
    "\n",
    "    #training loop\n",
    "    print(\"Starting the training loop!\")\n",
    "    N = X.size(0)\n",
    "    for epoch in range(Epochs_VAE):\n",
    "\n",
    "        #init lists for 2D plotting\n",
    "        vae1_total_loss = []\n",
    "        vae2_total_loss = []\n",
    "        vae1_kl_loss = []\n",
    "        vae2_kl_loss = []\n",
    "        vae1_reco_loss = []\n",
    "        vae2_reco_loss = []\n",
    "        \n",
    "        #cosine warmup step (sets the learning rate based on cosine schedule)\n",
    "        if epoch < warmup_epochs:\n",
    "            lr = vae_lr * (epoch + 1) / warmup_epochs\n",
    "            set_lr(lr)\n",
    "        else:\n",
    "            cos.step(epoch - warmup_epochs)\n",
    "\n",
    "        #shuffles data incides for each epoch\n",
    "        perm = torch.randperm(N, device=device)\n",
    "\n",
    "        #init losses\n",
    "        total_loss = total_reco1 = total_reco2 = total_kl1 = total_kl2 = total_disco = 0.0\n",
    "\n",
    "        #loops over dataset in steps of batch sizze\n",
    "        for i in range(0, N, Batch_size):\n",
    "            #picks indices for current batch\n",
    "            idx = perm[i:i+Batch_size]\n",
    "\n",
    "            #selects batch of samples from data set X\n",
    "            xb = X[idx]\n",
    "\n",
    "            #vae 1\n",
    "            recon1, mu1, logvar1, z1 = vae_1(xb)\n",
    "            \n",
    "            #vae 2\n",
    "            recon2, mu2, logvar2, z2 = vae_2(xb)\n",
    "\n",
    "            #get reco loss from custom func\n",
    "            reco1_per = reco_loss_fn(recon1, xb)\n",
    "            reco2_per = reco_loss_fn(recon2, xb)\n",
    "\n",
    "            #get kl div per sample\n",
    "            kl1_per = VAE.kl_divergence(mu1, logvar1)\n",
    "            kl2_per = VAE.kl_divergence(mu2, logvar2)\n",
    "\n",
    "            tot1_per = reco1_per + kl1_per\n",
    "            tot2_per = reco2_per + kl2_per\n",
    "\n",
    "            #from paper code weight\n",
    "            B = xb.shape[0]\n",
    "            w = torch.ones(B, device=tot1_per.device, dtype=tot1_per.dtype)\n",
    "\n",
    "            #disco loss (ask Melissa about since using mu instead of z)\n",
    "            #disco = disco_loss(mu1, mu2)\n",
    "            disco = distance_corr((tot1_per), (tot2_per), w, power=1)\n",
    "\n",
    "            #same scaling from vae legacy\n",
    "            reco1 = vae_1.reco_scale*reco1_per.mean()\n",
    "            reco2 = vae_2.reco_scale*reco2_per.mean()\n",
    "            kl1 = vae_1.kl_scale*kl1_per.mean()\n",
    "            kl2 = vae_2.kl_scale*kl2_per.mean() \n",
    "\n",
    "            #calc total loss (scalar)\n",
    "            loss = (reco1 + kl1) + (reco2 + kl2) + lambda_disco * disco\n",
    "\n",
    "            #zero grads\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            #backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            #do some gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(list(vae_1.parameters()) + list(vae_2.parameters()), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            #add losses to list for wandb plotting\n",
    "            total_loss+=loss.item()\n",
    "            total_reco1+=reco1.item()\n",
    "            total_reco2+=reco2.item()\n",
    "            total_kl1+=kl1.item()\n",
    "            total_kl2+=kl2.item()\n",
    "            total_disco+=disco.item()\n",
    "\n",
    "            #add loss to list for plotting\n",
    "            vae1_total_loss.append((reco1 + kl1).item())\n",
    "            vae2_total_loss.append((reco2 + kl2).item())\n",
    "            vae1_kl_loss.append(kl1.item())\n",
    "            vae2_kl_loss.append(kl2.item())\n",
    "            vae1_reco_loss.append(reco1.item())\n",
    "            vae2_reco_loss.append(reco2.item())\n",
    "\n",
    "        print(f\"[EPOCH {epoch}/{Epochs_VAE}] \"\n",
    "          f\"Loss={total_loss:.4f} \"\n",
    "          f\"Reco1={total_reco1:.4f} Reco2={total_reco2:.4f} \"\n",
    "          f\"KL1={total_kl1:.4f} KL2={total_kl2:.4f} \"\n",
    "          f\"DisCo={total_disco:.4f}\")\n",
    "\n",
    "        #log in wandb\n",
    "        wandb.log({\n",
    "            \"EpochVae\": epoch,\n",
    "            \"TotalLossVae\": total_loss,\n",
    "            \"RecoLossVae1\": total_reco1,\n",
    "            \"RecoLossVae2\": total_reco2,\n",
    "            \"KLLossVae1\": total_kl1,\n",
    "            \"KLLossVae2\": total_kl2,\n",
    "            \"DisCoLoss\": total_disco,\n",
    "        })\n",
    "\n",
    "        #convert to np array for plot\n",
    "        vae1_total_np = np.array(vae1_total_loss)\n",
    "        vae2_total_np = np.array(vae2_total_loss)\n",
    "        vae1_kl_np = np.array(vae1_kl_loss)\n",
    "        vae2_kl_np = np.array(vae2_kl_loss)\n",
    "        vae1_reco_np = np.array(vae1_reco_loss)\n",
    "        vae2_reco_np = np.array(vae2_reco_loss)\n",
    "\n",
    "        make_2D_hist(vae1_total_np, vae2_total_np,\n",
    "                    \"Total Loss (VAE1)\", \"Total Loss (VAE2)\",\n",
    "                    f\"Epoch {epoch}: Total VAE1 vs Total VAE2\",\n",
    "                    wandb_key=\"Hists2D/Total_VAE1_vs_Total_VAE2\")\n",
    "\n",
    "        make_2D_hist(vae1_kl_np, vae2_kl_np,\n",
    "                    \"KL Loss (VAE1)\", \"KL Loss (VAE2)\",\n",
    "                    f\"Epoch {epoch}: KL VAE1 vs KL VAE2\",\n",
    "                    wandb_key=\"Hists2D/KL_VAE1_vs_KL_VAE2\")\n",
    "\n",
    "        make_2D_hist(vae1_reco_np, vae2_reco_np,\n",
    "                    \"Reco Loss (VAE1)\", \"Reco Loss (VAE2)\",\n",
    "                    f\"Epoch {epoch}: Reco VAE1 vs Reco VAE2\",\n",
    "                    wandb_key=\"Hists2D/Reco_VAE1_vs_Reco_VAE2\")\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"Finished training.\")\n",
    "\n",
    "    #save models\n",
    "    torch.save(vae_1.state_dict(), \"vae1_trained.pth\")\n",
    "    torch.save(vae_2.state_dict(), \"vae2_trained.pth\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8160a1fc-439a-429f-a154-7c134e0ecfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n",
      "Logging in to wandb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jovyan/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mescheuller\u001b[0m (\u001b[33mescheuller-uc-san-diego\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20250909_192552-55nyv5c3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/55nyv5c3' target=\"_blank\">decent-breeze-137</a></strong> to <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training' target=\"_blank\">https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/55nyv5c3' target=\"_blank\">https://wandb.ai/escheuller-uc-san-diego/Double%20Disco%20Axo%20Training/runs/55nyv5c3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: decent-breeze-137\n",
      "Loading dataset...\n",
      "Printing h5 tree...\n",
      "[GROUP] Background_data\n",
      "  [GROUP] Test\n",
      "    DATA: shape=(4511092, 19, 3), dtype=float64\n",
      "    ET: shape=(4511092,), dtype=float16\n",
      "    HT: shape=(4511092,), dtype=float16\n",
      "    L1bits: shape=(4511092,), dtype=bool\n",
      "    PU: shape=(4511092, 7), dtype=float64\n",
      "  [GROUP] Train\n",
      "    DATA: shape=(1999965, 19, 3), dtype=float64\n",
      "    ET: shape=(1999965,), dtype=float16\n",
      "    HT: shape=(1999965,), dtype=float16\n",
      "    L1bits: shape=(1999965,), dtype=bool\n",
      "    PU: shape=(1999965, 7), dtype=float64\n",
      "[GROUP] Normalisation\n",
      "  norm_bias: shape=(19, 3), dtype=float64\n",
      "  norm_scale: shape=(19, 3), dtype=float64\n",
      "[GROUP] Signal_data\n",
      "  [GROUP] GluGluHToBB_M-125\n",
      "    DATA: shape=(179997, 19, 3), dtype=float64\n",
      "    ET: shape=(179997,), dtype=float16\n",
      "    HT: shape=(179997,), dtype=float16\n",
      "    L1bits: shape=(179997,), dtype=bool\n",
      "    PU: shape=(179997,), dtype=float64\n",
      "  [GROUP] GluGluHToGG_M-125\n",
      "    DATA: shape=(194433, 19, 3), dtype=float64\n",
      "    ET: shape=(194433,), dtype=float16\n",
      "    HT: shape=(194433,), dtype=float16\n",
      "    L1bits: shape=(194433,), dtype=bool\n",
      "    PU: shape=(194433,), dtype=float64\n",
      "  [GROUP] GluGluHToGG_M-90\n",
      "    DATA: shape=(196706, 19, 3), dtype=float64\n",
      "    ET: shape=(196706,), dtype=float16\n",
      "    HT: shape=(196706,), dtype=float16\n",
      "    L1bits: shape=(196706,), dtype=bool\n",
      "    PU: shape=(196706,), dtype=float64\n",
      "  [GROUP] GluGluHToTauTau\n",
      "    DATA: shape=(199261, 19, 3), dtype=float64\n",
      "    ET: shape=(199261,), dtype=float16\n",
      "    HT: shape=(199261,), dtype=float16\n",
      "    L1bits: shape=(199261,), dtype=bool\n",
      "    PU: shape=(199261,), dtype=float64\n",
      "  [GROUP] GluGlutoHHto2B2WtoLNu2Q\n",
      "    DATA: shape=(195151, 19, 3), dtype=float64\n",
      "    ET: shape=(195151,), dtype=float16\n",
      "    HT: shape=(195151,), dtype=float16\n",
      "    L1bits: shape=(195151,), dtype=bool\n",
      "    PU: shape=(195151,), dtype=float64\n",
      "  [GROUP] HHHTo6B\n",
      "    DATA: shape=(184324, 19, 3), dtype=float64\n",
      "    ET: shape=(184324,), dtype=float16\n",
      "    HT: shape=(184324,), dtype=float16\n",
      "    L1bits: shape=(184324,), dtype=bool\n",
      "    PU: shape=(184324,), dtype=float64\n",
      "  [GROUP] HHHto4B2Tau\n",
      "    DATA: shape=(184731, 19, 3), dtype=float64\n",
      "    ET: shape=(184731,), dtype=float16\n",
      "    HT: shape=(184731,), dtype=float16\n",
      "    L1bits: shape=(184731,), dtype=bool\n",
      "    PU: shape=(184731,), dtype=float64\n",
      "  [GROUP] HTo2LongLivedTo4b_MH-1000_MFF-450_CTau-100000mm\n",
      "    DATA: shape=(36272, 19, 3), dtype=float64\n",
      "    ET: shape=(36272,), dtype=float16\n",
      "    HT: shape=(36272,), dtype=float16\n",
      "    L1bits: shape=(36272,), dtype=bool\n",
      "    PU: shape=(36272,), dtype=float64\n",
      "  [GROUP] HTo2LongLivedTo4b_MH-1000_MFF-450_CTau-10000mm\n",
      "    DATA: shape=(26821, 19, 3), dtype=float64\n",
      "    ET: shape=(26821,), dtype=float16\n",
      "    HT: shape=(26821,), dtype=float16\n",
      "    L1bits: shape=(26821,), dtype=bool\n",
      "    PU: shape=(26821,), dtype=float64\n",
      "  [GROUP] HTo2LongLivedTo4b_MH-125_MFF-12_CTau-900mm\n",
      "    DATA: shape=(39489, 19, 3), dtype=float64\n",
      "    ET: shape=(39489,), dtype=float16\n",
      "    HT: shape=(39489,), dtype=float16\n",
      "    L1bits: shape=(39489,), dtype=bool\n",
      "    PU: shape=(39489,), dtype=float64\n",
      "  [GROUP] HTo2LongLivedTo4b_MH-125_MFF-25_CTau-1500mm\n",
      "    DATA: shape=(39520, 19, 3), dtype=float64\n",
      "    ET: shape=(39520,), dtype=float16\n",
      "    HT: shape=(39520,), dtype=float16\n",
      "    L1bits: shape=(39520,), dtype=bool\n",
      "    PU: shape=(39520,), dtype=float64\n",
      "  [GROUP] HTo2LongLivedTo4b_MH-125_MFF-50_CTau-3000mm\n",
      "    DATA: shape=(38219, 19, 3), dtype=float64\n",
      "    ET: shape=(38219,), dtype=float16\n",
      "    HT: shape=(38219,), dtype=float16\n",
      "    L1bits: shape=(38219,), dtype=bool\n",
      "    PU: shape=(38219,), dtype=float64\n",
      "  [GROUP] HTo2LongLivedTo4mu_MH-125_MFF-12_CTau-900mm\n",
      "    DATA: shape=(39807, 19, 3), dtype=float64\n",
      "    ET: shape=(39807,), dtype=float16\n",
      "    HT: shape=(39807,), dtype=float16\n",
      "    L1bits: shape=(39807,), dtype=bool\n",
      "    PU: shape=(39807,), dtype=float64\n",
      "  [GROUP] HTo2LongLivedTo4mu_MH-125_MFF-25_CTau-1500mm\n",
      "    DATA: shape=(39813, 19, 3), dtype=float64\n",
      "    ET: shape=(39813,), dtype=float16\n",
      "    HT: shape=(39813,), dtype=float16\n",
      "    L1bits: shape=(39813,), dtype=bool\n",
      "    PU: shape=(39813,), dtype=float64\n",
      "  [GROUP] HTo2LongLivedTo4mu_MH-125_MFF-50_CTau-3000mm\n",
      "    DATA: shape=(39831, 19, 3), dtype=float64\n",
      "    ET: shape=(39831,), dtype=float16\n",
      "    HT: shape=(39831,), dtype=float16\n",
      "    L1bits: shape=(39831,), dtype=bool\n",
      "    PU: shape=(39831,), dtype=float64\n",
      "  [GROUP] SMS-Higgsino\n",
      "    DATA: shape=(93479, 19, 3), dtype=float64\n",
      "    ET: shape=(93479,), dtype=float16\n",
      "    HT: shape=(93479,), dtype=float16\n",
      "    L1bits: shape=(93479,), dtype=bool\n",
      "    PU: shape=(93479,), dtype=float64\n",
      "  [GROUP] SUSYGluGluToBBHToBB_NarrowWidth_M-120\n",
      "    DATA: shape=(42461, 19, 3), dtype=float64\n",
      "    ET: shape=(42461,), dtype=float16\n",
      "    HT: shape=(42461,), dtype=float16\n",
      "    L1bits: shape=(42461,), dtype=bool\n",
      "    PU: shape=(42461,), dtype=float64\n",
      "  [GROUP] SUSYGluGluToBBHToBB_NarrowWidth_M-1200\n",
      "    DATA: shape=(42461, 19, 3), dtype=float64\n",
      "    ET: shape=(42461,), dtype=float16\n",
      "    HT: shape=(42461,), dtype=float16\n",
      "    L1bits: shape=(42461,), dtype=bool\n",
      "    PU: shape=(42461,), dtype=float64\n",
      "  [GROUP] SUSYGluGluToBBHToBB_NarrowWidth_M-350\n",
      "    DATA: shape=(192712, 19, 3), dtype=float64\n",
      "    ET: shape=(192712,), dtype=float16\n",
      "    HT: shape=(192712,), dtype=float16\n",
      "    L1bits: shape=(192712,), dtype=bool\n",
      "    PU: shape=(192712,), dtype=float64\n",
      "  [GROUP] SUSYGluGluToBBHToBB_NarrowWidth_M-600\n",
      "    DATA: shape=(45934, 19, 3), dtype=float64\n",
      "    ET: shape=(45934,), dtype=float16\n",
      "    HT: shape=(45934,), dtype=float16\n",
      "    L1bits: shape=(45934,), dtype=bool\n",
      "    PU: shape=(45934,), dtype=float64\n",
      "  [GROUP] SingleNeutrino_E-10-gun\n",
      "    DATA: shape=(199998, 19, 3), dtype=float64\n",
      "    ET: shape=(199998,), dtype=float16\n",
      "    HT: shape=(199998,), dtype=float16\n",
      "    L1bits: shape=(199998,), dtype=bool\n",
      "    PU: shape=(199998,), dtype=float64\n",
      "  [GROUP] SingleNeutrino_Pt-2To20-gun\n",
      "    DATA: shape=(199998, 19, 3), dtype=float64\n",
      "    ET: shape=(199998,), dtype=float16\n",
      "    HT: shape=(199998,), dtype=float16\n",
      "    L1bits: shape=(199998,), dtype=bool\n",
      "    PU: shape=(199998,), dtype=float64\n",
      "  [GROUP] VBFHToCC\n",
      "    DATA: shape=(197642, 19, 3), dtype=float64\n",
      "    ET: shape=(197642,), dtype=float16\n",
      "    HT: shape=(197642,), dtype=float16\n",
      "    L1bits: shape=(197642,), dtype=bool\n",
      "    PU: shape=(197642,), dtype=float64\n",
      "  [GROUP] VBFHToInvisible\n",
      "    DATA: shape=(198176, 19, 3), dtype=float64\n",
      "    ET: shape=(198176,), dtype=float16\n",
      "    HT: shape=(198176,), dtype=float16\n",
      "    L1bits: shape=(198176,), dtype=bool\n",
      "    PU: shape=(198176,), dtype=float64\n",
      "  [GROUP] VBFHToTauTau\n",
      "    DATA: shape=(196516, 19, 3), dtype=float64\n",
      "    ET: shape=(196516,), dtype=float16\n",
      "    HT: shape=(196516,), dtype=float16\n",
      "    L1bits: shape=(196516,), dtype=bool\n",
      "    PU: shape=(196516,), dtype=float64\n",
      "  [GROUP] VBFHto2B\n",
      "    DATA: shape=(197776, 19, 3), dtype=float64\n",
      "    ET: shape=(197776,), dtype=float16\n",
      "    HT: shape=(197776,), dtype=float16\n",
      "    L1bits: shape=(197776,), dtype=bool\n",
      "    PU: shape=(197776,), dtype=float64\n",
      "  [GROUP] WToTauTo3Mu\n",
      "    DATA: shape=(89251, 19, 3), dtype=float64\n",
      "    ET: shape=(89251,), dtype=float16\n",
      "    HT: shape=(89251,), dtype=float16\n",
      "    L1bits: shape=(89251,), dtype=bool\n",
      "    PU: shape=(89251,), dtype=float64\n",
      "  [GROUP] ggXToJpsiJpsiTo2Mu2E_m7\n",
      "    DATA: shape=(196724, 19, 3), dtype=float64\n",
      "    ET: shape=(196724,), dtype=float16\n",
      "    HT: shape=(196724,), dtype=float16\n",
      "    L1bits: shape=(196724,), dtype=bool\n",
      "    PU: shape=(196724,), dtype=float64\n",
      "  [GROUP] ggXToYYTo2Mu2E_m18\n",
      "    DATA: shape=(199997, 19, 3), dtype=float64\n",
      "    ET: shape=(199997,), dtype=float16\n",
      "    HT: shape=(199997,), dtype=float16\n",
      "    L1bits: shape=(199997,), dtype=bool\n",
      "    PU: shape=(199997,), dtype=float64\n",
      "  [GROUP] ggXToYYTo2Mu2E_m26\n",
      "    DATA: shape=(199013, 19, 3), dtype=float64\n",
      "    ET: shape=(199013,), dtype=float16\n",
      "    HT: shape=(199013,), dtype=float16\n",
      "    L1bits: shape=(199013,), dtype=bool\n",
      "    PU: shape=(199013,), dtype=float64\n",
      "  [GROUP] haa-4b-ma15-POWHEG\n",
      "    DATA: shape=(98834, 19, 3), dtype=float64\n",
      "    ET: shape=(98834,), dtype=float16\n",
      "    HT: shape=(98834,), dtype=float16\n",
      "    L1bits: shape=(98834,), dtype=bool\n",
      "    PU: shape=(98834,), dtype=float64\n",
      "  [GROUP] ttHto2B\n",
      "    DATA: shape=(187020, 19, 3), dtype=float64\n",
      "    ET: shape=(187020,), dtype=float16\n",
      "    HT: shape=(187020,), dtype=float16\n",
      "    L1bits: shape=(187020,), dtype=bool\n",
      "    PU: shape=(187020,), dtype=float64\n",
      "  [GROUP] ttHto2C\n",
      "    DATA: shape=(186283, 19, 3), dtype=float64\n",
      "    ET: shape=(186283,), dtype=float16\n",
      "    HT: shape=(186283,), dtype=float16\n",
      "    L1bits: shape=(186283,), dtype=bool\n",
      "    PU: shape=(186283,), dtype=float64\n",
      "Train shape: (1999965, 19, 3), Test shape: (4511092, 19, 3)\n",
      "Data finished loading.\n",
      "Building mask...\n",
      "Mask is ready.\n",
      "VAEs are ready.\n",
      "Moving data to device...\n",
      "Data on device.\n",
      "Starting the training loop!\n",
      "[EPOCH 0/200] Loss=1292197.0156 Reco1=576715.1865 Reco2=605990.8936 KL1=13.0033 KL2=10.1555 DisCo=10.9468\n",
      "[EPOCH 1/200] Loss=1133254.0869 Reco1=516901.7783 Reco2=541363.6562 KL1=12.7918 KL2=9.9139 DisCo=7.4966\n",
      "[EPOCH 2/200] Loss=930308.7056 Reco1=438872.4822 Reco2=438670.5728 KL1=12.3683 KL2=9.7341 DisCo=5.2744\n",
      "[EPOCH 3/200] Loss=801192.7310 Reco1=377374.2498 Reco2=363306.3066 KL1=12.1624 KL2=9.1773 DisCo=6.0491\n",
      "[EPOCH 4/200] Loss=749300.2598 Reco1=355891.9465 Reco2=335006.9771 KL1=14.2775 KL2=8.3006 DisCo=5.8379\n",
      "[EPOCH 5/200] Loss=726951.9385 Reco1=350233.9951 Reco2=321253.0979 KL1=19.1670 KL2=8.2372 DisCo=5.5437\n",
      "[EPOCH 6/200] Loss=707756.0430 Reco1=352740.6191 Reco2=305563.7153 KL1=28.1106 KL2=16.5215 DisCo=4.9407\n",
      "[EPOCH 7/200] Loss=684816.8066 Reco1=340853.2195 Reco2=304547.0911 KL1=54.4214 KL2=58.5729 DisCo=3.9303\n",
      "[EPOCH 8/200] Loss=668355.6182 Reco1=323304.2581 Reco2=311822.8865 KL1=102.0770 KL2=144.6729 DisCo=3.2982\n",
      "[EPOCH 9/200] Loss=646456.6675 Reco1=307939.3237 Reco2=308636.5376 KL1=230.4259 KL2=412.0005 DisCo=2.9238\n",
      "[EPOCH 10/200] Loss=613593.7480 Reco1=291900.7102 Reco2=293182.1741 KL1=513.3551 KL2=893.8780 DisCo=2.7104\n",
      "[EPOCH 11/200] Loss=578748.7920 Reco1=276523.9309 Reco2=274175.8677 KL1=798.9482 KL2=1482.2376 DisCo=2.5768\n",
      "[EPOCH 12/200] Loss=545189.8369 Reco1=258550.6486 Reco2=258321.7968 KL1=1194.3656 KL2=1958.0045 DisCo=2.5165\n",
      "[EPOCH 13/200] Loss=519577.9714 Reco1=244754.6562 Reco2=246734.6329 KL1=1490.6981 KL2=2105.9084 DisCo=2.4492\n",
      "[EPOCH 14/200] Loss=489880.7659 Reco1=229612.5652 Reco2=233191.7594 KL1=1714.8263 KL2=2274.8290 DisCo=2.3087\n",
      "[EPOCH 15/200] Loss=460754.6836 Reco1=214853.1302 Reco2=219967.0974 KL1=1920.7374 KL2=2456.4511 DisCo=2.1557\n",
      "[EPOCH 16/200] Loss=439016.0959 Reco1=203799.3909 Reco2=209810.1497 KL1=2032.4037 KL2=2602.9954 DisCo=2.0771\n",
      "[EPOCH 17/200] Loss=421281.1782 Reco1=195081.3666 Reco2=201252.4174 KL1=2112.7535 KL2=2685.2143 DisCo=2.0149\n",
      "[EPOCH 18/200] Loss=406858.0828 Reco1=188471.4276 Reco2=193740.6072 KL1=2161.9083 KL2=2768.7372 DisCo=1.9715\n",
      "[EPOCH 19/200] Loss=395305.8621 Reco1=184402.9664 Reco2=185889.1783 KL1=2209.6643 KL2=2814.1189 DisCo=1.9990\n",
      "[EPOCH 20/200] Loss=385923.8447 Reco1=181542.3311 Reco2=178711.2379 KL1=2255.3154 KL2=2848.2527 DisCo=2.0567\n",
      "[EPOCH 21/200] Loss=378023.3521 Reco1=178454.2668 Reco2=173520.1870 KL1=2283.4894 KL2=2889.0150 DisCo=2.0876\n",
      "[EPOCH 22/200] Loss=371688.8206 Reco1=175976.4121 Reco2=169507.8988 KL1=2298.9102 KL2=2927.2364 DisCo=2.0978\n",
      "[EPOCH 23/200] Loss=366835.2976 Reco1=173599.7800 Reco2=167063.1287 KL1=2307.2210 KL2=2957.3534 DisCo=2.0908\n",
      "[EPOCH 24/200] Loss=362802.6880 Reco1=171529.5643 Reco2=165216.0680 KL1=2320.1956 KL2=2991.7839 DisCo=2.0745\n",
      "[EPOCH 25/200] Loss=359552.0159 Reco1=169879.0558 Reco2=163686.0776 KL1=2330.9789 KL2=3007.3075 DisCo=2.0649\n",
      "[EPOCH 26/200] Loss=356768.2319 Reco1=168705.4713 Reco2=162074.3733 KL1=2341.1369 KL2=3020.1324 DisCo=2.0627\n",
      "[EPOCH 27/200] Loss=354620.6875 Reco1=167683.6040 Reco2=161119.7668 KL1=2348.7765 KL2=3030.3625 DisCo=2.0438\n",
      "[EPOCH 28/200] Loss=352683.1035 Reco1=166691.2611 Reco2=160148.6917 KL1=2355.3400 KL2=3037.9273 DisCo=2.0450\n",
      "[EPOCH 29/200] Loss=351022.2759 Reco1=165843.4900 Reco2=159477.0846 KL1=2360.1188 KL2=3043.2957 DisCo=2.0298\n",
      "[EPOCH 30/200] Loss=349783.8760 Reco1=165326.8253 Reco2=158901.8560 KL1=2363.6692 KL2=3042.5999 DisCo=2.0149\n",
      "[EPOCH 31/200] Loss=348756.7119 Reco1=164789.6150 Reco2=158254.4592 KL1=2367.8696 KL2=3048.0899 DisCo=2.0297\n",
      "[EPOCH 32/200] Loss=347714.0781 Reco1=164407.8552 Reco2=157899.8936 KL1=2370.6531 KL2=3049.3201 DisCo=1.9986\n",
      "[EPOCH 33/200] Loss=346787.2219 Reco1=163906.8647 Reco2=157332.1913 KL1=2375.4824 KL2=3055.3382 DisCo=2.0117\n",
      "[EPOCH 34/200] Loss=346241.5566 Reco1=163732.7111 Reco2=157133.5640 KL1=2380.2557 KL2=3056.1614 DisCo=1.9939\n",
      "[EPOCH 35/200] Loss=345874.5676 Reco1=163496.6527 Reco2=156900.4406 KL1=2383.0428 KL2=3054.5616 DisCo=2.0040\n",
      "[EPOCH 36/200] Loss=345500.1335 Reco1=163361.8464 Reco2=156765.2513 KL1=2385.0645 KL2=3058.4431 DisCo=1.9930\n",
      "[EPOCH 37/200] Loss=344908.8577 Reco1=163115.3160 Reco2=156522.0493 KL1=2387.3059 KL2=3058.7923 DisCo=1.9825\n",
      "[EPOCH 38/200] Loss=344968.2034 Reco1=163165.2563 Reco2=156354.0488 KL1=2389.0535 KL2=3057.4943 DisCo=2.0002\n",
      "[EPOCH 39/200] Loss=344715.8040 Reco1=163054.5740 Reco2=156380.7179 KL1=2389.7100 KL2=3060.0861 DisCo=1.9831\n",
      "[EPOCH 40/200] Loss=344793.5859 Reco1=163040.1385 Reco2=156369.8639 KL1=2390.1586 KL2=3060.1188 DisCo=1.9933\n",
      "[EPOCH 41/200] Loss=344807.3052 Reco1=163037.1063 Reco2=156300.7268 KL1=2390.3063 KL2=3060.1472 DisCo=2.0019\n",
      "[EPOCH 42/200] Loss=344293.1606 Reco1=162628.4495 Reco2=156180.1110 KL1=2405.2169 KL2=3061.4670 DisCo=2.0018\n",
      "[EPOCH 43/200] Loss=341539.5542 Reco1=161372.7660 Reco2=154747.2515 KL1=2421.8167 KL2=3061.3151 DisCo=1.9936\n",
      "[EPOCH 44/200] Loss=338570.0698 Reco1=159875.3625 Reco2=153522.9581 KL1=2441.6883 KL2=3061.3971 DisCo=1.9669\n",
      "[EPOCH 45/200] Loss=335698.3262 Reco1=158320.1453 Reco2=152390.5226 KL1=2460.8912 KL2=3065.9020 DisCo=1.9461\n",
      "[EPOCH 46/200] Loss=333187.7891 Reco1=156718.7107 Reco2=151217.4113 KL1=2478.5441 KL2=3059.3335 DisCo=1.9714\n",
      "[EPOCH 47/200] Loss=330011.3315 Reco1=155137.0204 Reco2=149967.9047 KL1=2494.4638 KL2=3064.3553 DisCo=1.9348\n",
      "[EPOCH 48/200] Loss=327103.8672 Reco1=153260.8826 Reco2=148844.9966 KL1=2520.3213 KL2=3062.5995 DisCo=1.9415\n",
      "[EPOCH 49/200] Loss=324121.4663 Reco1=151245.6877 Reco2=147892.5347 KL1=2548.3638 KL2=3061.4410 DisCo=1.9373\n",
      "[EPOCH 50/200] Loss=321775.4722 Reco1=149786.9904 Reco2=146969.1956 KL1=2569.1033 KL2=3069.5461 DisCo=1.9381\n",
      "[EPOCH 51/200] Loss=319206.0237 Reco1=148341.6851 Reco2=145910.0566 KL1=2593.6247 KL2=3055.2841 DisCo=1.9305\n",
      "[EPOCH 52/200] Loss=316980.8079 Reco1=146911.5347 Reco2=145060.7972 KL1=2617.7551 KL2=3056.1663 DisCo=1.9335\n",
      "[EPOCH 53/200] Loss=315030.3042 Reco1=145982.0178 Reco2=144178.0807 KL1=2634.1106 KL2=3054.8980 DisCo=1.9181\n",
      "[EPOCH 54/200] Loss=313333.8025 Reco1=144748.2488 Reco2=143455.0105 KL1=2652.6511 KL2=3052.0438 DisCo=1.9426\n",
      "[EPOCH 55/200] Loss=310891.3269 Reco1=143488.6509 Reco2=142358.0774 KL1=2673.3425 KL2=3046.3254 DisCo=1.9325\n",
      "[EPOCH 56/200] Loss=308655.8113 Reco1=142398.2838 Reco2=141403.6573 KL1=2685.0701 KL2=3041.2045 DisCo=1.9128\n",
      "[EPOCH 57/200] Loss=306669.7922 Reco1=141299.1892 Reco2=140596.0138 KL1=2696.5904 KL2=3045.4634 DisCo=1.9033\n",
      "[EPOCH 58/200] Loss=304451.6736 Reco1=139979.3083 Reco2=139794.9910 KL1=2711.9919 KL2=3030.8320 DisCo=1.8935\n",
      "[EPOCH 59/200] Loss=302927.5906 Reco1=138880.5360 Reco2=139166.9131 KL1=2732.5605 KL2=3027.2278 DisCo=1.9120\n",
      "[EPOCH 60/200] Loss=301017.5010 Reco1=137804.5247 Reco2=138497.7302 KL1=2748.1177 KL2=3029.4417 DisCo=1.8938\n",
      "[EPOCH 61/200] Loss=299010.0535 Reco1=136843.4253 Reco2=137661.7500 KL1=2758.9292 KL2=3019.8555 DisCo=1.8726\n",
      "[EPOCH 62/200] Loss=297285.0947 Reco1=136086.6353 Reco2=136805.5485 KL1=2774.7431 KL2=3024.2601 DisCo=1.8594\n",
      "[EPOCH 63/200] Loss=295208.6848 Reco1=134868.2434 Reco2=136250.7271 KL1=2790.0606 KL2=3020.2302 DisCo=1.8279\n",
      "[EPOCH 64/200] Loss=293623.2139 Reco1=134292.5912 Reco2=135587.7402 KL1=2797.7306 KL2=3015.3518 DisCo=1.7930\n",
      "[EPOCH 65/200] Loss=291672.7788 Reco1=133486.3523 Reco2=134977.6744 KL1=2812.9304 KL2=3013.0964 DisCo=1.7383\n",
      "[EPOCH 66/200] Loss=290099.5264 Reco1=132739.2949 Reco2=134384.5581 KL1=2826.9887 KL2=3020.7234 DisCo=1.7128\n",
      "[EPOCH 67/200] Loss=288511.2725 Reco1=131781.1647 Reco2=134009.8508 KL1=2837.2842 KL2=3017.3165 DisCo=1.6866\n",
      "[EPOCH 68/200] Loss=287153.8372 Reco1=131062.7532 Reco2=133384.9751 KL1=2846.2197 KL2=3017.6057 DisCo=1.6842\n",
      "[EPOCH 69/200] Loss=285520.0073 Reco1=130311.1189 Reco2=132718.9823 KL1=2858.6802 KL2=3017.3683 DisCo=1.6614\n",
      "[EPOCH 70/200] Loss=284267.6831 Reco1=129824.4791 Reco2=132046.8049 KL1=2865.6333 KL2=3016.8731 DisCo=1.6514\n",
      "[EPOCH 71/200] Loss=283108.6118 Reco1=129311.1050 Reco2=131411.8380 KL1=2872.8226 KL2=3010.4726 DisCo=1.6502\n",
      "[EPOCH 72/200] Loss=282011.5830 Reco1=128760.8726 Reco2=130877.3011 KL1=2878.4003 KL2=3005.5435 DisCo=1.6489\n",
      "[EPOCH 73/200] Loss=281051.1055 Reco1=128294.4160 Reco2=130366.9633 KL1=2886.3983 KL2=3000.3712 DisCo=1.6503\n",
      "[EPOCH 74/200] Loss=279704.9446 Reco1=127717.7129 Reco2=129779.5718 KL1=2889.6398 KL2=2993.1326 DisCo=1.6325\n",
      "[EPOCH 75/200] Loss=278815.7290 Reco1=127383.0276 Reco2=129227.5771 KL1=2892.2200 KL2=2991.9558 DisCo=1.6321\n",
      "[EPOCH 76/200] Loss=277856.1523 Reco1=126870.1039 Reco2=128751.6450 KL1=2900.0265 KL2=2992.2744 DisCo=1.6342\n",
      "[EPOCH 77/200] Loss=277297.3208 Reco1=126639.2491 Reco2=128434.6959 KL1=2902.3267 KL2=2992.9079 DisCo=1.6328\n",
      "[EPOCH 78/200] Loss=276474.4944 Reco1=126140.4448 Reco2=128012.0309 KL1=2907.1912 KL2=2990.8632 DisCo=1.6424\n",
      "[EPOCH 79/200] Loss=275953.2358 Reco1=125830.9035 Reco2=127815.9401 KL1=2909.9066 KL2=2980.6532 DisCo=1.6416\n",
      "[EPOCH 80/200] Loss=275082.9351 Reco1=125726.4633 Reco2=127333.4452 KL1=2915.0351 KL2=2980.6460 DisCo=1.6127\n",
      "[EPOCH 81/200] Loss=274524.0461 Reco1=125332.9421 Reco2=127097.0245 KL1=2919.7429 KL2=2980.3869 DisCo=1.6194\n",
      "[EPOCH 82/200] Loss=273955.3486 Reco1=125192.5920 Reco2=126774.1461 KL1=2924.4215 KL2=2979.5731 DisCo=1.6085\n",
      "[EPOCH 83/200] Loss=273620.9180 Reco1=124925.8165 Reco2=126565.4150 KL1=2926.2979 KL2=2978.7159 DisCo=1.6225\n",
      "[EPOCH 84/200] Loss=273021.8826 Reco1=124770.2669 Reco2=126259.0788 KL1=2926.7165 KL2=2978.3462 DisCo=1.6087\n",
      "[EPOCH 85/200] Loss=272647.6135 Reco1=124720.5330 Reco2=126042.6600 KL1=2929.9083 KL2=2977.8645 DisCo=1.5977\n",
      "[EPOCH 86/200] Loss=272117.4883 Reco1=124269.6758 Reco2=125850.0679 KL1=2930.5280 KL2=2978.6398 DisCo=1.6089\n",
      "[EPOCH 87/200] Loss=271786.1284 Reco1=124200.5259 Reco2=125575.0406 KL1=2931.2803 KL2=2978.9106 DisCo=1.6100\n",
      "[EPOCH 88/200] Loss=271575.7095 Reco1=124128.0000 Reco2=125553.3404 KL1=2931.8045 KL2=2977.6902 DisCo=1.5985\n",
      "[EPOCH 89/200] Loss=271364.2844 Reco1=124023.7717 Reco2=125400.4980 KL1=2933.3288 KL2=2976.3341 DisCo=1.6030\n",
      "[EPOCH 90/200] Loss=271143.5798 Reco1=123894.3948 Reco2=125224.5305 KL1=2932.7546 KL2=2974.5480 DisCo=1.6117\n",
      "[EPOCH 91/200] Loss=270819.7468 Reco1=123788.5592 Reco2=125149.7338 KL1=2933.6964 KL2=2975.3807 DisCo=1.5972\n",
      "[EPOCH 92/200] Loss=270502.0518 Reco1=123593.2860 Reco2=125016.2498 KL1=2934.3633 KL2=2976.9250 DisCo=1.5981\n",
      "[EPOCH 93/200] Loss=270412.5510 Reco1=123664.7664 Reco2=124933.9203 KL1=2934.2739 KL2=2977.8069 DisCo=1.5902\n",
      "[EPOCH 94/200] Loss=270195.8193 Reco1=123470.0843 Reco2=124834.4131 KL1=2935.0169 KL2=2977.9791 DisCo=1.5978\n",
      "[EPOCH 95/200] Loss=270263.4067 Reco1=123457.3399 Reco2=124729.0580 KL1=2934.4796 KL2=2978.5119 DisCo=1.6164\n",
      "[EPOCH 96/200] Loss=270166.5757 Reco1=123442.7098 Reco2=124654.8825 KL1=2935.0226 KL2=2978.5035 DisCo=1.6155\n",
      "[EPOCH 97/200] Loss=269896.6917 Reco1=123372.1387 Reco2=124635.5559 KL1=2935.6431 KL2=2978.9372 DisCo=1.5974\n",
      "[EPOCH 98/200] Loss=269991.6079 Reco1=123491.6143 Reco2=124689.0749 KL1=2936.0196 KL2=2978.1478 DisCo=1.5897\n",
      "[EPOCH 99/200] Loss=269798.8865 Reco1=123405.6440 Reco2=124596.5036 KL1=2936.4514 KL2=2978.3169 DisCo=1.5882\n",
      "[EPOCH 100/200] Loss=269673.8464 Reco1=123314.3564 Reco2=124487.0323 KL1=2936.3920 KL2=2978.1390 DisCo=1.5958\n",
      "[EPOCH 101/200] Loss=269693.5303 Reco1=123412.6183 Reco2=124555.0710 KL1=2936.1343 KL2=2978.4815 DisCo=1.5811\n",
      "[EPOCH 102/200] Loss=269550.0945 Reco1=123258.1757 Reco2=124447.3941 KL1=2936.0335 KL2=2978.6578 DisCo=1.5930\n",
      "[EPOCH 103/200] Loss=269542.4268 Reco1=123320.1608 Reco2=124452.2217 KL1=2935.9816 KL2=2978.7838 DisCo=1.5855\n",
      "[EPOCH 104/200] Loss=269612.1411 Reco1=123356.8366 Reco2=124470.9197 KL1=2936.0628 KL2=2978.8205 DisCo=1.5870\n",
      "[EPOCH 105/200] Loss=269459.5566 Reco1=123270.6590 Reco2=124445.6329 KL1=2936.1815 KL2=2978.8888 DisCo=1.5828\n",
      "[EPOCH 106/200] Loss=270313.3281 Reco1=123601.8357 Reco2=124693.7406 KL1=2934.1290 KL2=2965.9052 DisCo=1.6118\n",
      "[EPOCH 107/200] Loss=269621.1179 Reco1=123246.5515 Reco2=124260.5599 KL1=2932.4913 KL2=2946.6509 DisCo=1.6235\n",
      "[EPOCH 108/200] Loss=268242.4924 Reco1=122629.5065 Reco2=123648.5623 KL1=2926.1100 KL2=2941.7873 DisCo=1.6097\n",
      "[EPOCH 109/200] Loss=267375.2810 Reco1=122259.1716 Reco2=123073.5704 KL1=2922.4783 KL2=2940.1499 DisCo=1.6180\n",
      "[EPOCH 110/200] Loss=266335.1997 Reco1=121950.0894 Reco2=122597.7138 KL1=2918.8777 KL2=2924.5512 DisCo=1.5944\n",
      "[EPOCH 111/200] Loss=265582.1064 Reco1=121500.3699 Reco2=122256.0318 KL1=2922.9334 KL2=2920.5258 DisCo=1.5982\n",
      "[EPOCH 112/200] Loss=264346.4321 Reco1=120927.1979 Reco2=121698.3695 KL1=2926.8510 KL2=2917.4754 DisCo=1.5877\n",
      "[EPOCH 113/200] Loss=263571.7231 Reco1=120404.0573 Reco2=121199.2317 KL1=2920.4250 KL2=2915.4811 DisCo=1.6133\n",
      "[EPOCH 114/200] Loss=262702.1721 Reco1=120011.0079 Reco2=120977.7161 KL1=2916.7167 KL2=2913.3624 DisCo=1.5883\n",
      "[EPOCH 115/200] Loss=261902.9070 Reco1=119567.1943 Reco2=120594.1188 KL1=2909.9133 KL2=2909.8615 DisCo=1.5922\n",
      "[EPOCH 116/200] Loss=261226.5144 Reco1=119228.3467 Reco2=120338.5706 KL1=2898.8251 KL2=2914.5239 DisCo=1.5846\n",
      "[EPOCH 117/200] Loss=260527.4932 Reco1=118705.7611 Reco2=120116.5322 KL1=2892.9210 KL2=2910.4894 DisCo=1.5902\n",
      "[EPOCH 118/200] Loss=259533.9224 Reco1=118233.1403 Reco2=119788.2641 KL1=2886.7469 KL2=2906.0074 DisCo=1.5720\n",
      "[EPOCH 119/200] Loss=258986.9785 Reco1=117881.6606 Reco2=119623.1979 KL1=2899.8925 KL2=2889.9859 DisCo=1.5692\n",
      "[EPOCH 120/200] Loss=258445.5610 Reco1=117527.6691 Reco2=119320.1208 KL1=2892.3839 KL2=2895.6095 DisCo=1.5810\n",
      "[EPOCH 121/200] Loss=257527.5222 Reco1=116940.5215 Reco2=119018.7225 KL1=2889.5426 KL2=2907.1047 DisCo=1.5772\n",
      "[EPOCH 122/200] Loss=256993.7750 Reco1=116711.4592 Reco2=118870.5803 KL1=2885.1163 KL2=2904.8154 DisCo=1.5622\n",
      "[EPOCH 123/200] Loss=256496.5046 Reco1=116458.4937 Reco2=118626.7761 KL1=2881.8334 KL2=2904.9768 DisCo=1.5624\n",
      "[EPOCH 124/200] Loss=256116.5952 Reco1=116073.9252 Reco2=118443.3110 KL1=2878.0036 KL2=2903.0189 DisCo=1.5818\n",
      "[EPOCH 125/200] Loss=256054.7069 Reco1=116044.7029 Reco2=118509.7753 KL1=2872.0171 KL2=2885.4237 DisCo=1.5743\n",
      "[EPOCH 126/200] Loss=254921.5889 Reco1=115549.0970 Reco2=118120.5771 KL1=2867.5393 KL2=2888.4719 DisCo=1.5496\n",
      "[EPOCH 127/200] Loss=254785.6738 Reco1=115541.5617 Reco2=117872.2930 KL1=2871.0338 KL2=2887.8433 DisCo=1.5613\n",
      "[EPOCH 128/200] Loss=254377.3763 Reco1=115314.0306 Reco2=117867.0820 KL1=2880.4974 KL2=2887.6028 DisCo=1.5428\n",
      "[EPOCH 129/200] Loss=253875.4312 Reco1=115110.9365 Reco2=117566.3267 KL1=2875.3998 KL2=2875.8769 DisCo=1.5447\n",
      "[EPOCH 130/200] Loss=253685.9862 Reco1=114979.5604 Reco2=117541.4067 KL1=2873.5815 KL2=2868.6675 DisCo=1.5423\n",
      "[EPOCH 131/200] Loss=253351.5009 Reco1=114766.3911 Reco2=117414.2845 KL1=2865.9865 KL2=2868.9551 DisCo=1.5436\n",
      "[EPOCH 132/200] Loss=253222.0619 Reco1=114775.3510 Reco2=117222.3086 KL1=2863.6448 KL2=2871.5022 DisCo=1.5489\n",
      "[EPOCH 133/200] Loss=252896.5024 Reco1=114611.2798 Reco2=117209.5359 KL1=2859.2913 KL2=2866.4200 DisCo=1.5350\n",
      "[EPOCH 134/200] Loss=252694.5809 Reco1=114586.3571 Reco2=117106.2138 KL1=2858.4290 KL2=2858.6010 DisCo=1.5285\n",
      "[EPOCH 135/200] Loss=252416.7083 Reco1=114287.7075 Reco2=117043.2365 KL1=2858.3497 KL2=2862.7261 DisCo=1.5365\n",
      "[EPOCH 136/200] Loss=251820.4420 Reco1=114082.8797 Reco2=116760.5213 KL1=2852.1335 KL2=2872.7680 DisCo=1.5252\n",
      "[EPOCH 137/200] Loss=252010.3418 Reco1=114048.8113 Reco2=116802.4055 KL1=2849.1902 KL2=2869.5212 DisCo=1.5440\n",
      "[EPOCH 138/200] Loss=251200.1174 Reco1=113730.1399 Reco2=116588.9285 KL1=2854.1937 KL2=2868.8247 DisCo=1.5158\n",
      "[EPOCH 139/200] Loss=251490.6848 Reco1=113779.6135 Reco2=116601.3840 KL1=2855.4306 KL2=2872.7692 DisCo=1.5381\n",
      "[EPOCH 140/200] Loss=251014.2605 Reco1=113827.7202 Reco2=116385.7182 KL1=2853.8372 KL2=2865.7215 DisCo=1.5081\n",
      "[EPOCH 141/200] Loss=250902.0597 Reco1=113628.2368 Reco2=116394.2162 KL1=2850.6557 KL2=2864.7130 DisCo=1.5164\n",
      "[EPOCH 142/200] Loss=250927.6857 Reco1=113521.7044 Reco2=116286.5930 KL1=2849.2078 KL2=2858.9207 DisCo=1.5411\n",
      "[EPOCH 143/200] Loss=250563.2155 Reco1=113495.3911 Reco2=116250.2166 KL1=2842.9748 KL2=2851.9962 DisCo=1.5123\n",
      "[EPOCH 144/200] Loss=250153.2484 Reco1=113387.7496 Reco2=115981.1180 KL1=2839.7233 KL2=2858.5109 DisCo=1.5086\n",
      "[EPOCH 145/200] Loss=250173.7155 Reco1=113302.8951 Reco2=116008.0198 KL1=2844.0506 KL2=2857.2394 DisCo=1.5162\n",
      "[EPOCH 146/200] Loss=249773.0348 Reco1=113264.1646 Reco2=115808.4349 KL1=2848.8245 KL2=2844.7479 DisCo=1.5007\n",
      "[EPOCH 147/200] Loss=249779.5208 Reco1=113173.7369 Reco2=115780.4192 KL1=2846.0940 KL2=2846.6772 DisCo=1.5133\n",
      "[EPOCH 148/200] Loss=249249.6759 Reco1=113065.7250 Reco2=115643.4443 KL1=2847.1316 KL2=2843.3601 DisCo=1.4850\n",
      "[EPOCH 149/200] Loss=249077.1990 Reco1=112890.3445 Reco2=115416.8513 KL1=2842.2081 KL2=2848.8775 DisCo=1.5079\n",
      "[EPOCH 150/200] Loss=248937.6405 Reco1=112937.7621 Reco2=115469.2861 KL1=2838.6202 KL2=2842.0089 DisCo=1.4850\n",
      "[EPOCH 151/200] Loss=248896.1248 Reco1=112946.2178 Reco2=115329.9935 KL1=2835.1235 KL2=2844.3679 DisCo=1.4940\n",
      "[EPOCH 152/200] Loss=248522.8038 Reco1=112767.5332 Reco2=115239.7535 KL1=2835.7517 KL2=2849.8131 DisCo=1.4830\n",
      "[EPOCH 153/200] Loss=248569.3682 Reco1=112703.4579 Reco2=115217.8682 KL1=2844.8806 KL2=2853.1351 DisCo=1.4950\n",
      "[EPOCH 154/200] Loss=248557.6714 Reco1=112582.0770 Reco2=115147.7321 KL1=2843.7880 KL2=2851.4928 DisCo=1.5133\n",
      "[EPOCH 155/200] Loss=248651.2170 Reco1=112726.8041 Reco2=115203.2976 KL1=2841.1629 KL2=2841.1230 DisCo=1.5039\n",
      "[EPOCH 156/200] Loss=248116.0369 Reco1=112628.5190 Reco2=115028.4184 KL1=2840.8703 KL2=2833.0947 DisCo=1.4785\n",
      "[EPOCH 157/200] Loss=248157.6874 Reco1=112582.2894 Reco2=115053.3655 KL1=2834.5051 KL2=2828.9118 DisCo=1.4859\n",
      "[EPOCH 158/200] Loss=247997.4536 Reco1=112546.0565 Reco2=114913.9988 KL1=2832.9262 KL2=2835.7518 DisCo=1.4869\n",
      "[EPOCH 159/200] Loss=248070.6138 Reco1=112461.5300 Reco2=114945.7939 KL1=2831.4872 KL2=2831.8629 DisCo=1.5000\n",
      "[EPOCH 160/200] Loss=247882.5758 Reco1=112440.7390 Reco2=114906.6327 KL1=2833.3242 KL2=2827.7989 DisCo=1.4874\n",
      "[EPOCH 161/200] Loss=247537.1262 Reco1=112248.0867 Reco2=114837.9296 KL1=2832.8970 KL2=2829.4247 DisCo=1.4789\n",
      "[EPOCH 162/200] Loss=247198.8599 Reco1=112132.1621 Reco2=114656.6619 KL1=2832.3727 KL2=2830.6541 DisCo=1.4747\n",
      "[EPOCH 163/200] Loss=247303.7788 Reco1=112197.6732 Reco2=114708.2095 KL1=2832.1801 KL2=2834.7428 DisCo=1.4731\n",
      "[EPOCH 164/200] Loss=247261.8994 Reco1=112197.1783 Reco2=114657.1254 KL1=2829.9880 KL2=2835.4446 DisCo=1.4742\n",
      "[EPOCH 165/200] Loss=247056.9806 Reco1=112178.9467 Reco2=114545.9184 KL1=2828.0796 KL2=2832.8283 DisCo=1.4671\n",
      "[EPOCH 166/200] Loss=246744.6892 Reco1=111956.5974 Reco2=114518.1086 KL1=2826.0053 KL2=2836.7602 DisCo=1.4607\n",
      "[EPOCH 167/200] Loss=246872.2494 Reco1=112035.7164 Reco2=114531.3895 KL1=2825.8704 KL2=2837.8344 DisCo=1.4641\n",
      "[EPOCH 168/200] Loss=246841.6418 Reco1=112005.6918 Reco2=114494.1707 KL1=2825.4377 KL2=2835.3097 DisCo=1.4681\n",
      "[EPOCH 169/200] Loss=246587.2991 Reco1=111806.8311 Reco2=114418.3966 KL1=2827.3644 KL2=2831.8612 DisCo=1.4703\n",
      "[EPOCH 170/200] Loss=246750.7461 Reco1=111984.2057 Reco2=114339.1700 KL1=2831.0148 KL2=2836.0186 DisCo=1.4760\n",
      "[EPOCH 171/200] Loss=246383.5419 Reco1=111779.1949 Reco2=114307.0020 KL1=2826.9660 KL2=2839.2639 DisCo=1.4631\n",
      "[EPOCH 172/200] Loss=246559.9424 Reco1=111781.9882 Reco2=114367.0617 KL1=2828.0737 KL2=2830.3570 DisCo=1.4752\n",
      "[EPOCH 173/200] Loss=246628.3202 Reco1=111821.1071 Reco2=114340.9561 KL1=2828.2317 KL2=2829.8392 DisCo=1.4808\n",
      "[EPOCH 174/200] Loss=246174.1716 Reco1=111720.9454 Reco2=114247.7003 KL1=2827.7502 KL2=2829.1233 DisCo=1.4549\n",
      "[EPOCH 175/200] Loss=246192.8685 Reco1=111586.7657 Reco2=114179.2911 KL1=2827.4559 KL2=2835.8887 DisCo=1.4763\n",
      "[EPOCH 176/200] Loss=246078.3826 Reco1=111580.2282 Reco2=114221.3865 KL1=2828.0627 KL2=2836.1744 DisCo=1.4613\n",
      "[EPOCH 177/200] Loss=246227.6786 Reco1=111603.1848 Reco2=114123.3492 KL1=2826.3889 KL2=2832.8285 DisCo=1.4842\n",
      "[EPOCH 178/200] Loss=245997.3334 Reco1=111559.7653 Reco2=114181.4016 KL1=2824.6409 KL2=2830.6205 DisCo=1.4601\n",
      "[EPOCH 179/200] Loss=245855.5723 Reco1=111539.2212 Reco2=114128.1794 KL1=2824.6955 KL2=2831.8319 DisCo=1.4532\n",
      "[EPOCH 180/200] Loss=245750.3126 Reco1=111346.3722 Reco2=114092.8990 KL1=2827.5905 KL2=2833.2380 DisCo=1.4650\n",
      "[EPOCH 181/200] Loss=245742.3195 Reco1=111448.3469 Reco2=114086.4636 KL1=2830.5541 KL2=2831.1441 DisCo=1.4546\n",
      "[EPOCH 182/200] Loss=245551.2357 Reco1=111436.5393 Reco2=113974.1145 KL1=2831.3401 KL2=2829.7668 DisCo=1.4479\n",
      "[EPOCH 183/200] Loss=245705.7782 Reco1=111418.3318 Reco2=114018.9820 KL1=2829.7025 KL2=2829.5819 DisCo=1.4609\n",
      "[EPOCH 184/200] Loss=245497.2301 Reco1=111367.2516 Reco2=113926.8432 KL1=2826.3847 KL2=2830.5570 DisCo=1.4546\n",
      "[EPOCH 185/200] Loss=245457.5702 Reco1=111242.1143 Reco2=113883.7486 KL1=2826.9403 KL2=2831.0917 DisCo=1.4674\n",
      "[EPOCH 186/200] Loss=245245.8448 Reco1=111227.5868 Reco2=113842.5284 KL1=2826.1020 KL2=2830.1446 DisCo=1.4519\n",
      "[EPOCH 187/200] Loss=245261.8051 Reco1=111266.1632 Reco2=113863.2096 KL1=2827.1260 KL2=2834.6906 DisCo=1.4471\n",
      "[EPOCH 188/200] Loss=245173.3275 Reco1=111217.7046 Reco2=113718.3454 KL1=2823.7056 KL2=2835.5266 DisCo=1.4578\n",
      "[EPOCH 189/200] Loss=245222.4500 Reco1=111331.1013 Reco2=113790.1213 KL1=2821.2453 KL2=2838.1069 DisCo=1.4442\n",
      "[EPOCH 190/200] Loss=244940.3027 Reco1=111181.0320 Reco2=113738.6715 KL1=2819.5518 KL2=2837.3900 DisCo=1.4364\n",
      "[EPOCH 191/200] Loss=245103.7350 Reco1=111122.3066 Reco2=113796.3622 KL1=2821.8285 KL2=2836.9008 DisCo=1.4526\n",
      "[EPOCH 192/200] Loss=245133.8301 Reco1=111151.0101 Reco2=113747.5435 KL1=2823.3605 KL2=2836.3722 DisCo=1.4576\n",
      "[EPOCH 193/200] Loss=245015.4211 Reco1=111104.4184 Reco2=113731.3115 KL1=2822.0171 KL2=2835.0606 DisCo=1.4523\n",
      "[EPOCH 194/200] Loss=244852.4181 Reco1=111048.5361 Reco2=113690.3010 KL1=2821.2185 KL2=2835.8700 DisCo=1.4456\n",
      "[EPOCH 195/200] Loss=244780.7238 Reco1=111197.4674 Reco2=113629.9261 KL1=2821.4089 KL2=2836.5746 DisCo=1.4295\n",
      "[EPOCH 196/200] Loss=244883.4652 Reco1=111044.1712 Reco2=113656.3300 KL1=2819.9402 KL2=2837.6108 DisCo=1.4525\n",
      "[EPOCH 197/200] Loss=244753.2134 Reco1=110992.8394 Reco2=113619.0587 KL1=2820.2952 KL2=2836.3150 DisCo=1.4485\n",
      "[EPOCH 198/200] Loss=244778.9242 Reco1=111129.8625 Reco2=113707.5068 KL1=2820.8250 KL2=2834.6967 DisCo=1.4286\n",
      "[EPOCH 199/200] Loss=244742.5204 Reco1=110999.4720 Reco2=113621.6931 KL1=2821.3618 KL2=2833.3142 DisCo=1.4467\n",
      "Finished training.\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'vae_lr': 1e-4,\n",
    "    'beta': 0.5,\n",
    "    'alpha': 0.5,\n",
    "    'vae_latent': 8,\n",
    "    'vae_nodes': [28, 14],\n",
    "    'lambda_disco': 10000.0\n",
    "}\n",
    "\n",
    "run(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2761078-2f9b-4392-949d-43621fcfdcf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
